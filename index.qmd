---
title: "A Need for a Contemporary Field"
---

The future qualities of AI systems will be shaped by the data and evaluation practices we establish today. AI systems are a direct product of the data used to train and evaluate them. We this indirectly shape their behavior via the processes we use to design, gather, analyze, and report training and evaluation data. Datasets are very often re-used [@geiger2021garbage], likely because of the amount of work necessary to create the data. Data professionals spend more time on data preparation and cleansing than they do on model selection, training and deployment [^10] [^11] [^12], the focus is almost always on the latter [@birhane2022values]. 

> *“Instead of focusing on the code, companies should focus on developing systematic engineering practices for improving data in ways that are reliable, efficient, and systematic. In other words, companies need to move from a model-centric approach to a data-centric approach.”* 
> - Andrew Ng

Evidence points to a need for a more sophisticated approach to the collection of load-bearing benchmark datasets in AI fields. Work over the past decade has emerged explaining limitations of commonly re-used datasets, which invclude a lack of representativeness [@hullman2022worst], measurement quality [@jacobs2021measurement], accounting for the full range of reasonable interpretations in terms of annotations [@cabitza2023toward], and completeness in reporting of the annotation process [@geiger2021garbage; @daneshjou2021lack]. Although improvements to data collection processes have been proposed, they are at best slowly being adopted, and at worst being largely ignored. The increasingly sophisticated systems we build have an accompanying cost not only in terms of sheer training data size, but also in the labor required to curate the data (e.g. Large Language Models or LLMs) [@kandpal2025position]. This is exacerbated by the increasingly sophisticated qualities we aim to evaluate systems on, that in turn are challenging to define and measure (e.g. 'fairness') [@jacobs2021measurement]. Furthermore, as many systems aim to behave similar to humans, the human-like behavior of the sophisticated systems we are evaluating are a prime target for anthropomorphistically biased mis-interpretations of their outputs  [@altmeyer2024position]. 

[^10]: Anaconda data science report 2022: https://www.anaconda.com/resources/whitepaper/state-of-data-science-report-2022
[^11]: Anaconda data science report 2023: https://know.anaconda.com/rs/387-XNW-688/images/Anaconda%202023%20State%20of%20Data%20Science%20Report.pdf
[^12]: Anaconda data science report 2024: https://www.anaconda.com/wp-content/uploads/2025/01/Anaconda_SODS.pdf

The degree to which we invest in better design, collection, and analysis of training and evaluation data will determine the real-world performance of the AI systems we will build. It is thus crucial that we put in place better practices. However, improving how the field collects and labels data requires substantial efforts beyond the already substantial efforts invested. Resources must thus be dedicated to determining what best practices are, which will require dedicated research to solve issues unique to training vs. evaluation data. 

Firstly, improving the way we design and collect of training data, especially data that is likely to be widely re-used, is key to developing trustworthy models [@liang2022advances], but will require investment in deliberate and informed design, distribution and maintenance. Focus must be put into solving sampling problems that are not solved even when the dataset is very large, e.g. sampling data for training that is representative of the environment or desired behavior in deployment conditions [@hullman2022worst]. In some instances, data requirements may be too great for this to be currently possible. *Data scarcity*, or a lack of sufficient training data, is a common issue with contemporary techniques e.g. deep learning, across machine learning fields e.g. computer vision, healthcare, natural language processing  [@bansal2022systematic]. For example, recent generative language models are increasingly being trained with text crawled randomly from the internet over more carefully curated sources that contain information likely to be true, like scientific texts and wikipedia [^13], and still this may not be able to keep pace with increasing demands [@villalobos2024position]. A number of solutions have been proposed for once data has been collected [@bansal2022systematic], but a clearer solution might be to begin with better data. The challenge is the enormous cost to create large training datasets, with one estimate for LLMs being between up to 1000 times the training costs, which in turn are estimated as being tens of millions USD [@kandpal2025position]. Despite the need for solutions in this regard, the overwhelming focus remains on algorithmic work [@birhane2022values], and not the creation and curation of load-bearing training datasets. 

[^13]: For example, LLama 2 required 2T tokens for training, whereas Llama 3 required 15T tokens. All of Wikipedia and all scientific texts likely collectively amount to less than 1T tokens. 

Secondly, there is substantial room for improvement in terms of the practices of data labeling as well. Implementations of such practices are also costly. As an example, the case study in this thesis proposes enriching design, collection, analysis, and reporting of training/evaluation data for AI systems, using knowledge from the social sciences [@beck2022improving, @jacobs2021measurement], metrology (measurement science) [@welty2019metrology], and work in the computational sciences on 'ground-truthing' [@cabitza2023toward]. It requires a higher cost in terms of scientific labor, as it includes an *a-priori* empirical investigation of the data collection process in addition to the data collection process itself. In other words, it requires research on *how to collect the data*. Applying the knowledge to other tasks would require similar research, in principle for every combination of *construct* (i.e. the latent phenomenon of interest being measured), *content* (e.g. text, video, audio etc. and in some cases also subgroups, e.g. tweets vs. podcast transcripts vs. formal speeches etc.), and for relevant *characteristics* of annotators (i.e. ethnicity, political affiliation, etc.). These costs are in addition to the annotation labor. Open questions remain for the primary case-study as well as for the field as a whole, all of which anticipate future studies, and by extension further efforts. Although it is clear there is substantial room for improvement along with reprecussive benefits from such improvements, they will require resources. 

Questions remain as to how AI research as it is currently conducted can develop and implement solutions. This conclusiory manuscript thus highlights the crucial challenge for academic study of AI in the coming decade: developing an infrastructure that allows for the study of AI, including the data that are its raw materials, with little - or at the very least, substantially less - harmful bias. It highlights the need for identifiable academic publication venues that gather works on the study of ground-truthing, more modern publication formats that allow for dataset requirements to be studied prior to their collection, and for infrastructure that allows the burden of their collection to be distributed among stakeholders. It concludes that, while works like the case study embedded in this thesis are necessary, the various fields studying topics related to AI are poorly positioned to implement them. 

## Data challenges

### Training data

Contemporary training methods require increasingly large amounts of training data. This has lead to to the development of techniques aimed at increasing and/or augmenting available data [@alzubaidi2023survey]. Beyond the challenge of gathering data at the necessary scale, is the challenge of ensuring that it is generally representative of the environment where it is to be deployed [@hullman2022worst], has sufficiently diverse coverage of the various scenarios it will encounter [@liang2022advances], and is as free as possible of bias [@mehrabi2021survey]. Such a process requires deliberate design, and cannot be compensated for by increasing the amount of data collected [@hullman2022worst]. 

These issues are well illustrated with the most contemporary trends of Generative AI systems. Taking the development of Large Language Models (LLMs) as a use-case: the training dataset for Llama 3 included 15T tokens, up from 2T for Llama 2. Although not all details of the datasets have been shared, and setting aside questions of copywrite, licensing, and ethical concerns, any available text that is likely to have some quality is limited compared to these requirements. Among the refined sources of text available, Wikipedia, which comprises some 6.9M English articles, comprised of approximately 62M pages over all languages, is an estimated 5 billion tokens[^4]. If we take the approximate 4.4M papers published in 2024 and indexed on Scopus as an indication, academia published an estimated 45B tokens in that year. If we extend our reach to other repositories, e.g. the approximately 85 million academic papers available on Sci-hub[^9] or SCOPUS[^8] would result in an estimate of 700B. A similar figure might be estimated from libegen and the 7.5M[^5] books there. While academic pursuits result in increasing token counts, we have immediate access to a set of approximately 1.5T. Internet archive has some 44M books, which may yield up to 4.4T, although we expect duplicates with the libgen archive. Thus, a more likely source for the ever-increasing data requirements are repositories like Common Crawl[^6]. But this too has limits, and we are projected to have too little human-generated text to continue the increase in model size this decade [@villalobos2024position].  

[^4]: According to [wikipedia](https://en.wikipedia.org/wiki/Wikipedia:Size_of_Wikipedia#:~:text=Monthly%20statistics-,Number%20of%20pages,pages%20are%20created%20than%20articles.) at 9 May, 2025.
[^5]: According to [The Atlantic](https://www.theatlantic.com/technology/archive/2025/03/libgen-meta-openai/682093/LibGen). 
[^6]: https://commoncrawl.org/
[^8]: https://blog.scopus.com/posts/scopus-roadmap-whats-new-in-2022#:~:text=There%20are%20currently%2087%2B%20million,new%20articles%20per%20day%20indexed.
[^9]: https://www.sci-hub.se/about

The largest frontier models cost tens of millions of USD to train, with estimates of GPT-4 at 40M USD for hardware (chips, servers, and networking hardware) and energy, and estimated increases of 2.4 X per year suggesting that frontier models will cost 1B USD to train by 2027 [@cottier2024rising]. Notably, this cost exceeds annual revenues in companies training large scale LLMs [@kandpal2025position]. Human labor responsible for the collection, curation, and eventual annotation of training data in addition to the training of the model (including researchers, engineers, and managers, but not data center employees and operations staff) is estimated at 29%-49% of the overall cost [@cottier2024rising]. Notably, these cost calculations ignore the cost of producing the text itself, and though its value is difficult to calculate, estimates range from 10-1000 X more than the total cost of the training of the models [@kandpal2025position]. In other words, the more valuable thing is the data and not the model, and the lack of appropriate compensation for its use has given rise to a number of lawsuits (e.g. Authors Guild vs. OpenAI [^7]). 

[^7]: https://authorsguild.org/app/uploads/2023/12/Authors-Guild-OpenAI-Microsoft-Class-Action-Complaint-Dec-2023.pdf

Thus, the field of AI must wrestle with two opposing issues: we want to train models with data that has high quality - e.g we want the data to be representative of conditions where the model will be deployed, and thus relevant distributions in the training data must reflect the environment in which the models will be deployed [@hullman2022worst] - but the data requirements to train them appear thus far to be ever-increasing [@villalobos2024position]. Models trained with contemporary techniques add an additional challenge: that of scale. We want the data used in training to be 'good' because we want the models to be 'trustworthy' - in the case of LLMs, we want to have reason to think they will generate text that includes claims that we think are true. Perhaps our closest approximation to 'what we think is true' is contained the overall perspective presented in all of academic work and official reports - the estimating probability of the truth of potential explanations given carefully collected and analyzed observations. And yet, even if there were little to no barriers to using all of human academic text to train LLMs, this amount of text pales in quantity to the data requirements. 

### Evaluation data

Despite growing recognition of evaluation data problems [@hullman2022worst], which in turn have the potential to lead to harms [@mehrabi2021survey], actual progress toward better annotation data practices, from collection, to analysis, to reporting remains slow. Current incentives prioritize accuracy and efficiency rather than careful measurement design or long-term dataset stewardship [@birhane2022values]. Beyond the challenge of gathering data at the necessary scale, are issues about the quality of the data itself, specifically the quality of the overall process that created the data. This challenge is essentially one of measurement[@welty2019metrology].

Datasets for evaluation very often contain human input [@geiger2020garbage; @geiger2021garbage]. When collecting annotations, labels, or other forms of input from people in order to construct training/evaluation datasets, we are attempting to collect measurements of latent, unobservable *constructs* [@jacobs2021measurement]. In other words, when we consider the input from multiple people in aggregate, we do not directly observe e.g. the presence or absence of an object in a digital image; rather we observe the probability that a person from a given population will indicate the presence of absence of the object in the image [@welty2019metrology]. In the parlance of psychology, one cannot directly observe an other's Extraversion score, as one might observe an other's height. And even though height is observable, our measurements of it are still imperfect: in using a measurement device like a ruler multiple times, should we measure precisely enough, we would observe variance in each measurement, and a true score for height imperfectly represented by our imperfect measurements [@welty2019metrology]. 

Any standardized procedure for comparing two or more individuals is treated like a measurement instrument in the social sciences [@urbina2014essentials]. The repeatable procedures that we use to gather annotations are similarly measurement instruments [@beck2022improving]. Given the complexity of measuring unobservable phenomena, instruments are subjected to scrutiny prior to being considered usable for their intended purpose, in the form of studies that examine the properties of the measurements produced byt he instrument. For example, the process of *construct validation* involves estimating the extent to which an instrument measures an unobservable construct [@Wehner2020]. It assumes an unknowable true score, and that all attempts to measure the true score are imperfect. There is no single solution to demonstrating the validity of a construct, but rather an accumulation of evidence, across multiple studies, with observations made using different methods [@smith2005construct]. Thus, by extension the datasets that we collect and use to benchmark the performance of models, are similarly measurement instruments [@welty2019metrology]. 

Thus, the field of AI must wrestle with the opposing pressure of accuracy vs. the cost of developing measurement instruments for practically infinite use-cases: we want measurements that are as 'accurate' - i.e. we want the data to be as clear a representation of our phenomenon of interest as it could be - but developing an accurate instrument is a costly exercise to add to the cost of collecting the data itself. Further, the topics we wish to measure in the ever-increasingly-complex AI systems as themselves measurement challenges. Topics like 'fairness' [@jacobs2021measurement], and 'intelligence' [citation] are not only challenging to develop instruments for, but also to define.

### Ground-truthing is a field

Data used for training and evaluation of models is of central importance. Solutions to problems in both training and evaluation data collection require deliberate study for standards to improve and disseminate. As it stands no central field of study for this topic exists, despite its central importance to all AI-related fields. Unlike the field of Machine Learning, other disciplines (psychology, economics, software engineering) have entire fields dedicated to the design, collection, analysis, and reporting of data that involves human behavior (psychometrics, econometrics, software testing). 

## Labor Challenges

### Scientific Labor

As it stands, our current knowledge gathering apparatus - science as it is now practiced and reported - is overburdened and inefficient. The ever increasing volume of published manuscripts on AI and related topics[^2] makes it impossible to stay abreast of the overall field: 10% of over 4 million publications indexed on the SCOPUS academic database in 2024, up from around 7% in 2022, had terms related to AI in their title, keywords or abstracts (see Appendix A). The number of submitted manuscripts also increases year over year, with popular conferences like NeurIPS receiving upwards of 12k submissions in 2023[^1]. This makes it more and more difficult to find reviewers, and by extension to monitor the overall quality of the field [@Zhang_2022]. These figures do not include preprints posted on servers like arXiv, which show over 42k works with AI related terms in the abstract for 2024, more than doubling the about 17.5k posts in 2019. 

A primary resource is the labor of academic students, from the bachelor to the PhD level. One study in Canada in 2012 showed that one third of all academic output comes form PhD students [@lariviere2012shoulders]. There are clear suggestions to better allocation of this work: e.g. PhD candidates are bogged down by requirements to write and defend theses despite the decreasing trend of thesis citations over time [@lariviere2008declining], and evidence that PhD candidates with fixed duration contracts are exceeding that duration by several months, resorting to completing their thesis on their own time and risking failing at completion [@van2013took]. A thesis defense is a more curated experience as the 'peer reviewers' are chosen by the supervisors of the student - on the other hand, peer reviewers in the world of academic publication are far broader than the networks of the PhD candidate's supervisory staff [@lariviere2012shoulders]. Understandably, publications, and not theses, remain the key factor in the assesment of their value as scientists [@anderson2022effect]. Predictably, academics appear to be turning to LLMs for assistance, as evidence of its use is showing in academic work in both peer reviews [@liang2024monitoring], and in manuscripts [@gray2024chatgpt]. This overburden raises questions beyond the poor evaluation of the models that underlie 'AI' to the 'AI' research process itself, as well as to its likelihood of applying improvements. 

[^1]: https://github.com/tranhungnghiep/AI-Conference-Info?utm_source=chatgpt.com
[^2]: terms included AI, artificial intelligence, machine learning, and neural networks. See Appendix for specific search strings.

### Collaboration to address data requirement challenges

One approach to both gathering the necessary training data at scale as well as the labels or annotations have been shown in collaborations between scientists and the general public. Online platforms host and facilitate the creation of various resources, ranging from media and other forms of data, labels and annotation projects, as well as forums for discussion. For example inaturalist.org is an online community with over 8 million users who make contributions in the form of images taken on their smartphones, and/or labels of the species in the images [@van2018inaturalist]. Zooniverse.org is an online community of over 2.8M users that hosts projects defined by scientists to gather labels from non-scientists [@fortson2012galaxy]. A third example is commonvoice.mozilla.org/en, which is a large dataset of speech transcription in 76 languages, provided by approximately 150k participants [@ardila-etal-2020-common]. 

Another similarly scalable infrastructure for dataset creation might be possible by adapting an academic publication format called the Registered Report [@chambers2013registered]. Initially designed to compensate for editorial decisions being made based on the results, rather than the quality of the methods. In many fields, aspects of the data collection design, as well as the design of analysis and prediction of results occur *a-priori*, in principle not to bias interpretation of results. In a Registered Report, researchers submit a manuscript that includes information relevant to how the study will be conducted, including motivation of the work (i.e. introduction), details of data collection processes, as well as analyses. Typical review stages apply, i.e. suggestions for revisions or rejections, or the manuscript may receive an *in-principle acceptance*, whereby reviewers and editor agree to a publication should the methods used in the manuscript either follow closely the in-principle accepted version, or appropriate justifications be made for any changes that may have occurred. Thus acceptance of publications is made based on the strength of the methods, which also are strengthened by a peer-review process prior to data collection. 

The Registered-Report format is exceptionally well-suited to the collection of datasets intended for AI training and/or evaluation. Firstly, they allow for peer-review prior to collection, whereby a panel of experts will provide critiques that will either strengthen the eventual design, or reject it in favor of publishing other stronger designs. Given the scope and resources needed to collect AI datasets, this format could be adapted such that it is published in its entirety prior to data collection. This may thus allow for a more public critique of the design prior to paying the resource cost of collection, and further allow for the submissions of responses in the form of data that conforms to the design in the published manuscript in a decentralized fashion, from multiple stakeholders, thus reducing the bias from any single data collection point and allowing for the sharing of financial and other resource burdens. 


## Concluding Call to Action

Better data design enables better science, more responsible innovation, and safer, more meaningful AI systems, but is costly, even in comparison to the current amount of effort and funding dedicated to AI (and related fields) research. A key priority is that the AI/ML research community, conferences, funding agencies, and publishers must recognize data research, analysis as well as creation, as a research contribution.New formats like data-focused Registered Reports and micropublications should be adopted to 1) distribute efforts, and 2) aggregate knowledge. 

Further, we must rethink how we allocate labor. Load-bearing datasets require more effort than the initial collection and subsequent distribution, but also maintenance. This beyond research required to create useful datasets, we also require resources to maintain them. 



# Appendix {#sec-appendix}

## Appendix A: Citation Trends Plot

```{r publication_plots, echo=FALSE,message=FALSE,warning=FALSE}
library("here")
library("tidyverse")
library('patchwork')
library('viridis')

my_colors <- viridis(n=10, option="magma")

# query : (TITLE-ABS-KEY((((machine OR deep OR reinforcement OR supervised OR unsupervised) AND learning) OR (""neural networks"") OR (AI or ""artificial intelligence""))) AND PUBYEAR > 1999 AND PUBYEAR < 2027)

ml_pub_df <- read_csv(here("data", "Scopus-10-Analyze-Year.csv")) %>% mutate(type = "ml_papers")

#query : (PUBYEAR > 1999 AND PUBYEAR < 2027)
overall_pub_df <- read_csv(here("data", "Scopus-10-Analyze-Year (1).csv")) %>% mutate(type = "overall")

pub_df <- bind_rows(ml_pub_df, overall_pub_df)

p1 <- pub_df %>% filter(YEAR<2025) %>%
  ggplot(aes(YEAR, citations, color = type, group = type)) +
  geom_line(linewidth = 1.2) +
  geom_point(size = 2) +
  scale_color_manual(values=c(my_colors[3], my_colors[7])) +
  scale_y_continuous(labels = scales::comma) +
  labs(title = "SCOPUS Publication #:",
       subtitle="ML vs. Overall",
           x = "Year",
           y = "Number of Publications",
       color = "Category") +
  theme_minimal(base_size = 12) +
  theme(legend.position = "top")

p2 <- pub_df %>% pivot_wider(names_from = type, values_from = citations) %>%
  mutate(proportion = ml_papers/overall*100) %>% filter(YEAR<2025) %>%
  ggplot(aes(YEAR, proportion)) +
  geom_line(linewidth = 1.2, color=my_colors[5]) +
  geom_point(size = 2,, color=my_colors[5]) +
  labs(title = "SCOPUS Publication %",
       subtitle="ML",
           x = "Year",
           y = "Percentage of Publications") +
      theme_minimal(base_size = 12)
  
(p1+p2)
```

## Appendix B: Search terms

### SCOPUS:

for AI related topics:
TITLE-ABS-KEY ( ( ( ( machine OR deep OR reinforcement OR supervised OR unsupervised ) AND learning ) OR ( "neural networks" ) OR ( ai OR "artificial intelligence" ) ) ) AND PUBYEAR > 1999 AND PUBYEAR < 2027

for overall publication records:
PUBYEAR > 1999 AND PUBYEAR < 2027

### arXiv:
[Abstract] 
AI or "artificial intelligence" 
OR machine AND learning
OR supervised AND learning
OR reinforcement AND learning
OR neural AND networks


17,459 results in 2019
23,923 results in 2020
27,610 results in 2021
29,690 results in 2022
33,419 results in 2023
42,183 in 2024


## Appendix C: Token estimates

Taking a study on the word length requirements of education journals as a proxy, @fairbairn2009profile report the following figures:

```{r token_counts,echo=FALSE,message=FALSE,warning=FALSE}
paper_lengths <- data.frame(
  TokenRange = factor(c("<3000", "3000-3900", "4000-4900", "5000-5900", 
                 "6000-6900", "7000-7900", "8000-8900", 
                 "9000-9990", ">=10000"), 
                 levels = c("<3000", "3000-3900", "4000-4900", "5000-5900", 
                 "6000-6900", "7000-7900", "8000-8900", 
                 "9000-9990", ">=10000")),
  Count = c(22, 36, 53, 114, 108, 85, 56, 12, 31)
)

paper_lengths %>%
  ggplot(aes(x=TokenRange, y=Count, fill=Count)) +
    geom_point(shape = 21, size = 5) +
    theme_minimal() +
    scale_fill_viridis(option="plasma") +  
  labs(title = "Distribution of Token Counts in Papers",
       x = "Token Range",
       y = "Count") + theme(
    axis.text.x = element_text(angle = 45, hjust = 1)
  )
```

```{r}
# Estimate midpoints for each bin
midpoints <- c(2500, 3450, 4450, 5450, 6450, 7450, 8450, 9500, 10500)

# Add midpoints to the dataframe
paper_lengths$Midpoint <- midpoints

# Estimate mean token count
mean_est <- sum(paper_lengths$Midpoint * paper_lengths$Count) / sum(paper_lengths$Count)

# Calculate weighted variance
var_est <- sum(paper_lengths$Count * (paper_lengths$Midpoint - mean_est)^2) / sum(paper_lengths$Count)

# Take square root to get standard deviation
sd_est <- sqrt(var_est)

# Print result
paste()
print(paste("SD words: ", sd_est))
print(paste("Mean words: ", mean_est))
```


https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them?utm_source=chatgpt.com
According to Open AI, a token is 3/4 of a word

```{r}
sd_tokens <- sd_est*1.25
mean_tokens <- mean_est*1.25 

print(paste("SD Tokens: ", round(sd_tokens, 2)))
print(paste("Mean Tokens: ", round(mean_tokens, 2)))

paste( round(((mean_tokens-sd_tokens) * 4500000) / 1000000000, 2), 
       " Billion to", 
       round (((mean_tokens+sd_tokens) * 4500000) / 1000000000, 2), 
       "Billion tokens per year from academia.")

paste( 4.8*1.25, "Billion tokens from Wikipedia.")
```

Wikipedia:
https://en.wikipedia.org/wiki/Wikipedia%3ASize_of_Wikipedia?utm_source=chatgpt.com


```{r}
5000000000/2000000000000*100
```



Sci Hub: 
https://www.sci-hub.mk/
88343822 documents as of 9 May 2025 13:47. 

```{r}
(7927.71 * 88343822) / 1000000000 
```

https://www.theatlantic.com/technology/archive/2025/03/libgen-meta-openai/682093/
LibGen

7.5 million books and 81 million research papers.

```{r}
# at about 100k words per book
(100000*7500000) / 1000000000

(100000*44000000) / 1000000000
```


Llama 2 was 2T tokens, Llama 3 was 15T. I didn't really see where they got their data from. So I made some guesses. 

Looked at Wiki. The whole thing is like 5B. 

Sci-Hub, 88.3M papers, which at 8kish tokens is an upper boundary of 700B or so.

7.5 million books on libgen you get about another 700B or so, at 100k-ish per words. 

Internet archive has some 44 **million** books. 4.4T. 
