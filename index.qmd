---
title: "A Need for a Contemporary Field"
editor: 
  markdown: 
    wrap: 72
---

## Abstract

AI systems are shaped by the training data, algorithm, and evaluation
data used to build them. Recent work has shown a need for greater
amounts of data, and greater effort in the design, collection, analysis
and reporting of data used for building AI systems. Solving these
problems may require more effort and resources than the field can offer.
Present work makes suggestions aimed at making better use of labor and
resources in the AI research field to meet these needs.

## Introduction

The future quality of AI systems will be shaped by the data, evaluation,
and reporting practices we establish today. AI systems are a product of
the data used to train and evaluate them, thus we indirectly shape their
behavior by the processes we design to collect training and evaluation
data [@hullman2022worst]. Training and evaluation data have ongoing
effects as they are often re-used [@geiger2021garbage], likely because
of their high cost. Data professionals spend more time on data
preparation and cleansing than they do on model selection, training and
deployment [^1] [^2] [^3], and yet the focus remains on the latter
[@birhane2022values] with data professionals indicating that they prefer
the 'model work' over the 'data work' [@sambasivan2021everyone].

[^1]: Anaconda data science report 2022:
    https://www.anaconda.com/resources/whitepaper/state-of-data-science-report-2022

[^2]: Anaconda data science report 2023:
    https://know.anaconda.com/rs/387-XNW-688/images/Anaconda%202023%20State%20of%20Data%20Science%20Report.pdf

[^3]: Anaconda data science report 2024:
    https://www.anaconda.com/wp-content/uploads/2025/01/Anaconda_SODS.pdf

> *“Instead of focusing on the code, companies should focus on
> developing systematic engineering practices for improving data in ways
> that are reliable, efficient, and systematic. In other words,
> companies need to move from a model-centric approach to a data-centric
> approach.”* - Andrew Ng[^4]

[^4]: [landing.ai](https://landing.ai/data-centric-ai#:~:text=%E2%80%9CInstead%20of%20focusing%20on%20the,a%20data%2Dcentric%20approach.%E2%80%9D)

Decisions made when collecting data comprise the design, whether or not
they are made consciously and carefully. Work over the past decade has
emerged explaining limitations of decisions made in the design of
commonly re-used datasets, which include a lack of representativeness
[@hullman2022worst], measurement quality [@jacobs2021measurement],
accounting for the full range of reasonable interpretations in terms of
annotations [@cabitza2023toward], and completeness in reporting of the
annotation process [@geiger2021garbage; @daneshjou2021lack]. Evidence
points to a need for a more sophisticated approach to the design of
datasets in AI fields, especially frequently re-used benchmarks that
become 'load-bearing' cornerstones in their niches.

Although improvements to data collection processes have been proposed,
they are at best slowly being adopted, and at worst being largely
ignored. The increasingly sophisticated systems we build have an
accompanying cost not only in terms of sheer training data size, but
also in terms of the labor required to curate the data
[@kandpal2025position]. This is exacerbated by the increasing scale
required to train systems using the most advanced methods
[@villalobos2024position], and increasingly sophisticated qualities we
aim to evaluate systems on, that in turn are challenging to define and
measure e.g. fairness [@jacobs2021measurement], and intelligence
[@gignac2024defining]. Furthermore, as many systems aim to behave
similar to humans, the human-like behavior of the sophisticated systems
we are evaluating are a prime target for anthropomorphistically biased
mis-interpretations of their outputs [@altmeyer2024position].

The degree to which we invest in researching best practices for the
design, collection, and analysis of training and evaluation data will
determine the real-world performance of the AI systems we will build. It
is thus crucial that we put in place better practices. The emerging
field of *AI metrics* focuses on developing best practices for the
measurement AI system performance [@gignac2024defining], by treating
datasets as measurement instruments to be developed and maintained
[@welty2019metrology]. However, improving how AI fields collect and
label data requires substantial efforts beyond the already substantial
efforts invested. Resources must thus be dedicated to determining what
best practices are, which will require dedicated research to solve
issues unique to training vs. evaluation data, and to implementing said
practices.

### Too high a cost?

Questions remain as to how AI research as it is currently conducted can
develop and implement solutions. This conclusiory manuscript thus
highlights the crucial challenge for academic study of AI in the coming
decade: developing an infrastructure that allows for the study of AI,
including the data that are its raw materials, with little - or at the
very least, substantially less - harmful bias. It highlights
shortcomings in the data used for training, and the accompanying 'ground
truth' (e.g. labels, annotations etc.), along with challenges in
gathering these at scale, as well as the scientific and annotation labor
necessary to meet these challenges. It highlights the need for
identifiable academic publication venues that gather and disseminate
works on the study of data design, collection, analysis, and reporting
of data for AI training and evaluation, as well as more modern
publication formats that allow for dataset requirements to be studied
prior to their collection, and for infrastructure that allows the burden
of their collection to be distributed among stakeholders. It concludes
that, while works like the case study embedded in this thesis are
necessary, the various fields studying topics related to AI are poorly
positioned to implement them without substantial modernization.

## Data challenges

### Training Data

Improving the way we design and collect training data, especially data
that is likely to be widely re-used, is key to developing trustworthy
models [@liang2022advances; @welty2019metrology], but requires
investment in deliberate and informed design [@hullman2022worst], and
maintenance [@liem2023treat]. Beyond the challenge of gathering data at
the necessary scale, is the challenge of ensuring that it is generally
representative of the environment where it is to be deployed
[@hullman2022worst], has sufficiently diverse coverage of the various
scenarios it will encounter [@liang2022advances], and is as free as
possible of bias [@mehrabi2021survey]. Such a process requires
deliberate design, and cannot be compensated for by increasing the
amount of data collected [@hullman2022worst].

In some instances, data requirements may be too great for this to be
currently possible with data that currently exists. *Data scarcity*, or
a lack of sufficient training data, is a common issue with contemporary
techniques e.g. deep learning, across machine learning fields e.g.
computer vision, healthcare, natural language processing
[@bansal2022systematic]. A number of solutions have been proposed for
once data has been collected [@bansal2022systematic;
@alzubaidi2023survey], but a clearer solution might be to begin with
more carefully collected data. Despite the need for solutions in this
regard, the overwhelming focus in AI fields remains on algorithmic work
[@birhane2022values], and not the creation and curation of load-bearing
datasets.

This issue is well illustrated with the most contemporary trends of
Generative AI systems. Taking the development of Large Language Models
(LLMs) as a use-case: trends show massive increases in the requirements
of the training data, with the data for Llama 3 including 15T tokens, up
from 2T for Llama 2. Although not all details of the datasets have been
shared, and setting aside questions of copyright, licensing, and ethical
concerns, any available text that is likely to have some quality is
scarce compared to these requirements. For example, among the more
refined sources of text available are the 6.9M English Wikipedia
articles[^5] at an estimated 6.24B tokens[^6] - articles across all
languages comprise only approximately 10 times that amount. If we extend
our reach to other repositories, e.g. the approximately 85-90 million
academic papers available on Sci-hub[^7] or SCOPUS[^8], we gain a rough
estimate of 700B tokens. A similar figure might be estimated from
libegen and the 7.5M[^9] books there. The internet archive has some 44M
books and texts, which may yield up to 4.4T tokens if we assume 'texts'
have approximately the same length as books. Thus, a more likely source
for the ever-increasing data requirements are repositories of randomly
selected text from the internet, like Common Crawl[^10]. But this too
has limits, and we are projected to have too little human-generated text
to continue the increase in model size this decade, even if all of
Common Crawl is used [@villalobos2024position].

[^5]: According to
    [wikipedia](https://en.wikipedia.org/wiki/Wikipedia:Size_of_Wikipedia#:~:text=Monthly%20statistics-,Number%20of%20pages,pages%20are%20created%20than%20articles.)
    on 9 May, 2025.

[^6]: Using the conversion method of words to tokens recommended by
    [Open
    AI](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them).

[^7]: According to [sci-hub](https://www.sci-hub.se/about) on 9 May,
    2025.

[^8]: According to
    [SCOPUS](https://blog.scopus.com/posts/scopus-roadmap-whats-new-in-2022#:~:text=There%20are%20currently%2087%2B%20million,new%20articles%20per%20day%20indexed.)
    on 9 May, 2025.

[^9]: According to [The
    Atlantic](https://www.theatlantic.com/technology/archive/2025/03/libgen-meta-openai/682093/LibGen).

[^10]: According to [Common Crawl](https://commoncrawl.org/) on 9 May,
    2025.

Beyond the issue of merely acquiring data that exists, is the issue of
the thus-far-ignored cost of training data. GPT-4 cost an estimated 40M
USD to train, including human labor, hardware, and energy. Cost for
frontier models is projected to increase at a rate of 2.4 times per
year, reaching an estimated 1B USD to train by 2027
[@cottier2024rising]. Notably, these cost calculations ignore the cost
of producing the text itself, and though its value is difficult to
calculate, estimates range from 10-1000 times more than the total cost
of model training, and would exceed annual revenues of the organizations
that are training the models, like OpenAI [@kandpal2025position]. Thus,
the resources spent do not include the costs of producing training data,
for which lack of appropriate compensation has given rise to a number of
lawsuits (e.g. Authors Guild vs. OpenAI [^11]). In other words, the more
valuable thing is the data and not the model, and the few organizations
that have the resources to train the models may not have been able to
afford training if the data weren't acquired at a near-0 cost.

[^11]: https://authorsguild.org/app/uploads/2023/12/Authors-Guild-OpenAI-Microsoft-Class-Action-Complaint-Dec-2023.pdf

Thus, the field of AI must wrestle with opposing issues: we want to
train models with data that has high quality - e.g. we want the data to
be representative of conditions where the model will be deployed, and
thus relevant distributions in the training data must reflect the
environment in which the models will be deployed [@hullman2022worst] -
but the data requirements to train them appear thus far to be
ever-increasing [@villalobos2024position], and have thus far not
included the actual cost of production [@kandpal2025position]. We want
the data used in training to be 'good' because we want the models to be
'trustworthy' - in the case of LLMs, we want to have reason to think
they will generate text that includes claims that we think are true, and
thus likely prefer text that contains information that is likely to be
true. Perhaps our closest approximation to 'what is likely to be true'
is contained in the perspectives presented across all of academic work
and official reports. And yet, even if there were little to no barriers
to using all of human academic work to train AI systems, this amount of
text pales in quantity to the data requirements. And although thus far
some models have been made available for academic use (e.g. Meta's
Llama), it is not clear for how long this will be the case, and when, if
ever, academia will have the resources to train its own.

### Evaluation Data

There is substantial room for improvement in terms of the practices of
data labeling as well, however implementations of such practices are
also costly. The field of *AI metrics* distinguishes between a)
observable AI system behavior, and b) the targets of measurement. In
other words, a given score on a given 'AI' benchmark designed to measure
'intelligence' is not itself a direct measure of 'intelligence'. AI
metrics treats the indirectly observable phenomena as *computational
constructs* - indirectly observable aspects of AI system behavior, a
parallel to *constructs* in other fields - indirectly observable
phenomena [@gignac2024defining]. Thus the benchmark itself is a
measurement instrument, aimed at measuring a specific aspect of the AI
system.

The process that gives rise to the labels in the data that, which
include the annotation interface and task instructions shown to
annotators, is also a measurement instrument [@beck2022improving]. The
case study in this thesis contributes to the field of AI metrics by
proposing enriching design, collection, analysis, and reporting of
training/evaluation data for AI systems, using knowledge from the social
sciences [@beck2022improving, @jacobs2021measurement], metrology
(measurement science) [@welty2019metrology], and work in the
computational sciences on 'ground-truthing' [@cabitza2023toward]. It
requires a higher cost in terms of scientific labor, as it includes an
*a-priori* empirical investigation of the data collection process in
addition to the data collection process itself. In other words, it
requires research on *how to collect the data*. Applying the knowledge
to other tasks would require similar research, in principle for every
combination of *construct* (i.e. the latent phenomenon of interest being
measured), *content* (e.g. text, video, audio etc. and in some cases
also subgroups, e.g. tweets vs. podcast transcripts vs. formal speeches
etc.), and for relevant *characteristics* of annotators (i.e. ethnicity,
political affiliation, etc.). These costs are in addition to the
annotation labor.

Although it is clear there is substantial room for improvement along
with the repercussive benefits from such improvements, they will require
resources. Despite growing recognition of evaluation data problems
[@hullman2022worst], which in turn have the potential to lead to harms
[@mehrabi2021survey], actual progress toward better annotation data
practices, from collection, to analysis, to reporting remains slow.
Current incentives prioritize immediately observable proxies for
accuracy [@birhane2022values] rather than careful measurement design
[@beck2022improving] or long-term dataset stewardship [@liem2023treat].
Beyond the challenge of gathering data at the necessary scale, are
issues about the quality of the data itself, specifically the quality of
the overall process that created the data. This challenge is essentially
one of measurement [@welty2019metrology].

Datasets for evaluation very often contain human input
[@geiger2020garbage; @geiger2021garbage; @daneshjou2021lack;
@sav2023annotation]. When collecting annotations, labels, or other forms
of input from people in order to construct training/evaluation datasets,
we are attempting to collect measurements of latent, unobservable
*constructs* [@jacobs2021measurement; @gignac2024defining]. In other
words, when we examine data created from the observations of multiple
people in aggregate, we do not directly observe e.g. the presence or
absence of an object in a digital image; rather we observe the
probability that a person from a given population will indicate the
presence of absence of the object in the image [@welty2019metrology]. In
the parlance of psychology, one cannot directly observe an other's
intelligence as one might observe an other's height
[@gignac2024defining]. And even though height is observable, our
measurements of it are still imperfect: in using a measurement device
like a ruler multiple times, should we measure precisely enough, we
would observe variance in each measurement with the true score for
height imperfectly represented by our collection of imperfect
measurements [@welty2019metrology].

Any standardized procedure for comparing two or more individuals is
treated like a measurement instrument in the social sciences
[@urbina2014essentials]. The repeatable procedures that we use to gather
annotations from humans are similarly measurement instruments
[@beck2022improving]. In addition, because we use the datasets that we
collect to benchmark the performance of models, they are also
measurement instruments [@welty2019metrology]. Given the complexity of
measuring unobservable phenomena, instruments in other fields are
subjected to scrutiny prior to being considered usable for their
intended purpose, in the form of studies that examine the properties of
the measurements produced by the instrument. For example, the process of
*construct validation* involves estimating the extent to which an
instrument measures an unobservable construct [@Wehner2020]. It assumes
an unknowable true score, and that all attempts to measure the true
score are imperfect. There is no single solution to demonstrating the
validity of a construct, but rather an accumulation of evidence, across
multiple studies, with observations made using different methods
[@smith2005construct].

Thus, the field of AI must wrestle with the opposing pressure of
accuracy vs. the cost of developing measurement instruments for
practically infinite use-cases: we want measurements that are
'accurate' - i.e. we want the data to be as clear a representation of
our phenomenon of interest as it could be - but developing an accurate
instrument is a costly exercise to add to the cost of collecting the
data itself. Further, the topics we wish to measure in the
ever-increasingly-complex AI systems as themselves measurement
challenges. Topics like 'fairness' [@jacobs2021measurement], and
'intelligence' [@gignac2024defining] are not only challenging to develop
instruments for, but also to define.

## Labor Challenges

### Scientific Labor

Addressing the aforementioned data challenges will require substantial
additional efforts. However our current knowledge gathering apparatus -
science as it is now practiced and reported - is dated, and inefficient.
Thus, a substantial amount of the labor will likely have to come from an
already overburdened work force. A re-consideration of what duties
comprise academic work is thus warranted, specifically those that
contribute to the body of human knowledge, and whether they can be
modernized.

One key example involves a academic writing and publication,
specifically a primary source of scientific labor in academic settings,
the PhD student, and their requirements to produce a dissertation or
thesis document in order to progress in their career. PhD students are
responsible for as much as a third of all academic output
[@lariviere2012shoulders], likely by working long hours, including
weekend hours, constantly under time-pressure to produce output
[@van2024caught]. Unsurprisingly, PhD students are a vulnerable
population, showing a high prevalence of depression and anxiety - as
high as 24%, and 17% respectively in a recent meta-analysis
[@satinsky2021systematic]. Like all early career researchers (ECRs, i.e.
PhD students, Post Doctoral Researchers, and Assistant Professors), PhD
students, contribute substantially to the body of human knowledge via
academic publication [@rorstad2015publication].

However, PhD candidates are bogged down by dated requirements to write
and defend theses despite the declining relevance of the document, and
the contribution it makes to poor outcomes for the student. For example,
the trend of thesis citations over time shows decline
[@lariviere2008declining]. In the Netherlands where this thesis will be
defended, evidence has shown that PhD candidates with fixed duration
contracts are exceeding that duration by several months, resorting to
completing their thesis on their own time and risking failing at
completion [@van2013took]. Though a key ceremonial moment, a thesis
defense is substantially less rigorous than a publication: it is
curated, as the 'peer reviewers' are chosen by the supervisors of the
student - in the 'real world' of academia, on the other hand, peer
reviewers are selected from far broader networks than those of the PhD
candidate's supervisory staff [@lariviere2012shoulders]. It is thus not
surprising that publications, and not theses, remain the key factor in
the assessment of the value of Academics as scientists
[@anderson2022effect]. Although more and more thesis content is being
comprised of academic publications anyway (i.e. *thesis by publication*,
@jackson2013completing), the substantial labor required to assemble
works into a single document, write additional (introductory /
conclusiory) chapters that are themselves complete manuscripts or nearly
so, then help organize a formal event for the defense take away from
more meaningful labor that this workforce could provide.

A second key example involves a cornerstone of trust in science: the
editorial peer-review process dates back several hundred years, and
exists as a means to encourage and maintain the quality of scholarly
work [@kelly2014peer]. Early career and experienced researchers alike
contribute to peer-review[^12], labor conservatively valued at over 1.5B
USD in 2020 [@aczel2021billion]. ECRs perform a substantial amount of
the work often without credit[^13], and in some cases perform the work
completely on their own [@mcdowell2019co], especially in high-volume
conferences like NeurIPS [@shah2018design]. This is a crucial note, as
the submissions to high-volume conferences show massive increases year
over year[^14] and struggle to meet the review requirements: e.g.
NeurIPS 2021 required outreach to recruit over 1k additional
peer-reviewers beyond the over 7k initial volunteers, to produce the 31k
reviews needed for the conference[^15]. As conferences continue to grow,
the amount of review work is being divided among a regularly stable
labor pool, reducing the attention paid to each individual work, a
problem exacerbated by the repeat submission of borderline papers - i.e.
papers that were nearly accepted [@Zhang_2022]. This results in
important flaws being missed: e.g. [@kapoor2023leakage] show a growing
crisis across 17 Machine Learning fields from a lack of trustworthy
results due to data leakage.

[^12]: https://www.elsevier.com/connect/more-early-career-researchers-are-stepping-up-to-peer-review

[^13]: https://elifesciences.org/inside-elife/982053f4/early-career-researchers-views-on-peer-review

[^14]: https://github.com/tranhungnghiep/AI-Conference-Info?utm_source=chatgpt.com

[^15]: https://neuripsconf.medium.com/what-we-learned-from-neurips-2020-reviewing-process-e24549eea38f

A third key example involves the ever-increasing volume of published
manuscripts on AI and related topics[^16], which makes it impossible to
stay abreast of the overall field. 10% of over 4 million publications
indexed on the SCOPUS academic database in 2024 had terms related to AI
in their title, keywords or abstracts (see Appendix A), up from around
7% in 2022. The number of submitted manuscripts also increases year over
year, with popular conferences like NeurIPS receiving upwards of 12K
submissions in 2023[^17]. These figures do not include preprints posted
on servers like arXiv, which show over 42K works with AI related terms
in the abstract for 2024, more than doubling the about 17.5K posts in
2019. This makes it more and more difficult to monitor the overall
quality of the field [@Zhang_2022].

[^16]: terms included AI, artificial intelligence, machine learning, and
    neural networks. See Appendix for specific search strings.

[^17]: https://github.com/tranhungnghiep/AI-Conference-Info?utm_source=chatgpt.com

This overburden raises questions beyond the poor evaluation of the works
that underlie our current understanding of 'AI' to the 'AI' research
process itself, as well as to its likelihood of applying improvements.
Predictably, and perhaps understandably, academics appear to be turning
to Large Language Models (LLMs) for assistance, as evidence of its use
is showing in academic work in both peer reviews [@liang2024monitoring],
and in manuscripts [@gray2024chatgpt]. This is problematic as LLMs often
report information that is not factual [@wang-etal-2024-factuality].
However, a more direct solution might be to re-think how cornerstone
components of academic work are conducted.

### Annotation Labor

It has been reported that Open AI hired a San Francisco-based firm that
sourced annotation labor from Kenya, Uganda, and India to provide the
human inputs necessary to fine tune their models [^18]. Despite the rate
of pay being far less than the federal minimum wage in the US, ranging
from to , OpenAI spent 600k USD in 2021 to label text as being violent,
sexual, or hatespeech alone. Commonly used platforms like mTurk[^19],
and Prolific[^20] pay at or near minimum wage in the US. While the
availability of such services has allowed for rapid gathering of
evaluation data for AI-related projects, works that highlight data
scarcity warn that it may be insufficient to meet the needs of coming
models.

[^18]: https://www.business-humanrights.org/pt/%C3%BAltimas-not%C3%ADcias/openai-and-sama-hired-underpaid-workers-in-kenia-to-filter-toxic-content-for-chatgpt/

[^19]: https://en.wikipedia.org/wiki/Amazon_Mechanical_Turk

[^20]: https://www.prolific.com/participants

Work in the case study of this thesis further suggests that *more*
annotation work may be required than is used for current projects, as it
requires a focus on the development of an annotation instrument prior to
primary data collection phases, pre-studies to estimate the required
number of annotators, and accounting for possible variance in
perspectives. For high-stakes use-cases, such as applications in
government or healthcare, we may wish to have data independently
examined prior to its certification for use, similarly to how some
scholars recommend external examination of AI models prior to deployment
[@garrett2024testing], which may in turn require further iterations of
data collection.

## Reporting challenges

### Volume and synthesis

Receiving declining attention are theses. Components currently largely
absent are peer-reviews (although open review tries to address this) and
rejected manuscripts - many of which get resubmitted \[citation\].
Partially absent are teaching materials, conference materials, e.g.
posters, slides, and recordings of talks etc. Importantly, code scripts
and/or notebooks, various forms of data, parameters and settings, and
the actual model(s) produced are often absent. This also doesn't include
replications, which have been shown to be useful in other fields. Only
recently are there readily available guidelines to more completely
report key information: data/model \| sheets / cards,
[@geiger2021garbage].

Many of the outputs of academic work would benefit from some sort of
maintenance. i.e. we must treat academic insights like software
artifacts, with management and possibly updates.

Reviewer quality, and the difficulty in finding reviewers.

## A contemporary field

### AI Metrics should be a field

Data used for training and evaluation of models is of central
importance. Solutions to problems in both training and evaluation data
collection require deliberate study for standards to improve and
disseminate. In other words, beyond the implementation of currently
thought best practices, is the requirement that the best practices be
improved along with demands, and knowledge of them disseminated such
that general practice is affected.

As it stands, there is no central field of study for this topic, despite
its central importance to all AI-related fields. Other disciplines
(psychology, economics, software engineering) have entire fields
dedicated to the design, collection, analysis, and reporting of data
that involves human behavior (psychometrics, econometrics, software
testing). Although datasets that become 'load bearing' are frequently
cited e.g. Imagenet, AI Metrics itself is not a central focus at the
most highly cited academic publication venues, nor is there a focused
publication venue or conference. Thus, crucial topics remain
under-resourced and under-researched.

### Collaboration to address data requirement challenges

The human labor responsible for the collection, curation, and eventual
annotation of training data in addition to the training of the model
(including researchers, engineers, and managers, but not data center
employees and operations staff) is estimated at 29%-49% of the overall
cost, which already amounts to tens of millions for frontier models
[@cottier2024rising]. Thus, a key challenge to implementing better
practices involves providing labor both in terms of the scientific work
necessary to design data collection and annotation processes, but also
to provide the annotations themselves.

One approach to both gathering the necessary training data at scale as
well as the labels or annotations have been shown in collaborations
between scientists and the general public. Online platforms host and
facilitate the creation of various resources, ranging from media and
other forms of data, labels and annotation projects, as well as forums
for discussion. For example inaturalist.org is an online community with
over 8 million users who make contributions in the form of images taken
on their smartphones, and/or labels of the species in the images
[@van2018inaturalist]. Zooniverse.org is an online community of over
2.8M users that hosts projects defined by scientists to gather labels
from non-scientists [@fortson2012galaxy]. A third example is
commonvoice.mozilla.org/en, which is a large dataset of speech
transcription in 76 languages, provided by approximately 150k
participants [@ardila-etal-2020-common]. People are willing to even
share de-identified medical data as long as they are given agency over
it [@liang2022advances]. This approach simultaneously allows for the
contribution of diverse media for annotation, labor needed to annotate
them.

Another similarly scalable infrastructure for dataset creation within
academia might be possible by adapting an academic publication format
called the Registered Report [@chambers2013registered]. Initially
designed to compensate for editorial decisions being made based on the
results, rather than the quality of the methods. In many fields, aspects
of the data collection design, as well as the design of analysis and
prediction of results occur *a-priori*, in principle not to bias
interpretation of results. In a Registered Report, researchers submit a
manuscript that includes information relevant to how the study will be
conducted, including motivation of the work (i.e. introduction), details
of data collection processes, as well as analyses. Typical review stages
apply, i.e. suggestions for revisions or rejections, or the manuscript
may receive an *in-principle acceptance*, whereby reviewers and editor
agree to a publication should the methods used in the manuscript either
follow closely the in-principle accepted version, or appropriate
justifications be made for any changes that may have occurred. Thus
acceptance of publications is made based on the strength of the methods,
which also are strengthened by a peer-review process prior to data
collection.

The Registered-Report format is exceptionally well-suited to the
collection of datasets intended for AI training and/or evaluation.
Firstly, they allow for peer-review prior to collection, whereby a panel
of experts will provide critiques that will either strengthen the
eventual design, or reject it in favor of publishing other stronger
designs. Given the scope and resources needed to collect AI datasets,
this format could be adapted such that it is published in its entirety
prior to data collection. This may thus allow for a more public critique
of the design prior to paying the resource cost of collection, and
further allow for the submissions of responses in the form of data that
conforms to the design in the published manuscript in a decentralized
fashion, from multiple stakeholders, thus reducing the bias from any
single data collection point and allowing for the sharing of financial
and other resource burdens. As online platforms focused on different
topics proliferate, they may be attached to approved data collection
protocols.

A further approach to contributing more scientific labor involves
undergraduate bachelor and masters students. CREP is a crowdsourced
initiative where undergraduate students, under faculty supervision,
replicate high-impact psychology studies, thus allowing for direct
instruction of students while provided needed replications of pivotal
studies [^21]. These replications are pre-registered, and may be
published. For instance, a meta-analysis of nine student-led
replications of the "red-romance effect" found no significant effect
[@wagge2019demonstration], thus functioning both to instruct students
and contribute to the scientific record. Similar approaches can be taken
towards annotating data, with students replicating registered data
collection protocols, or supplying the annotations themselves.

[^21]: [Collaborative Replications and Education
    Project](CREP)(https://osf.io/wfc6u/)

### Contemporary Approaches to Academic publication

OSF.io and github have become repositories for materials. Open Review
allows for peer reviews of works, as well as a trail of the reviews.

Living systematic reviews are useful for rapidly evolving fields.
Notably, works are appearing that aim to develop AI tools to assist in
the writing of such reviews by assisting with screening \[\].

## Concluding Call to Action

Better data design enables better science, more responsible innovation,
and safer, more meaningful AI systems, but is costly, even in comparison
to the current amount of effort and funding dedicated to AI (and related
fields) research. A key priority is that the AI/ML research community,
conferences, funding agencies, and publishers must recognize data
research, analysis as well as creation, as a research contribution.New
formats like data-focused Registered Reports and micropublications
should be adopted to 1) distribute efforts, and 2) aggregate knowledge.

Further, we must rethink how we allocate labor. Load-bearing datasets
require more effort than the initial collection and subsequent
distribution, but also maintenance. This beyond research required to
create useful datasets, we also require resources to maintain them.

# Appendix {#sec-appendix}

## Appendix A: Citation Trends Plot

```{r publication_plots, echo=FALSE,message=FALSE,warning=FALSE}
library("here")
library("tidyverse")
library('patchwork')
library('viridis')

my_colors <- viridis(n=10, option="magma")

# query : (TITLE-ABS-KEY((((machine OR deep OR reinforcement OR supervised OR unsupervised) AND learning) OR (""neural networks"") OR (AI or ""artificial intelligence""))) AND PUBYEAR > 1999 AND PUBYEAR < 2027)

ml_pub_df <- read_csv(here("data", "Scopus-10-Analyze-Year.csv")) %>% mutate(type = "ml_papers")

#query : (PUBYEAR > 1999 AND PUBYEAR < 2027)
overall_pub_df <- read_csv(here("data", "Scopus-10-Analyze-Year (1).csv")) %>% mutate(type = "overall")

pub_df <- bind_rows(ml_pub_df, overall_pub_df)

p1 <- pub_df %>% filter(YEAR<2025) %>%
  ggplot(aes(YEAR, citations, color = type, group = type)) +
  geom_line(linewidth = 1.2) +
  geom_point(size = 2) +
  scale_color_manual(values=c(my_colors[3], my_colors[7])) +
  scale_y_continuous(labels = scales::comma) +
  labs(title = "SCOPUS Publication #:",
       subtitle="ML vs. Overall",
           x = "Year",
           y = "Number of Publications",
       color = "Category") +
  theme_minimal(base_size = 12) +
  theme(legend.position = "top")

p2 <- pub_df %>% pivot_wider(names_from = type, values_from = citations) %>%
  mutate(proportion = ml_papers/overall*100) %>% filter(YEAR<2025) %>%
  ggplot(aes(YEAR, proportion)) +
  geom_line(linewidth = 1.2, color=my_colors[5]) +
  geom_point(size = 2,, color=my_colors[5]) +
  labs(title = "SCOPUS Publication %",
       subtitle="ML",
           x = "Year",
           y = "Percentage of Publications") +
      theme_minimal(base_size = 12)
  
(p1+p2)
```

## Appendix B: Search terms

### SCOPUS:

for AI related topics: TITLE-ABS-KEY ( ( ( ( machine OR deep OR
reinforcement OR supervised OR unsupervised ) AND learning ) OR (
"neural networks" ) OR ( ai OR "artificial intelligence" ) ) ) AND
PUBYEAR \> 1999 AND PUBYEAR \< 2027

for overall publication records: PUBYEAR \> 1999 AND PUBYEAR \< 2027

### arXiv:

\[Abstract\] AI or "artificial intelligence" OR machine AND learning OR
supervised AND learning OR reinforcement AND learning OR neural AND
networks

17,459 results in 2019 23,923 results in 2020 27,610 results in 2021
29,690 results in 2022 33,419 results in 2023 42,183 in 2024

## Appendix C: Token estimates

Taking a study on the word length requirements of education journals as
a proxy, @fairbairn2009profile report the following figures:

```{r token_counts,echo=FALSE,message=FALSE,warning=FALSE}
paper_lengths <- data.frame(
  TokenRange = factor(c("<3000", "3000-3900", "4000-4900", "5000-5900", 
                 "6000-6900", "7000-7900", "8000-8900", 
                 "9000-9990", ">=10000"), 
                 levels = c("<3000", "3000-3900", "4000-4900", "5000-5900", 
                 "6000-6900", "7000-7900", "8000-8900", 
                 "9000-9990", ">=10000")),
  Count = c(22, 36, 53, 114, 108, 85, 56, 12, 31)
)

paper_lengths %>%
  ggplot(aes(x=TokenRange, y=Count, fill=Count)) +
    geom_point(shape = 21, size = 5) +
    theme_minimal() +
    scale_fill_viridis(option="plasma") +  
  labs(title = "Distribution of Token Counts in Papers",
       x = "Token Range",
       y = "Count") + theme(
    axis.text.x = element_text(angle = 45, hjust = 1)
  )
```

```{r}
# Estimate midpoints for each bin
midpoints <- c(2500, 3450, 4450, 5450, 6450, 7450, 8450, 9500, 10500)

# Add midpoints to the dataframe
paper_lengths$Midpoint <- midpoints

# Estimate mean token count
mean_est <- sum(paper_lengths$Midpoint * paper_lengths$Count) / sum(paper_lengths$Count)

# Calculate weighted variance
var_est <- sum(paper_lengths$Count * (paper_lengths$Midpoint - mean_est)^2) / sum(paper_lengths$Count)

# Take square root to get standard deviation
sd_est <- sqrt(var_est)

# Print result
paste()
print(paste("SD words: ", sd_est))
print(paste("Mean words: ", mean_est))
```

https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them?utm_source=chatgpt.com
According to Open AI, a token is 3/4 of a word

```{r}
sd_tokens <- sd_est*1.25
mean_tokens <- mean_est*1.25 

print(paste("SD Tokens: ", round(sd_tokens, 2)))
print(paste("Mean Tokens: ", round(mean_tokens, 2)))

paste( round(((mean_tokens-sd_tokens) * 4500000) / 1000000000, 2), 
       " Billion to", 
       round (((mean_tokens+sd_tokens) * 4500000) / 1000000000, 2), 
       "Billion tokens per year from academia.")

paste( 4.8*1.25, "Billion tokens from Wikipedia.")
```

Wikipedia:
https://en.wikipedia.org/wiki/Wikipedia%3ASize_of_Wikipedia?utm_source=chatgpt.com

```{r}
5000000000/2000000000000*100
```

Sci Hub: https://www.sci-hub.mk/ 88343822 documents as of 9 May 2025
13:47.

```{r}
(7927.71 * 88343822) / 1000000000 
```

https://www.theatlantic.com/technology/archive/2025/03/libgen-meta-openai/682093/
LibGen

7.5 million books and 81 million research papers.

```{r}
# at about 100k words per book
(100000*7500000) / 1000000000

(100000*44000000) / 1000000000
```

Llama 2 was 2T tokens, Llama 3 was 15T. I didn't really see where they
got their data from. So I made some guesses.

Looked at Wiki. The whole thing is like 5B.

Sci-Hub, 88.3M papers, which at 8kish tokens is an upper boundary of
700B or so.

7.5 million books on libgen you get about another 700B or so, at
100k-ish per words.

Internet archive has some 44 **million** books. 4.4T.
