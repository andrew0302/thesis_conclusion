[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Need for a Contemporary Field",
    "section": "",
    "text": "AI systems are a direct product of the data used to train and evaluate them. We shape the behavior of AI systems with the processes we use to design, gather, analyze, and report the training and evaluation data. Work over the past decade has emerged explaining limitations of commonly re-used datasets, with many commonly used ‘benchmark’ datasets showing a lack of representativeness (Hullman et al., 2022), measurement quality (Jacobs & Wallach, 2021), accounting for the full range of reasonable interpretations in terms of annotations (Cabitza et al., 2023), and completeness in reporting of the annotation process (Geiger et al., 2020, 2021). Although improvements to data collection processes have been proposed, they are at best slowly being adopted, and at worst being largely ignored. This issue is further exacerbated by the increasingly sophisticated qualities we aim to evaluate systems on, that in turn are challenging to define and measure (e.g. ‘fairness’) (Jacobs & Wallach, 2021). Furthermore, the human-like behavior of the sophisticated systems we are evaluating are a prime target for anthropomorphistically biased mis-interpretations of their outputs, as many aim to behave similar to humans (Altmeyer et al., 2024). The future qualities of AI systems will be shaped by the data practices we establish today: the degree to which we invest in better design, collection, and analysis of training and evaluation data will determine the real-world performance of the AI systems we will build. It is thus crucial that we put in place better practices.\nDeveloping better data design, collection, and analysis practices requires substantial efforts. As an example, the case study in this thesis proposes enriching design, collection, analysis and reporting of training/evaluation data for AI systems, using knowledge from the social sciences (Beck et al., 2022), metrology (measurement science) (Welty et al., 2019), and work in the computational sciences on ‘ground-truthing’ (Cabitza et al., 2023). It requires a-priori empirical investigation of the data collection process in addition to the data collection process itself, in principle for every combination of construct (i.e. the latent phenomenon of interest being measured), content (e.g. text, video, audio etc. and in some cases also subgroups, e.g. tweets vs. podcast transcripts vs. formal speeches etc.), and for relevant characteristics of annotators (i.e. ethnicity, political affiliation, etc.). Open questions remain for the primary case-study as well as for the field as a whole, all of which anticipate future studies, and by extension further efforts.\nQuestions remain as to how AI research as it is currently conducted can implement these solutions. As it stands, our current knowledge gathering apparatus - science as it is now practiced and reported - is overburdened. The ever increasing volume of published manuscripts on AI and related topics1 makes it impossible to stay abreast of the overall field: 10% of over 4 million publications indexed on the SCOPUS academic database in 2024, up from around 7% in 2022, had terms related to AI in their title, keywords or abstracts (see Appendix A). The number of submitted manuscripts also increases year over year, with popular conferences like NeurIPS receiving upwards of 12k submissions in 20232. This makes it more and more difficult to find reviewers, and by extension to monitor the overall quality of the field (Zhang et al., 2022). These figures do not include preprints posted on servers like arXiv, which show over 42k works with AI related terms in the abstract for 2024, more than doubling the about 17.5k posts in 2019. PhD candidates, who contribute a substantial proportion of academic work (Larivière, 2012), are bogged down by requirements to write and defend theses despite the decreasing trend of thesis citations over time (Larivière et al., 2008), and evidence that PhD candidates with fixed duration contracts exceed that duration by several months, resorting to completing their thesis on their own time and risking failing at completion (Van de Schoot et al., 2013). Further, publications, and not theses, remain the key factor in the assesment of their value as scientists (Anderson et al., 2022) - efforts which could be put towards meeting some of this labor gap. Predictably, academics appear to be turning to Large Language Models for assistance, as use is showing in academic work in both peer reviews (Liang et al., 2024), and in manuscripts (Gray, 2024). This overburden raises questions beyond the poor evaluation of the models that underlie ‘AI’ to the ‘AI’ research process itself, as well as to its likelihood of applying improvements.\nThis conclusiory manuscript thus highlights the crucial challenge for academic study of AI in the coming decade: developing an infrastructure that allows for the study of AI, including the data that are its raw materials, with little - or at the very least, substantially less - harmful bias. It highlights the need for identifiable academic publication venues that gather works on the study of ground-truthing, more modern publication formats that allow for dataset requirements to be studied prior to their collection, and for infrastructure that allows the burden of their collection to be distributed among stakeholders. It concludes that, while works like the case study embedded in this thesis are necessary, the various fields studying topics related to AI are poorly positioned to implement them."
  },
  {
    "objectID": "index.html#the-problem-ground-truth-needs-more-than-good-will",
    "href": "index.html#the-problem-ground-truth-needs-more-than-good-will",
    "title": "thesis_conclusion",
    "section": "",
    "text": "Despite growing recognition of dataset problems, actual progress toward better ground-truth data practices remains slow.\nCurrent incentives prioritize speed, performance, and novelty rather than careful measurement design or long-term dataset stewardship.\nThe cost of collecting meaningful, high-quality datasets — particularly for complex constructs like values, intentions, or intelligence — is underappreciated and underfunded. Collecting good data is expensive, but neglecting this cost leads to cascading problems in AI evaluation.\nWithout better practices, datasets remain opaque, unexamined, and prone to embedding systemic biases from the start."
  },
  {
    "objectID": "index.html#lessons-from-other-fields-registered-reports-and-the-science-of-measurement",
    "href": "index.html#lessons-from-other-fields-registered-reports-and-the-science-of-measurement",
    "title": "A Need for a Contemporary Field",
    "section": "Lessons from Other Fields: Registered Reports and the Science of Measurement",
    "text": "Lessons from Other Fields: Registered Reports and the Science of Measurement\n\nIn fields like psychology and medicine, Registered Reports emerged to separate the design of a study from its results, reducing biases like hindsight bias and outcome switching.\nOther fields actively study how to measure complex constructs: e.g., psychometrics for cognitive ability, epidemiology for disease burden, and criminology for recidivism. These fields show that developing good measurement instruments is a dedicated scientific effort — not a side activity.\nIf AI research depends critically on ground-truth data, then the field needs a dedicated research agenda focused on ground-truth design and measurement science. Beyond lessons learned in other fields are ground-truthing specific questions."
  },
  {
    "objectID": "index.html#biases-in-agi-research-highlight-the-need-for-careful-grounding",
    "href": "index.html#biases-in-agi-research-highlight-the-need-for-careful-grounding",
    "title": "A Need for a Contemporary Field",
    "section": "Biases in AGI Research Highlight the Need for Careful Grounding",
    "text": "Biases in AGI Research Highlight the Need for Careful Grounding\n\nIn our ICSE-SEIS 2023 paper, we critique unscientific performance claims in AGI-related work.\nAGI research is especially vulnerable to confirmation bias, wishful thinking, and premature performance claims without rigorous benchmarks.\nDefining the constructs we aim to measure (e.g., “intelligence” in LLMs) must be a scientific task in itself. Intelligence may manifest differently in AI systems than in humans, requiring new conceptualizations and new measurement instruments.\nWithout careful construct definition and measurement, claims about AGI capabilities risk being scientifically meaningless."
  },
  {
    "objectID": "index.html#micropublication-models-capturing-data-collection-as-a-first-class-output",
    "href": "index.html#micropublication-models-capturing-data-collection-as-a-first-class-output",
    "title": "A Need for a Contemporary Field",
    "section": "Micropublication Models: Capturing Data Collection as a First-Class Output",
    "text": "Micropublication Models: Capturing Data Collection as a First-Class Output\n\nMicropublications are modular, peer-reviewed publications that focus on specific research artifacts like datasets, annotation protocols, or measurement plans.\nExtending the Registered Reports model, we can approve data collection protocols before data is collected, including sampling design, measurement instruments, and annotation strategies.\nDecentralized contributors (e.g., multiple labs or individuals) could publish individual datasets under a shared, peer-reviewed protocol.\nThis would move ground-truth collection toward transparent, modular, and cumulative science, rather than isolated, one-off efforts."
  },
  {
    "objectID": "index.html#building-infrastructure-for-transparent-ground-truth",
    "href": "index.html#building-infrastructure-for-transparent-ground-truth",
    "title": "A Need for a Contemporary Field",
    "section": "Building Infrastructure for Transparent Ground Truth",
    "text": "Building Infrastructure for Transparent Ground Truth\n\nOur SEIS 2023 paper pushes for open, linked, reproducible artifacts, not just final models or results.\nOur CHI 2023 paper critiques the opacity of machine learning artifacts and calls for linking data, documentation, and transparent evaluation processes.\nAlexandria:\n\nA web platform combining Wikipedia-style collaborative editing with GitHub-style version control.\nSupports the CREDIT taxonomy for structured contributor recognition.\nProof of concept: Built almost entirely by student developers under supervision, showing that decentralized academic innovation is possible.\nA few additional steps would yield a fully operational infrastructure supporting micropublication of ground-truth artifacts."
  },
  {
    "objectID": "index.html#the-vision-toward-responsible-ground-truth-for-ai-and-agi",
    "href": "index.html#the-vision-toward-responsible-ground-truth-for-ai-and-agi",
    "title": "A Need for a Contemporary Field",
    "section": "The Vision: Toward Responsible Ground Truth for AI and AGI",
    "text": "The Vision: Toward Responsible Ground Truth for AI and AGI\n\nResearchers pre-register their ground-truth collection plans, including constructs, instruments, and expected properties of the data.\nPeer-reviewed protocols are made public before data collection.\nAnnotators, coders, and dataset curators are properly credited through micropublications.\nDatasets grow openly, collaboratively, with tracked provenance and version control.\nThis infrastructure builds trustworthy, reproducible foundations for the next generation of AI and AGI research."
  },
  {
    "objectID": "index.html#concluding-call-to-action",
    "href": "index.html#concluding-call-to-action",
    "title": "A Need for a Contemporary Field",
    "section": "Concluding Call to Action",
    "text": "Concluding Call to Action\n\nThe AI/ML research community, conferences, funding agencies, and publishers must recognize ground-truth data creation as a first-class research contribution.\nNew formats like data-focused Registered Reports and micropublications should be adopted.\nBetter data design enables better science, more responsible innovation, and safer, more meaningful AI systems.\n\nIt’s the ‘real’ world of publication that matters. As Larivière (2012) note, a thesis defense is a more curated experience as the ‘peer reviewers’ are chosen by the supervisors of the student - on the other hand, peer reviewers in the world of academic publication are far broader than the networks of the PhD candidate’s supervisory staff. One study in Canada in 2012 showed that one third of all academic output comes form PhD students (Larivière, 2012). One study showed the decline of citations of PhD theses over time (Larivière et al., 2008). Perhaps there are other, more productive ways to contribute rather than taking the time to write a thesis."
  },
  {
    "objectID": "index.html#the-problem-ground-truthing-requires-focus",
    "href": "index.html#the-problem-ground-truthing-requires-focus",
    "title": "thesis_conclusion",
    "section": "",
    "text": "-Despite growing recognition of dataset problems, actual progress toward better ground-truth data practices, from collection, to analysis remains slow. - Current incentives prioritize speed, performance, and novelty rather than careful measurement design or long-term dataset stewardship.[maybe write instead what’s good about current work - i.e what have they prioritized so far] - The cost of collecting meaningful, high-quality datasets — particularly for complex constructs like values, intentions, or intelligence — is underappreciated and underfunded. [Thesis contributions here.] Whether a simple or complex ground-truthing problem, treating the phenomenon of interest like a construct has benefits. Collecting good data is expensive, but neglecting this cost leads to cascading problems in AI evaluation. - Without better practices, datasets remain opaque, unexamined, and prone to embedding systemic biases from the start."
  },
  {
    "objectID": "index.html#appendix-a-citation-trends-plot",
    "href": "index.html#appendix-a-citation-trends-plot",
    "title": "A Need for a Contemporary Field",
    "section": "Appendix A: Citation Trends Plot",
    "text": "Appendix A: Citation Trends Plot"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "A Need for a Contemporary Field",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nterms included AI, artificial intelligence, machine learning, and neural networks. See Appendix for specific search strings↩︎\nhttps://github.com/tranhungnghiep/AI-Conference-Info?utm_source=chatgpt.com↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html#increasing-data-quality-requires-focused-study",
    "href": "index.html#increasing-data-quality-requires-focused-study",
    "title": "A Need for a Contemporary Field",
    "section": "Increasing Data Quality Requires Focused Study",
    "text": "Increasing Data Quality Requires Focused Study\nDatasets for evaluation very often contain human input (Geiger et al., 2020, 2021). Unlike the field of Machine Learning, other disciplines (psychology, economics, software engineering) have entire fields dedicated to the design, collection, analysis, and reporting of data that involves human behavior (psychometrics, econometrics, software testing). Despite growing recognition of dataset problems (Hullman et al., 2022), which in turn have the potential to lead to harms (Mehrabi et al., 2021), actual progress toward better ground-truth data practices, from collection, to analysis remains slow. Current incentives prioritize accuracy and efficiency rather than careful measurement design or long-term dataset stewardship (Birhane et al., 2022).\nWhen collecting annotations, labels, or other forms of input from people in order to construct training/evaluation datasets, we are attempting to collect measurements of latent, unobservable constructs (Jacobs & Wallach, 2021). In other words, when we consider the input from multiple people in aggregate, we do not directly observe e.g. the presence or absence of an object in a digital image; rather we observe the probability that a person from a given population will indicate the presence of absence of the object in the image (Welty et al., 2019). In the parlance of psychology, one cannot directly observe an other’s Extraversion score, as one might observe an other’s height. Although height is observable, our measurements of it are still imperfect: in using a measurement device like a ruler multiple times, should we measure precisely enough, we would like observe variance in each measurement, with the true score for height imperfectly represented by our imperfect measurements (Welty et al., 2019).\nAny standardized procedure for comparing two or more individuals is treated like a measurement instrument in the social sciences (Urbina, 2014). The repeatable procedures that we use to gather annotations are similarly measurement instruments (Beck et al., 2022). Given the complexity of measuring unobservable phenomena, instruments are subjected to scrutiny prior to being considered usable for their intended purpose. The process of construct validation involves estimating the extent to which an instrument measures an unobservable construct (Wehner et al., 2020). It assumes an unknowable true score, and that all attempts to measure the true score are imperfect. Thus there is no single solution to demonstrating the validity of a construct, but rather an accumulation of evidence, across multiple studies, with observations made using different methods (Smith, 2005). Thus, by extension the datasets that we collect and use to benchmark the performance of models, are similarly measurement instruments (Welty et al., 2019).\nThe cost of collecting meaningful, high-quality datasets — particularly for complex constructs like values, intentions, or intelligence — is underappreciated and underfunded. [Thesis contributions here.] Whether a simple or complex ground-truthing problem, treating the phenomenon of interest like a construct has benefits. Collecting good data is expensive, but neglecting this cost leads to cascading problems in AI evaluation.\nWithout better practices, datasets remain opaque, unexamined, and prone to embedding systemic biases from the start."
  },
  {
    "objectID": "index.html#appendix-b-search-terms",
    "href": "index.html#appendix-b-search-terms",
    "title": "A Need for a Contemporary Field",
    "section": "Appendix B: Search terms",
    "text": "Appendix B: Search terms\n\nSCOPUS:\nfor AI related topics: TITLE-ABS-KEY ( ( ( ( machine OR deep OR reinforcement OR supervised OR unsupervised ) AND learning ) OR ( “neural networks” ) OR ( ai OR “artificial intelligence” ) ) ) AND PUBYEAR &gt; 1999 AND PUBYEAR &lt; 2027\nfor overall publication records: PUBYEAR &gt; 1999 AND PUBYEAR &lt; 2027\n\n\narXiv:\n[Abstract] AI or “artificial intelligence” OR machine AND learning OR supervised AND learning OR reinforcement AND learning OR neural AND networks\n17,459 results in 2019 23,923 results in 2020 27,610 results in 2021 29,690 results in 2022 33,419 results in 2023 42,183 in 2024"
  }
]