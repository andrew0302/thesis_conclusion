[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Need for a Contemporary Field",
    "section": "",
    "text": "AI systems are shaped by the training data, algorithm, and evaluation data used to build them. Recent work has shown a need for greater amounts of data, and greater effort in the design, collection, analysis and reporting of data used for building AI systems. Solving these problems may require more effort and resources than the field can offer. Present work makes suggestions aimed at making better use of labor and resources in the AI research field to meet these needs."
  },
  {
    "objectID": "index.html#the-problem-ground-truth-needs-more-than-good-will",
    "href": "index.html#the-problem-ground-truth-needs-more-than-good-will",
    "title": "thesis_conclusion",
    "section": "",
    "text": "Despite growing recognition of dataset problems, actual progress toward better ground-truth data practices remains slow.\nCurrent incentives prioritize speed, performance, and novelty rather than careful measurement design or long-term dataset stewardship.\nThe cost of collecting meaningful, high-quality datasets — particularly for complex constructs like values, intentions, or intelligence — is underappreciated and underfunded. Collecting good data is expensive, but neglecting this cost leads to cascading problems in AI evaluation.\nWithout better practices, datasets remain opaque, unexamined, and prone to embedding systemic biases from the start."
  },
  {
    "objectID": "index.html#lessons-from-other-fields-registered-reports-and-the-science-of-measurement",
    "href": "index.html#lessons-from-other-fields-registered-reports-and-the-science-of-measurement",
    "title": "A Need for a Contemporary Field",
    "section": "Lessons from Other Fields: Registered Reports and the Science of Measurement",
    "text": "Lessons from Other Fields: Registered Reports and the Science of Measurement\n\nIn fields like psychology and medicine, Registered Reports emerged to separate the design of a study from its results, reducing biases like hindsight bias and outcome switching.\nOther fields actively study how to measure complex constructs: e.g., psychometrics for cognitive ability, epidemiology for disease burden, and criminology for recidivism. These fields show that developing good measurement instruments is a dedicated scientific effort — not a side activity.\nIf AI research depends critically on ground-truth data, then the field needs a dedicated research agenda focused on ground-truth design and measurement science. Beyond lessons learned in other fields are ground-truthing specific questions."
  },
  {
    "objectID": "index.html#biases-in-agi-research-highlight-the-need-for-careful-grounding",
    "href": "index.html#biases-in-agi-research-highlight-the-need-for-careful-grounding",
    "title": "A Need for a Contemporary Field",
    "section": "Biases in AGI Research Highlight the Need for Careful Grounding",
    "text": "Biases in AGI Research Highlight the Need for Careful Grounding\n\nIn our ICSE-SEIS 2023 paper, we critique unscientific performance claims in AGI-related work.\nAGI research is especially vulnerable to confirmation bias, wishful thinking, and premature performance claims without rigorous benchmarks.\nDefining the constructs we aim to measure (e.g., “intelligence” in LLMs) must be a scientific task in itself. Intelligence may manifest differently in AI systems than in humans, requiring new conceptualizations and new measurement instruments.\nWithout careful construct definition and measurement, claims about AGI capabilities risk being scientifically meaningless."
  },
  {
    "objectID": "index.html#micropublication-models-capturing-data-collection-as-a-first-class-output",
    "href": "index.html#micropublication-models-capturing-data-collection-as-a-first-class-output",
    "title": "A Need for a Contemporary Field",
    "section": "Micropublication Models: Capturing Data Collection as a First-Class Output",
    "text": "Micropublication Models: Capturing Data Collection as a First-Class Output\n\nMicropublications are modular, peer-reviewed publications that focus on specific research artifacts like datasets, annotation protocols, or measurement plans.\nExtending the Registered Reports model, we can approve data collection protocols before data is collected, including sampling design, measurement instruments, and annotation strategies.\nDecentralized contributors (e.g., multiple labs or individuals) could publish individual datasets under a shared, peer-reviewed protocol.\nThis would move ground-truth collection toward transparent, modular, and cumulative science, rather than isolated, one-off efforts."
  },
  {
    "objectID": "index.html#building-infrastructure-for-transparent-ground-truth",
    "href": "index.html#building-infrastructure-for-transparent-ground-truth",
    "title": "A Need for a Contemporary Field",
    "section": "Building Infrastructure for Transparent Ground Truth",
    "text": "Building Infrastructure for Transparent Ground Truth\n\nOur SEIS 2023 paper pushes for open, linked, reproducible artifacts, not just final models or results.\nOur CHI 2023 paper critiques the opacity of machine learning artifacts and calls for linking data, documentation, and transparent evaluation processes.\nAlexandria:\n\nA web platform combining Wikipedia-style collaborative editing with GitHub-style version control.\nSupports the CREDIT taxonomy for structured contributor recognition.\nProof of concept: Built almost entirely by student developers under supervision, showing that decentralized academic innovation is possible.\nA few additional steps would yield a fully operational infrastructure supporting micropublication of ground-truth artifacts."
  },
  {
    "objectID": "index.html#the-vision-toward-responsible-ground-truth-for-ai-and-agi",
    "href": "index.html#the-vision-toward-responsible-ground-truth-for-ai-and-agi",
    "title": "A Need for a Contemporary Field",
    "section": "The Vision: Toward Responsible Ground Truth for AI and AGI",
    "text": "The Vision: Toward Responsible Ground Truth for AI and AGI\n\nResearchers pre-register their ground-truth collection plans, including constructs, instruments, and expected properties of the data.\nPeer-reviewed protocols are made public before data collection.\nAnnotators, coders, and dataset curators are properly credited through micropublications.\nDatasets grow openly, collaboratively, with tracked provenance and version control.\nThis infrastructure builds trustworthy, reproducible foundations for the next generation of AI and AGI research."
  },
  {
    "objectID": "index.html#concluding-call-to-action",
    "href": "index.html#concluding-call-to-action",
    "title": "A Need for a Contemporary Field",
    "section": "Concluding Call to Action",
    "text": "Concluding Call to Action\nBetter data design enables better science, more responsible innovation, and safer, more meaningful AI systems, but is costly, even in comparison to the current amount of effort and funding dedicated to AI (and related fields) research. A key priority is that the AI/ML research community, conferences, funding agencies, and publishers must recognize data research, analysis as well as creation, as a research contribution.New formats like data-focused Registered Reports and micropublications should be adopted to 1) distribute efforts, and 2) aggregate knowledge.\nFurther, we must rethink how we allocate labor. Load-bearing datasets require more effort than the initial collection and subsequent distribution, but also maintenance. This beyond research required to create useful datasets, we also require resources to maintain them."
  },
  {
    "objectID": "index.html#the-problem-ground-truthing-requires-focus",
    "href": "index.html#the-problem-ground-truthing-requires-focus",
    "title": "thesis_conclusion",
    "section": "",
    "text": "-Despite growing recognition of dataset problems, actual progress toward better ground-truth data practices, from collection, to analysis remains slow. - Current incentives prioritize speed, performance, and novelty rather than careful measurement design or long-term dataset stewardship.[maybe write instead what’s good about current work - i.e what have they prioritized so far] - The cost of collecting meaningful, high-quality datasets — particularly for complex constructs like values, intentions, or intelligence — is underappreciated and underfunded. [Thesis contributions here.] Whether a simple or complex ground-truthing problem, treating the phenomenon of interest like a construct has benefits. Collecting good data is expensive, but neglecting this cost leads to cascading problems in AI evaluation. - Without better practices, datasets remain opaque, unexamined, and prone to embedding systemic biases from the start."
  },
  {
    "objectID": "index.html#appendix-a-citation-trends-plot",
    "href": "index.html#appendix-a-citation-trends-plot",
    "title": "A Need for a Contemporary Field",
    "section": "Appendix A: Citation Trends Plot",
    "text": "Appendix A: Citation Trends Plot"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "A Need for a Contemporary Field",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnaconda data science report 2022: https://www.anaconda.com/resources/whitepaper/state-of-data-science-report-2022↩︎\nAnaconda data science report 2023: https://know.anaconda.com/rs/387-XNW-688/images/Anaconda%202023%20State%20of%20Data%20Science%20Report.pdf↩︎\nAnaconda data science report 2024: https://www.anaconda.com/wp-content/uploads/2025/01/Anaconda_SODS.pdf↩︎\nlanding.ai↩︎\nAccording to wikipedia on 9 May, 2025.↩︎\nUsing the conversion method of words to tokens recommended by Open AI.↩︎\nAccording to sci-hub on 9 May, 2025.↩︎\nAccording to SCOPUS on 9 May, 2025.↩︎\nAccording to The Atlantic.↩︎\nAccording to Common Crawl on 9 May, 2025.↩︎\nhttps://authorsguild.org/app/uploads/2023/12/Authors-Guild-OpenAI-Microsoft-Class-Action-Complaint-Dec-2023.pdf↩︎\nhttps://www.elsevier.com/connect/more-early-career-researchers-are-stepping-up-to-peer-review↩︎\nhttps://elifesciences.org/inside-elife/982053f4/early-career-researchers-views-on-peer-review↩︎\nhttps://github.com/tranhungnghiep/AI-Conference-Info?utm_source=chatgpt.com↩︎\nhttps://neuripsconf.medium.com/what-we-learned-from-neurips-2020-reviewing-process-e24549eea38f↩︎\nSearch terms included AI, artificial intelligence, machine learning, and neural networks. See Appendix B for details.↩︎\nhttps://github.com/tranhungnghiep/AI-Conference-Info?utm_source=chatgpt.com↩︎\naccording to glassdoor on May 28, 2025↩︎\nhttps://www.mturk.com/pricing#:~:text=MTurk%20Fee,per%20assignment%20or%20bonus%20payment.↩︎\nhttps://www.prolific.com/participants↩︎\nhttps://www.business-humanrights.org/pt/%C3%BAltimas-not%C3%ADcias/openai-and-sama-hired-underpaid-workers-in-kenia-to-filter-toxic-content-for-chatgpt/↩︎\nusa.gov↩︎\nhttps://daily.zooniverse.org/2013/09/24/hannys-voorwerp/↩︎\nhttps://www.zooniverse.org/projects/zookeeper/galaxy-zoo/↩︎\nCollaborative Replications and Education Project↩︎\nhttps://en.wikipedia.org/wiki/Systematic_review↩︎\nhttps://en.wikipedia.org/wiki/Meta-analysis↩︎\nhttps://en.wikipedia.org/wiki/Umbrella_review↩︎\nhttps://asreview.nl/↩︎\nhttps://replicate.com/↩︎\nhttps://huggingface.co/↩︎\nhttps://osf.io/↩︎\nhttps://quarto.org/↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html#increasing-data-quality-requires-focused-study",
    "href": "index.html#increasing-data-quality-requires-focused-study",
    "title": "A Need for a Contemporary Field",
    "section": "Increasing Data Quality Requires Focused Study",
    "text": "Increasing Data Quality Requires Focused Study\nDatasets for evaluation very often contain human input (Geiger et al., 2020, 2021). Unlike the field of Machine Learning, other disciplines (psychology, economics, software engineering) have entire fields dedicated to the design, collection, analysis, and reporting of data that involves human behavior (psychometrics, econometrics, software testing). Despite growing recognition of dataset problems (Hullman et al., 2022), which in turn have the potential to lead to harms (Mehrabi et al., 2021), actual progress toward better ground-truth data practices, from collection, to analysis remains slow. Current incentives prioritize accuracy and efficiency rather than careful measurement design or long-term dataset stewardship (Birhane et al., 2022).\nWhen collecting annotations, labels, or other forms of input from people in order to construct training/evaluation datasets, we are attempting to collect measurements of latent, unobservable constructs (Jacobs & Wallach, 2021). In other words, when we consider the input from multiple people in aggregate, we do not directly observe e.g. the presence or absence of an object in a digital image; rather we observe the probability that a person from a given population will indicate the presence of absence of the object in the image (Welty et al., 2019). In the parlance of psychology, one cannot directly observe an other’s Extraversion score, as one might observe an other’s height. Although height is observable, our measurements of it are still imperfect: in using a measurement device like a ruler multiple times, should we measure precisely enough, we would like observe variance in each measurement, with the true score for height imperfectly represented by our imperfect measurements (Welty et al., 2019).\nAny standardized procedure for comparing two or more individuals is treated like a measurement instrument in the social sciences (Urbina, 2014). The repeatable procedures that we use to gather annotations are similarly measurement instruments (Beck et al., 2022). Given the complexity of measuring unobservable phenomena, instruments are subjected to scrutiny prior to being considered usable for their intended purpose. The process of construct validation involves estimating the extent to which an instrument measures an unobservable construct (Wehner et al., 2020). It assumes an unknowable true score, and that all attempts to measure the true score are imperfect. Thus there is no single solution to demonstrating the validity of a construct, but rather an accumulation of evidence, across multiple studies, with observations made using different methods (Smith, 2005). Thus, by extension the datasets that we collect and use to benchmark the performance of models, are similarly measurement instruments (Welty et al., 2019).\nThe cost of collecting meaningful, high-quality datasets — particularly for complex constructs like values, intentions, or intelligence — is underappreciated and underfunded. [Thesis contributions here.] Whether a simple or complex ground-truthing problem, treating the phenomenon of interest like a construct has benefits. Collecting good data is expensive, but neglecting this cost leads to cascading problems in AI evaluation.\nWithout better practices, datasets remain opaque, unexamined, and prone to embedding systemic biases from the start."
  },
  {
    "objectID": "index.html#appendix-b-search-terms",
    "href": "index.html#appendix-b-search-terms",
    "title": "A Need for a Contemporary Field",
    "section": "Appendix B: Search terms",
    "text": "Appendix B: Search terms\n\nSCOPUS:\nfor AI related topics: TITLE-ABS-KEY ( ( ( ( machine OR deep OR reinforcement OR supervised OR unsupervised ) AND learning ) OR ( “neural networks” ) OR ( ai OR “artificial intelligence” ) ) ) AND PUBYEAR &gt; 1999 AND PUBYEAR &lt; 2027\nfor overall publication records: PUBYEAR &gt; 1999 AND PUBYEAR &lt; 2027\n\n\narXiv:\n[Abstract] AI or “artificial intelligence” OR machine AND learning OR supervised AND learning OR reinforcement AND learning OR neural AND networks\n17,459 results in 2019 23,923 results in 2020 27,610 results in 2021 29,690 results in 2022 33,419 results in 2023 42,183 in 2024"
  },
  {
    "objectID": "index.html#increasing-data-requirements-require-curation",
    "href": "index.html#increasing-data-requirements-require-curation",
    "title": "A Need for a Contemporary Field",
    "section": "Increasing data requirements require curation",
    "text": "Increasing data requirements require curation\nContemporary training methods require increasingly large amounts of training data. Taking the development of Large Language Models as a use-case, the training dataset for Llama 3 included 15T tokens, up from 2T for Llama 2. Although not all details of the datasets have been shared, and setting aside questions of copywrite, licensing, and ethical concerns, any available text that is likely to have some quality is limited compared to these requirements. Among the refined sources of text available, Wikipedia, which comprises some 6.9M English articles, comprised of approximately 62M pages, is an estimated 5 billion tokens3. If we take the approximate 4.4M papers published in 2024 and indexed on Scopus as an indication, academia published an estimated 45B tokens in that year. If we extend our reach to other repositories, e.g. the approximately 88.3 million academic papers available on Sci-hub would result in an approximate upper boundary of estimate 700B or so. A similar figure might be estimated from libegen and the 7.5M4 books there. While academic pursuits clearly result in increasing token counts, we have immediate access to a set of approximately 1.5T. Internet archive has some 44M books, which may yield up to 4.4T, although we expect duplicates with the libgen archive. Thus, a more likely source for the ever-increasing data requirements are repositories like Common Crawl5. But this too has limits, and we are projected to have too little human-generated text to continue the increase in model size this decade (Villalobos et al., 2024).\nThe largest frontier models cost tens of millions of USD to train, with estimates of GPT-4 at 40M USD for hardware (chips, servers, and networking hardware) and energy, and estimated increases of 2.4 X per year suggesting that frontier models will cost 1B USD to train by 2027 (Cottier et al., 2024). Notably, this cost exceeds annual revenues in companies training large scale LLMs (Kandpal & Raffel, 2025). Human labor responsible for the collection, curation, and eventual annotation of training data in addition to the training of the model (including researchers, engineers, and managers, but not data center employees and operations staff) is estimated at 29%-49% of the overall cost (Cottier et al., 2024). Notably, these cost calculations ignore the cost of producing the text itself, and though its value is difficult to calculate, estimates range from 10-1000 X more than the total cost of the training of the models (Kandpal & Raffel, 2025). In other words, the more valuable thing is the data and not the model, and the lack of appropriate compensation for its use has given rise to a number of lawsuits (e.g. Authors Guild vs. OpenAI 6).\nThus, the field of AI must wrestle with two opposing issues: we want to train them with data that has high quality - e.g we want the data to be representative of conditions where the model will be deployed, and thus relevant distributions in the training data must reflect the environment in which the models will be deployed (Hullman et al., 2022), but the data requirements to train them appear thus far to be ever-increasing (Villalobos et al., 2024). We want the data used in training to be ‘good’ because we want the models to be ‘trustworthy’ - in the case of LLMs, we want to have reason to think they will generate text that includes claims that we think are true. Perhaps our closest approximation to ‘what we think is true’ is contained the overall perspective presented in all of academic work - the estimating probability of the truth of potential explanations given carefully collected and analyzed observations. And yet, even if there were little to no barriers to using all of human academic text to train LLMs, this amount of text pales in quantity to the vastness of crawled text on the internet. Thus the creation and curation of load-bearing training datasets is a central issue, despite the overwhelming focus on algorithmic work (Birhane et al., 2022)."
  },
  {
    "objectID": "index.html#appendix-c-token-estimates",
    "href": "index.html#appendix-c-token-estimates",
    "title": "A Need for a Contemporary Field",
    "section": "Appendix C: Token estimates",
    "text": "Appendix C: Token estimates\nTaking a study on the word length requirements of education journals as a proxy, Fairbairn et al. (2009) report the following figures:\n\n\n\n\n\n\n\n\n\n\n# Estimate midpoints for each bin\nmidpoints &lt;- c(2500, 3450, 4450, 5450, 6450, 7450, 8450, 9500, 10500)\n\n# Add midpoints to the dataframe\npaper_lengths$Midpoint &lt;- midpoints\n\n# Estimate mean token count\nmean_est &lt;- sum(paper_lengths$Midpoint * paper_lengths$Count) / sum(paper_lengths$Count)\n\n# Calculate weighted variance\nvar_est &lt;- sum(paper_lengths$Count * (paper_lengths$Midpoint - mean_est)^2) / sum(paper_lengths$Count)\n\n# Take square root to get standard deviation\nsd_est &lt;- sqrt(var_est)\n\n# Print result\npaste()\n\ncharacter(0)\n\nprint(paste(\"SD words: \", sd_est))\n\n[1] \"SD words:  1925.27260683454\"\n\nprint(paste(\"Mean words: \", mean_est))\n\n[1] \"Mean words:  6342.166344294\"\n\n\nhttps://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them?utm_source=chatgpt.com According to Open AI, a token is 3/4 of a word\n\nsd_tokens &lt;- sd_est*1.25\nmean_tokens &lt;- mean_est*1.25 \n\nprint(paste(\"SD Tokens: \", round(sd_tokens, 2)))\n\n[1] \"SD Tokens:  2406.59\"\n\nprint(paste(\"Mean Tokens: \", round(mean_tokens, 2)))\n\n[1] \"Mean Tokens:  7927.71\"\n\npaste( round(((mean_tokens-sd_tokens) * 4500000) / 1000000000, 2), \n       \" Billion to\", \n       round (((mean_tokens+sd_tokens) * 4500000) / 1000000000, 2), \n       \"Billion tokens per year from academia.\")\n\n[1] \"24.85  Billion to 46.5 Billion tokens per year from academia.\"\n\npaste( 4.8*1.25, \"Billion tokens from Wikipedia.\")\n\n[1] \"6 Billion tokens from Wikipedia.\"\n\n\nWikipedia: https://en.wikipedia.org/wiki/Wikipedia%3ASize_of_Wikipedia?utm_source=chatgpt.com\n\n5000000000/2000000000000*100\n\n[1] 0.25\n\n\nSci Hub: https://www.sci-hub.mk/ 88343822 documents as of 9 May 2025 13:47.\n\n(7927.71 * 88343822) / 1000000000 \n\n[1] 700.3642\n\n\nhttps://www.theatlantic.com/technology/archive/2025/03/libgen-meta-openai/682093/ LibGen\n7.5 million books and 81 million research papers.\n\n# at about 100k words per book\n(100000*7500000) / 1000000000\n\n[1] 750\n\n(100000*44000000) / 1000000000\n\n[1] 4400\n\n\nLlama 2 was 2T tokens, Llama 3 was 15T. I didn’t really see where they got their data from. So I made some guesses.\nLooked at Wiki. The whole thing is like 5B.\nSci-Hub, 88.3M papers, which at 8kish tokens is an upper boundary of 700B or so.\n7.5 million books on libgen you get about another 700B or so, at 100k-ish per words.\nInternet archive has some 44 million books. 4.4T."
  },
  {
    "objectID": "index.html#training-data-requirements-for-frontier-models",
    "href": "index.html#training-data-requirements-for-frontier-models",
    "title": "A Need for a Contemporary Field",
    "section": "Training data requirements for frontier models",
    "text": "Training data requirements for frontier models\nContemporary training methods require increasingly large amounts of training data. Taking the development of Large Language Models as a use-case, the training dataset for Llama 3 included 15T tokens, up from 2T for Llama 2. Although not all details of the datasets have been shared, and setting aside questions of copywrite, licensing, and ethical concerns, any available text that is likely to have some quality is limited compared to these requirements. Among the refined sources of text available, Wikipedia, which comprises some 6.9M English articles, comprised of approximately 62M pages over all langugages, is an estimated 5 billion tokens6. If we take the approximate 4.4M papers published in 2024 and indexed on Scopus as an indication, academia published an estimated 45B tokens in that year. If we extend our reach to other repositories, e.g. the approximately 85 million academic papers available on Sci-hub7 or SCOPUS8 would result in an approximate upper boundary of estimate 700B or so. A similar figure might be estimated from libegen and the 7.5M9 books there. While academic pursuits result in increasing token counts, we have immediate access to a set of approximately 1.5T. Internet archive has some 44M books, which may yield up to 4.4T, although we expect duplicates with the libgen archive. Thus, a more likely source for the ever-increasing data requirements are repositories like Common Crawl10. But this too has limits, and we are projected to have too little human-generated text to continue the increase in model size this decade (Villalobos et al., 2024).\nThe largest frontier models cost tens of millions of USD to train, with estimates of GPT-4 at 40M USD for hardware (chips, servers, and networking hardware) and energy, and estimated increases of 2.4 X per year suggesting that frontier models will cost 1B USD to train by 2027 (Cottier et al., 2024). Notably, this cost exceeds annual revenues in companies training large scale LLMs (Kandpal & Raffel, 2025). Human labor responsible for the collection, curation, and eventual annotation of training data in addition to the training of the model (including researchers, engineers, and managers, but not data center employees and operations staff) is estimated at 29%-49% of the overall cost (Cottier et al., 2024). Notably, these cost calculations ignore the cost of producing the text itself, and though its value is difficult to calculate, estimates range from 10-1000 X more than the total cost of the training of the models (Kandpal & Raffel, 2025). In other words, the more valuable thing is the data and not the model, and the lack of appropriate compensation for its use has given rise to a number of lawsuits (e.g. Authors Guild vs. OpenAI 11).\nThus, the field of AI must wrestle with two opposing issues: we want to train them with data that has high quality - e.g we want the data to be representative of conditions where the model will be deployed, and thus relevant distributions in the training data must reflect the environment in which the models will be deployed (Hullman et al., 2022), but the data requirements to train them appear thus far to be ever-increasing (Villalobos et al., 2024). Frontier models add an additional challenge: that of scale. We want the data used in training to be ‘good’ because we want the models to be ‘trustworthy’ - in the case of LLMs, we want to have reason to think they will generate text that includes claims that we think are true. Perhaps our closest approximation to ‘what we think is true’ is contained the overall perspective presented in all of academic work and official reports - the estimating probability of the truth of potential explanations given carefully collected and analyzed observations. And yet, even if there were little to no barriers to using all of human academic text to train LLMs, this amount of text pales in quantity to the vastness of crawled text on the internet."
  },
  {
    "objectID": "index.html#evaluatiuon-data-requires-focused-study",
    "href": "index.html#evaluatiuon-data-requires-focused-study",
    "title": "A Need for a Contemporary Field",
    "section": "Evaluatiuon Data Requires Focused Study",
    "text": "Evaluatiuon Data Requires Focused Study\nDatasets for evaluation very often contain human input (Geiger et al., 2020, 2021). Unlike the field of Machine Learning, other disciplines (psychology, economics, software engineering) have entire fields dedicated to the design, collection, analysis, and reporting of data that involves human behavior (psychometrics, econometrics, software testing). Despite growing recognition of dataset problems (Hullman et al., 2022), which in turn have the potential to lead to harms (Mehrabi et al., 2021), actual progress toward better ground-truth data practices, from collection, to analysis remains slow. Current incentives prioritize accuracy and efficiency rather than careful measurement design or long-term dataset stewardship (Birhane et al., 2022).\nWhen collecting annotations, labels, or other forms of input from people in order to construct training/evaluation datasets, we are attempting to collect measurements of latent, unobservable constructs (Jacobs & Wallach, 2021). In other words, when we consider the input from multiple people in aggregate, we do not directly observe e.g. the presence or absence of an object in a digital image; rather we observe the probability that a person from a given population will indicate the presence of absence of the object in the image (Welty et al., 2019). In the parlance of psychology, one cannot directly observe an other’s Extraversion score, as one might observe an other’s height. Although height is observable, our measurements of it are still imperfect: in using a measurement device like a ruler multiple times, should we measure precisely enough, we would like observe variance in each measurement, with the true score for height imperfectly represented by our imperfect measurements (Welty et al., 2019).\nAny standardized procedure for comparing two or more individuals is treated like a measurement instrument in the social sciences (Urbina, 2014). The repeatable procedures that we use to gather annotations are similarly measurement instruments (Beck et al., 2022). Given the complexity of measuring unobservable phenomena, instruments are subjected to scrutiny prior to being considered usable for their intended purpose. The process of construct validation involves estimating the extent to which an instrument measures an unobservable construct (Wehner et al., 2020). It assumes an unknowable true score, and that all attempts to measure the true score are imperfect. Thus there is no single solution to demonstrating the validity of a construct, but rather an accumulation of evidence, across multiple studies, with observations made using different methods (Smith, 2005). Thus, by extension the datasets that we collect and use to benchmark the performance of models, are similarly measurement instruments (Welty et al., 2019).\nThe cost of collecting meaningful, high-quality datasets — particularly for complex constructs like values, intentions, or intelligence — is underappreciated and underfunded. [Thesis contributions here.] Whether a simple or complex ground-truthing problem, treating the phenomenon of interest like a construct has benefits. Collecting good data is expensive, but neglecting this cost leads to cascading problems in AI evaluation.\nWithout better practices, datasets remain opaque, unexamined, and prone to embedding systemic biases from the start."
  },
  {
    "objectID": "index.html#challenges-for-training-data",
    "href": "index.html#challenges-for-training-data",
    "title": "A Need for a Contemporary Field",
    "section": "Challenges for training data",
    "text": "Challenges for training data\nContemporary training methods require increasingly large amounts of training data. This has lead to to the development of techniques aimed at increasing and/or augmenting available data (Alzubaidi et al., 2023). Beyond the challenge of gathering data at the necessary scale, is the challenge of ensuring that it is generally representative of the environment where it is to be deployed (Hullman et al., 2022), has sufficiently diverse coverage of the various scenarios it will encounter (Liang et al., 2022), and is as free as possible of bias (Mehrabi et al., 2021). Such a process requires deliberate design, and cannot be compensated for by increasing the amount of data collected (Hullman et al., 2022).\nThese issues are well illustrated with the most contemporary trends of Generative AI systems. Taking the development of Large Language Models (LLMs) as a use-case: the training dataset for Llama 3 included 15T tokens, up from 2T for Llama 2. Although not all details of the datasets have been shared, and setting aside questions of copywrite, licensing, and ethical concerns, any available text that is likely to have some quality is limited compared to these requirements. Among the refined sources of text available, Wikipedia, which comprises some 6.9M English articles, comprised of approximately 62M pages over all languages, is an estimated 5 billion tokens5. If we take the approximate 4.4M papers published in 2024 and indexed on Scopus as an indication, academia published an estimated 45B tokens in that year. If we extend our reach to other repositories, e.g. the approximately 85 million academic papers available on Sci-hub6 or SCOPUS7 would result in an estimate of 700B. A similar figure might be estimated from libegen and the 7.5M8 books there. While academic pursuits result in increasing token counts, we have immediate access to a set of approximately 1.5T. Internet archive has some 44M books, which may yield up to 4.4T, although we expect duplicates with the libgen archive. Thus, a more likely source for the ever-increasing data requirements are repositories like Common Crawl9. But this too has limits, and we are projected to have too little human-generated text to continue the increase in model size this decade (Villalobos et al., 2024).\nThe largest frontier models cost tens of millions of USD to train, with estimates of GPT-4 at 40M USD for hardware (chips, servers, and networking hardware) and energy, and estimated increases of 2.4 X per year suggesting that frontier models will cost 1B USD to train by 2027 (Cottier et al., 2024). Notably, this cost exceeds annual revenues in companies training large scale LLMs (Kandpal & Raffel, 2025). Human labor responsible for the collection, curation, and eventual annotation of training data in addition to the training of the model (including researchers, engineers, and managers, but not data center employees and operations staff) is estimated at 29%-49% of the overall cost (Cottier et al., 2024). Notably, these cost calculations ignore the cost of producing the text itself, and though its value is difficult to calculate, estimates range from 10-1000 X more than the total cost of the training of the models (Kandpal & Raffel, 2025). In other words, the more valuable thing is the data and not the model, and the lack of appropriate compensation for its use has given rise to a number of lawsuits (e.g. Authors Guild vs. OpenAI 10).\nThus, the field of AI must wrestle with two opposing issues: we want to train models with data that has high quality - e.g we want the data to be representative of conditions where the model will be deployed, and thus relevant distributions in the training data must reflect the environment in which the models will be deployed (Hullman et al., 2022), but the data requirements to train them appear thus far to be ever-increasing (Villalobos et al., 2024). Models trained with contemporary techniques add an additional challenge: that of scale. We want the data used in training to be ‘good’ because we want the models to be ‘trustworthy’ - in the case of LLMs, we want to have reason to think they will generate text that includes claims that we think are true. Perhaps our closest approximation to ‘what we think is true’ is contained the overall perspective presented in all of academic work and official reports - the estimating probability of the truth of potential explanations given carefully collected and analyzed observations. And yet, even if there were little to no barriers to using all of human academic text to train LLMs, this amount of text pales in quantity to the data requirements.\n\nChallenges for evaluation data\nDatasets for evaluation very often contain human input (Geiger et al., 2020, 2021). Unlike the field of Machine Learning, other disciplines (psychology, economics, software engineering) have entire fields dedicated to the design, collection, analysis, and reporting of data that involves human behavior (psychometrics, econometrics, software testing). Despite growing recognition of dataset problems (Hullman et al., 2022), which in turn have the potential to lead to harms (Mehrabi et al., 2021), actual progress toward better ground-truth data practices, from collection, to analysis remains slow. Current incentives prioritize accuracy and efficiency rather than careful measurement design or long-term dataset stewardship (Birhane et al., 2022).\nWhen collecting annotations, labels, or other forms of input from people in order to construct training/evaluation datasets, we are attempting to collect measurements of latent, unobservable constructs (Jacobs & Wallach, 2021). In other words, when we consider the input from multiple people in aggregate, we do not directly observe e.g. the presence or absence of an object in a digital image; rather we observe the probability that a person from a given population will indicate the presence of absence of the object in the image (Welty et al., 2019). In the parlance of psychology, one cannot directly observe an other’s Extraversion score, as one might observe an other’s height. Although height is observable, our measurements of it are still imperfect: in using a measurement device like a ruler multiple times, should we measure precisely enough, we would like observe variance in each measurement, with the true score for height imperfectly represented by our imperfect measurements (Welty et al., 2019).\nAny standardized procedure for comparing two or more individuals is treated like a measurement instrument in the social sciences (Urbina, 2014). The repeatable procedures that we use to gather annotations are similarly measurement instruments (Beck et al., 2022). Given the complexity of measuring unobservable phenomena, instruments are subjected to scrutiny prior to being considered usable for their intended purpose. The process of construct validation involves estimating the extent to which an instrument measures an unobservable construct (Wehner et al., 2020). It assumes an unknowable true score, and that all attempts to measure the true score are imperfect. There is no single solution to demonstrating the validity of a construct, but rather an accumulation of evidence, across multiple studies, with observations made using different methods (Smith, 2005). Thus, by extension the datasets that we collect and use to benchmark the performance of models, are similarly measurement instruments (Welty et al., 2019).\n\n\nGround-truthing is a field\nData used for training and evaluation of models is of central importance, requiring continuous study."
  },
  {
    "objectID": "index.html#challenges-for-evaluation-data",
    "href": "index.html#challenges-for-evaluation-data",
    "title": "A Need for a Contemporary Field",
    "section": "Challenges for evaluation data",
    "text": "Challenges for evaluation data\nDatasets for evaluation very often contain human input (Geiger et al., 2020, 2021). Unlike the field of Machine Learning, other disciplines (psychology, economics, software engineering) have entire fields dedicated to the design, collection, analysis, and reporting of data that involves human behavior (psychometrics, econometrics, software testing). Despite growing recognition of dataset problems (Hullman et al., 2022), which in turn have the potential to lead to harms (Mehrabi et al., 2021), actual progress toward better ground-truth data practices, from collection, to analysis remains slow. Current incentives prioritize accuracy and efficiency rather than careful measurement design or long-term dataset stewardship (Birhane et al., 2022).\nWhen collecting annotations, labels, or other forms of input from people in order to construct training/evaluation datasets, we are attempting to collect measurements of latent, unobservable constructs (Jacobs & Wallach, 2021). In other words, when we consider the input from multiple people in aggregate, we do not directly observe e.g. the presence or absence of an object in a digital image; rather we observe the probability that a person from a given population will indicate the presence of absence of the object in the image (Welty et al., 2019). In the parlance of psychology, one cannot directly observe an other’s Extraversion score, as one might observe an other’s height. Although height is observable, our measurements of it are still imperfect: in using a measurement device like a ruler multiple times, should we measure precisely enough, we would like observe variance in each measurement, with the true score for height imperfectly represented by our imperfect measurements (Welty et al., 2019).\nAny standardized procedure for comparing two or more individuals is treated like a measurement instrument in the social sciences (Urbina, 2014). The repeatable procedures that we use to gather annotations are similarly measurement instruments (Beck et al., 2022). Given the complexity of measuring unobservable phenomena, instruments are subjected to scrutiny prior to being considered usable for their intended purpose. The process of construct validation involves estimating the extent to which an instrument measures an unobservable construct (Wehner et al., 2020). It assumes an unknowable true score, and that all attempts to measure the true score are imperfect. Thus there is no single solution to demonstrating the validity of a construct, but rather an accumulation of evidence, across multiple studies, with observations made using different methods (Smith, 2005). Thus, by extension the datasets that we collect and use to benchmark the performance of models, are similarly measurement instruments (Welty et al., 2019).\nThe cost of collecting meaningful, high-quality datasets — particularly for complex constructs like values, intentions, or intelligence — is underappreciated and underfunded. [Thesis contributions here.] Whether a simple or complex ground-truthing problem, treating the phenomenon of interest like a construct has benefits. Collecting good data is expensive, but neglecting this cost leads to cascading problems in AI evaluation.\nWithout better practices, datasets remain opaque, unexamined, and prone to embedding systemic biases from the start."
  },
  {
    "objectID": "index.html#addressing-challenges-for-training-data-requires-study",
    "href": "index.html#addressing-challenges-for-training-data-requires-study",
    "title": "A Need for a Contemporary Field",
    "section": "Addressing challenges for training data requires study",
    "text": "Addressing challenges for training data requires study\nContemporary training methods require increasingly large amounts of training data. This has lead to to the development of techniques aimed at increasing and/or augmenting available data (Alzubaidi et al., 2023). Beyond the challenge of gathering data at the necesary scale, is the challenge of ensuring that it is generally representative of the environment where it is to be deployed, has sufficiently diverse coverage of the various scenarios it will encounter (Liang et al., 2022), and as free as possible of bias (Mehrabi et al., 2021). Such a process requires deliberate design, and cannot be compensated for by increasing the amount of data collected (Hullman et al., 2022).\nThese issues are magnified with the most contemporary trends of Generative AI systems. Taking the development of Large Language Models as a use-case, the training dataset for Llama 3 included 15T tokens, up from 2T for Llama 2. Although not all details of the datasets have been shared, and setting aside questions of copywrite, licensing, and ethical concerns, any available text that is likely to have some quality is limited compared to these requirements. Among the refined sources of text available, Wikipedia, which comprises some 6.9M English articles, comprised of approximately 62M pages over all languages, is an estimated 5 billion tokens6. If we take the approximate 4.4M papers published in 2024 and indexed on Scopus as an indication, academia published an estimated 45B tokens in that year. If we extend our reach to other repositories, e.g. the approximately 85 million academic papers available on Sci-hub7 or SCOPUS8 would result in an estimate of 700B. A similar figure might be estimated from libegen and the 7.5M9 books there. While academic pursuits result in increasing token counts, we have immediate access to a set of approximately 1.5T. Internet archive has some 44M books, which may yield up to 4.4T, although we expect duplicates with the libgen archive. Thus, a more likely source for the ever-increasing data requirements are repositories like Common Crawl10. But this too has limits, and we are projected to have too little human-generated text to continue the increase in model size this decade (Villalobos et al., 2024).\nThe largest frontier models cost tens of millions of USD to train, with estimates of GPT-4 at 40M USD for hardware (chips, servers, and networking hardware) and energy, and estimated increases of 2.4 X per year suggesting that frontier models will cost 1B USD to train by 2027 (Cottier et al., 2024). Notably, this cost exceeds annual revenues in companies training large scale LLMs (Kandpal & Raffel, 2025). Human labor responsible for the collection, curation, and eventual annotation of training data in addition to the training of the model (including researchers, engineers, and managers, but not data center employees and operations staff) is estimated at 29%-49% of the overall cost (Cottier et al., 2024). Notably, these cost calculations ignore the cost of producing the text itself, and though its value is difficult to calculate, estimates range from 10-1000 X more than the total cost of the training of the models (Kandpal & Raffel, 2025). In other words, the more valuable thing is the data and not the model, and the lack of appropriate compensation for its use has given rise to a number of lawsuits (e.g. Authors Guild vs. OpenAI 11).\nThus, the field of AI must wrestle with two opposing issues: we want to train them with data that has high quality - e.g we want the data to be representative of conditions where the model will be deployed, and thus relevant distributions in the training data must reflect the environment in which the models will be deployed (Hullman et al., 2022), but the data requirements to train them appear thus far to be ever-increasing (Villalobos et al., 2024). Frontier models add an additional challenge: that of scale. We want the data used in training to be ‘good’ because we want the models to be ‘trustworthy’ - in the case of LLMs, we want to have reason to think they will generate text that includes claims that we think are true. Perhaps our closest approximation to ‘what we think is true’ is contained the overall perspective presented in all of academic work and official reports - the estimating probability of the truth of potential explanations given carefully collected and analyzed observations. And yet, even if there were little to no barriers to using all of human academic text to train LLMs, this amount of text pales in quantity to the data requirements."
  },
  {
    "objectID": "index.html#addressing-challenges-for-training-data",
    "href": "index.html#addressing-challenges-for-training-data",
    "title": "A Need for a Contemporary Field",
    "section": "Addressing challenges for training data",
    "text": "Addressing challenges for training data\nContemporary training methods require increasingly large amounts of training data. This has lead to to the development of techniques aimed at increasing and/or augmenting available data (Alzubaidi et al., 2023). Beyond the challenge of gathering data at the necesary scale, is the challenge of ensuring that it is generally representative of the environment where it is to be deployed, has sufficiently diverse coverage of the various scenarios it will encounter (Liang et al., 2022), and as free as possible of bias (Mehrabi et al., 2021). Such a process requires deliberate design, and cannot be compensated for by increasing the amount of data collected (Hullman et al., 2022).\nThese issues are magnified with the most contemporary trends of Generative AI systems. Taking the development of Large Language Models as a use-case, the training dataset for Llama 3 included 15T tokens, up from 2T for Llama 2. Although not all details of the datasets have been shared, and setting aside questions of copywrite, licensing, and ethical concerns, any available text that is likely to have some quality is limited compared to these requirements. Among the refined sources of text available, Wikipedia, which comprises some 6.9M English articles, comprised of approximately 62M pages over all languages, is an estimated 5 billion tokens6. If we take the approximate 4.4M papers published in 2024 and indexed on Scopus as an indication, academia published an estimated 45B tokens in that year. If we extend our reach to other repositories, e.g. the approximately 85 million academic papers available on Sci-hub7 or SCOPUS8 would result in an estimate of 700B. A similar figure might be estimated from libegen and the 7.5M9 books there. While academic pursuits result in increasing token counts, we have immediate access to a set of approximately 1.5T. Internet archive has some 44M books, which may yield up to 4.4T, although we expect duplicates with the libgen archive. Thus, a more likely source for the ever-increasing data requirements are repositories like Common Crawl10. But this too has limits, and we are projected to have too little human-generated text to continue the increase in model size this decade (Villalobos et al., 2024).\nThe largest frontier models cost tens of millions of USD to train, with estimates of GPT-4 at 40M USD for hardware (chips, servers, and networking hardware) and energy, and estimated increases of 2.4 X per year suggesting that frontier models will cost 1B USD to train by 2027 (Cottier et al., 2024). Notably, this cost exceeds annual revenues in companies training large scale LLMs (Kandpal & Raffel, 2025). Human labor responsible for the collection, curation, and eventual annotation of training data in addition to the training of the model (including researchers, engineers, and managers, but not data center employees and operations staff) is estimated at 29%-49% of the overall cost (Cottier et al., 2024). Notably, these cost calculations ignore the cost of producing the text itself, and though its value is difficult to calculate, estimates range from 10-1000 X more than the total cost of the training of the models (Kandpal & Raffel, 2025). In other words, the more valuable thing is the data and not the model, and the lack of appropriate compensation for its use has given rise to a number of lawsuits (e.g. Authors Guild vs. OpenAI 11).\nThus, the field of AI must wrestle with two opposing issues: we want to train them with data that has high quality - e.g we want the data to be representative of conditions where the model will be deployed, and thus relevant distributions in the training data must reflect the environment in which the models will be deployed (Hullman et al., 2022), but the data requirements to train them appear thus far to be ever-increasing (Villalobos et al., 2024). Frontier models add an additional challenge: that of scale. We want the data used in training to be ‘good’ because we want the models to be ‘trustworthy’ - in the case of LLMs, we want to have reason to think they will generate text that includes claims that we think are true. Perhaps our closest approximation to ‘what we think is true’ is contained the overall perspective presented in all of academic work and official reports - the estimating probability of the truth of potential explanations given carefully collected and analyzed observations. And yet, even if there were little to no barriers to using all of human academic text to train LLMs, this amount of text pales in quantity to the data requirements."
  },
  {
    "objectID": "index.html#data-for-ai-is-a-field",
    "href": "index.html#data-for-ai-is-a-field",
    "title": "A Need for a Contemporary Field",
    "section": "Data for AI is a field",
    "text": "Data for AI is a field\nData used for training and evaluation of models is of central importance, requiring continuous study.\n\nChallenges for training data\nContemporary training methods require increasingly large amounts of training data. This has lead to to the development of techniques aimed at increasing and/or augmenting available data (Alzubaidi et al., 2023). Beyond the challenge of gathering data at the necessary scale, is the challenge of ensuring that it is generally representative of the environment where it is to be deployed (Hullman et al., 2022), has sufficiently diverse coverage of the various scenarios it will encounter (Liang et al., 2022), and is as free as possible of bias (Mehrabi et al., 2021). Such a process requires deliberate design, and cannot be compensated for by increasing the amount of data collected (Hullman et al., 2022).\nThese issues are well illustrated with the most contemporary trends of Generative AI systems. Taking the development of Large Language Models (LLMs) as a use-case: the training dataset for Llama 3 included 15T tokens, up from 2T for Llama 2. Although not all details of the datasets have been shared, and setting aside questions of copywrite, licensing, and ethical concerns, any available text that is likely to have some quality is limited compared to these requirements. Among the refined sources of text available, Wikipedia, which comprises some 6.9M English articles, comprised of approximately 62M pages over all languages, is an estimated 5 billion tokens7. If we take the approximate 4.4M papers published in 2024 and indexed on Scopus as an indication, academia published an estimated 45B tokens in that year. If we extend our reach to other repositories, e.g. the approximately 85 million academic papers available on Sci-hub8 or SCOPUS9 would result in an estimate of 700B. A similar figure might be estimated from libegen and the 7.5M10 books there. While academic pursuits result in increasing token counts, we have immediate access to a set of approximately 1.5T. Internet archive has some 44M books, which may yield up to 4.4T, although we expect duplicates with the libgen archive. Thus, a more likely source for the ever-increasing data requirements are repositories like Common Crawl11. But this too has limits, and we are projected to have too little human-generated text to continue the increase in model size this decade (Villalobos et al., 2024).\nThe largest frontier models cost tens of millions of USD to train, with estimates of GPT-4 at 40M USD for hardware (chips, servers, and networking hardware) and energy, and estimated increases of 2.4 X per year suggesting that frontier models will cost 1B USD to train by 2027 (Cottier et al., 2024). Notably, this cost exceeds annual revenues in companies training large scale LLMs (Kandpal & Raffel, 2025). Human labor responsible for the collection, curation, and eventual annotation of training data in addition to the training of the model (including researchers, engineers, and managers, but not data center employees and operations staff) is estimated at 29%-49% of the overall cost (Cottier et al., 2024). Notably, these cost calculations ignore the cost of producing the text itself, and though its value is difficult to calculate, estimates range from 10-1000 X more than the total cost of the training of the models (Kandpal & Raffel, 2025). In other words, the more valuable thing is the data and not the model, and the lack of appropriate compensation for its use has given rise to a number of lawsuits (e.g. Authors Guild vs. OpenAI 12).\nThus, the field of AI must wrestle with two opposing issues: we want to train models with data that has high quality - e.g we want the data to be representative of conditions where the model will be deployed, and thus relevant distributions in the training data must reflect the environment in which the models will be deployed (Hullman et al., 2022), but the data requirements to train them appear thus far to be ever-increasing (Villalobos et al., 2024). Models trained with contemporary techniques add an additional challenge: that of scale. We want the data used in training to be ‘good’ because we want the models to be ‘trustworthy’ - in the case of LLMs, we want to have reason to think they will generate text that includes claims that we think are true. Perhaps our closest approximation to ‘what we think is true’ is contained the overall perspective presented in all of academic work and official reports - the estimating probability of the truth of potential explanations given carefully collected and analyzed observations. And yet, even if there were little to no barriers to using all of human academic text to train LLMs, this amount of text pales in quantity to the data requirements.\n\n\nChallenges for evaluation data\nDatasets for evaluation very often contain human input (Geiger et al., 2020, 2021). Unlike the field of Machine Learning, other disciplines (psychology, economics, software engineering) have entire fields dedicated to the design, collection, analysis, and reporting of data that involves human behavior (psychometrics, econometrics, software testing). Despite growing recognition of dataset problems (Hullman et al., 2022), which in turn have the potential to lead to harms (Mehrabi et al., 2021), actual progress toward better ground-truth data practices, from collection, to analysis remains slow. Current incentives prioritize accuracy and efficiency rather than careful measurement design or long-term dataset stewardship (Birhane et al., 2022).\nWhen collecting annotations, labels, or other forms of input from people in order to construct training/evaluation datasets, we are attempting to collect measurements of latent, unobservable constructs (Jacobs & Wallach, 2021). In other words, when we consider the input from multiple people in aggregate, we do not directly observe e.g. the presence or absence of an object in a digital image; rather we observe the probability that a person from a given population will indicate the presence of absence of the object in the image (Welty et al., 2019). In the parlance of psychology, one cannot directly observe an other’s Extraversion score, as one might observe an other’s height. Although height is observable, our measurements of it are still imperfect: in using a measurement device like a ruler multiple times, should we measure precisely enough, we would like observe variance in each measurement, with the true score for height imperfectly represented by our imperfect measurements (Welty et al., 2019).\nAny standardized procedure for comparing two or more individuals is treated like a measurement instrument in the social sciences (Urbina, 2014). The repeatable procedures that we use to gather annotations are similarly measurement instruments (Beck et al., 2022). Given the complexity of measuring unobservable phenomena, instruments are subjected to scrutiny prior to being considered usable for their intended purpose. The process of construct validation involves estimating the extent to which an instrument measures an unobservable construct (Wehner et al., 2020). It assumes an unknowable true score, and that all attempts to measure the true score are imperfect. There is no single solution to demonstrating the validity of a construct, but rather an accumulation of evidence, across multiple studies, with observations made using different methods (Smith, 2005). Thus, by extension the datasets that we collect and use to benchmark the performance of models, are similarly measurement instruments (Welty et al., 2019).\n\n\nCollaboration to address data requirement challenges\nOne approach to both gathering the necessary data at scale as well as labels or annotations have been shown in the form of online communities. For example inaturalist.org is an online community with over 8 million users who make contributions in the form of images taken on their smartphones, and/or labels of the species in the images. Similarly, zooniverse.org is an online community of over 2.8M users that hosts projects defined by scientists to gather labels from non-scientists. A third example is commonvoice.mozilla.org/en, which is a large dataset of speech transcription in 76 languages, provided by approximately 150k participants."
  },
  {
    "objectID": "index.html#data-for-ai-as-a-field",
    "href": "index.html#data-for-ai-as-a-field",
    "title": "A Need for a Contemporary Field",
    "section": "Data for AI as a field",
    "text": "Data for AI as a field\nData used for training and evaluation of models is of central importance, requiring continuous study.\n\nChallenges for training data\nContemporary training methods require increasingly large amounts of training data. This has lead to to the development of techniques aimed at increasing and/or augmenting available data (Alzubaidi et al., 2023). Beyond the challenge of gathering data at the necessary scale, is the challenge of ensuring that it is generally representative of the environment where it is to be deployed (Hullman et al., 2022), has sufficiently diverse coverage of the various scenarios it will encounter (Liang et al., 2022), and is as free as possible of bias (Mehrabi et al., 2021). Such a process requires deliberate design, and cannot be compensated for by increasing the amount of data collected (Hullman et al., 2022).\nThese issues are well illustrated with the most contemporary trends of Generative AI systems. Taking the development of Large Language Models (LLMs) as a use-case: the training dataset for Llama 3 included 15T tokens, up from 2T for Llama 2. Although not all details of the datasets have been shared, and setting aside questions of copywrite, licensing, and ethical concerns, any available text that is likely to have some quality is limited compared to these requirements. Among the refined sources of text available, Wikipedia, which comprises some 6.9M English articles, comprised of approximately 62M pages over all languages, is an estimated 5 billion tokens5. If we take the approximate 4.4M papers published in 2024 and indexed on Scopus as an indication, academia published an estimated 45B tokens in that year. If we extend our reach to other repositories, e.g. the approximately 85 million academic papers available on Sci-hub6 or SCOPUS7 would result in an estimate of 700B. A similar figure might be estimated from libegen and the 7.5M8 books there. While academic pursuits result in increasing token counts, we have immediate access to a set of approximately 1.5T. Internet archive has some 44M books, which may yield up to 4.4T, although we expect duplicates with the libgen archive. Thus, a more likely source for the ever-increasing data requirements are repositories like Common Crawl9. But this too has limits, and we are projected to have too little human-generated text to continue the increase in model size this decade (Villalobos et al., 2024).\nThe largest frontier models cost tens of millions of USD to train, with estimates of GPT-4 at 40M USD for hardware (chips, servers, and networking hardware) and energy, and estimated increases of 2.4 X per year suggesting that frontier models will cost 1B USD to train by 2027 (Cottier et al., 2024). Notably, this cost exceeds annual revenues in companies training large scale LLMs (Kandpal & Raffel, 2025). Human labor responsible for the collection, curation, and eventual annotation of training data in addition to the training of the model (including researchers, engineers, and managers, but not data center employees and operations staff) is estimated at 29%-49% of the overall cost (Cottier et al., 2024). Notably, these cost calculations ignore the cost of producing the text itself, and though its value is difficult to calculate, estimates range from 10-1000 X more than the total cost of the training of the models (Kandpal & Raffel, 2025). In other words, the more valuable thing is the data and not the model, and the lack of appropriate compensation for its use has given rise to a number of lawsuits (e.g. Authors Guild vs. OpenAI 10).\nThus, the field of AI must wrestle with two opposing issues: we want to train models with data that has high quality - e.g we want the data to be representative of conditions where the model will be deployed, and thus relevant distributions in the training data must reflect the environment in which the models will be deployed (Hullman et al., 2022), but the data requirements to train them appear thus far to be ever-increasing (Villalobos et al., 2024). Models trained with contemporary techniques add an additional challenge: that of scale. We want the data used in training to be ‘good’ because we want the models to be ‘trustworthy’ - in the case of LLMs, we want to have reason to think they will generate text that includes claims that we think are true. Perhaps our closest approximation to ‘what we think is true’ is contained the overall perspective presented in all of academic work and official reports - the estimating probability of the truth of potential explanations given carefully collected and analyzed observations. And yet, even if there were little to no barriers to using all of human academic text to train LLMs, this amount of text pales in quantity to the data requirements.\n\n\nChallenges for evaluation data\nDatasets for evaluation very often contain human input (Geiger et al., 2020, 2021). Unlike the field of Machine Learning, other disciplines (psychology, economics, software engineering) have entire fields dedicated to the design, collection, analysis, and reporting of data that involves human behavior (psychometrics, econometrics, software testing). Despite growing recognition of dataset problems (Hullman et al., 2022), which in turn have the potential to lead to harms (Mehrabi et al., 2021), actual progress toward better ground-truth data practices, from collection, to analysis remains slow. Current incentives prioritize accuracy and efficiency rather than careful measurement design or long-term dataset stewardship (Birhane et al., 2022).\nWhen collecting annotations, labels, or other forms of input from people in order to construct training/evaluation datasets, we are attempting to collect measurements of latent, unobservable constructs (Jacobs & Wallach, 2021). In other words, when we consider the input from multiple people in aggregate, we do not directly observe e.g. the presence or absence of an object in a digital image; rather we observe the probability that a person from a given population will indicate the presence of absence of the object in the image (Welty et al., 2019). In the parlance of psychology, one cannot directly observe an other’s Extraversion score, as one might observe an other’s height. Although height is observable, our measurements of it are still imperfect: in using a measurement device like a ruler multiple times, should we measure precisely enough, we would like observe variance in each measurement, with the true score for height imperfectly represented by our imperfect measurements (Welty et al., 2019).\nAny standardized procedure for comparing two or more individuals is treated like a measurement instrument in the social sciences (Urbina, 2014). The repeatable procedures that we use to gather annotations are similarly measurement instruments (Beck et al., 2022). Given the complexity of measuring unobservable phenomena, instruments are subjected to scrutiny prior to being considered usable for their intended purpose. The process of construct validation involves estimating the extent to which an instrument measures an unobservable construct (Wehner et al., 2020). It assumes an unknowable true score, and that all attempts to measure the true score are imperfect. There is no single solution to demonstrating the validity of a construct, but rather an accumulation of evidence, across multiple studies, with observations made using different methods (Smith, 2005). Thus, by extension the datasets that we collect and use to benchmark the performance of models, are similarly measurement instruments (Welty et al., 2019)."
  },
  {
    "objectID": "index.html#challenges-in-scientific-labor",
    "href": "index.html#challenges-in-scientific-labor",
    "title": "A Need for a Contemporary Field",
    "section": "Challenges in scientific labor",
    "text": "Challenges in scientific labor\nAs it stands, our current knowledge gathering apparatus - science as it is now practiced and reported - is overburdened and inefficient. The ever increasing volume of published manuscripts on AI and related topics11 makes it impossible to stay abreast of the overall field: 10% of over 4 million publications indexed on the SCOPUS academic database in 2024, up from around 7% in 2022, had terms related to AI in their title, keywords or abstracts (see Appendix A). The number of submitted manuscripts also increases year over year, with popular conferences like NeurIPS receiving upwards of 12k submissions in 202312. This makes it more and more difficult to find reviewers, and by extension to monitor the overall quality of the field (Zhang et al., 2022). These figures do not include preprints posted on servers like arXiv, which show over 42k works with AI related terms in the abstract for 2024, more than doubling the about 17.5k posts in 2019. PhD candidates, who contribute a substantial proportion of academic work (Larivière, 2012), are bogged down by requirements to write and defend theses despite the decreasing trend of thesis citations over time (Larivière et al., 2008), and evidence that PhD candidates with fixed duration contracts exceed that duration by several months, resorting to completing their thesis on their own time and risking failing at completion (Van de Schoot et al., 2013). Further, publications, and not theses, remain the key factor in the assesment of their value as scientists (Anderson et al., 2022) - efforts which could be put towards meeting some of this labor gap. Predictably, academics appear to be turning to LLMs for assistance, as evidence of its use is showing in academic work in both peer reviews (Liang et al., 2024), and in manuscripts (Gray, 2024). This overburden raises questions beyond the poor evaluation of the models that underlie ‘AI’ to the ‘AI’ research process itself, as well as to its likelihood of applying improvements.\n\nCollaboration to address data requirement challenges\nOne approach to both gathering the necessary training data at scale as well as the labels or annotations have been shown in collaborations between scientists and the general public. Online platforms host and facilitate the creation of various resources, ranging from media and other forms of data, labels and annotation projects, as well as forums for discussion. For example inaturalist.org is an online community with over 8 million users who make contributions in the form of images taken on their smartphones, and/or labels of the species in the images (Van Horn et al., 2018). Zooniverse.org is an online community of over 2.8M users that hosts projects defined by scientists to gather labels from non-scientists (Fortson et al., 2012). A third example is commonvoice.mozilla.org/en, which is a large dataset of speech transcription in 76 languages, provided by approximately 150k participants (Ardila et al., 2020).\nAnother similarly scalable infrastructure for dataset creation might be possible by adapting an academic publication format called the Registered Report (Chambers, 2013). Initially designed to compensate for editorial decisions being made based on the results, rather than the quality of the methods. In many fields, aspects of the data collection design, as well as the design of analysis and prediction of results occur a-priori, in principle not to bias interpretation of results. In a Registered Report, researchers submit a manuscript that includes information relevant to how the study will be conducted, including motivation of the work (i.e. introduction), details of data collection processes, as well as analyses. Typical review stages apply, i.e. suggestions for revisions or rejections, or the manuscript may receive an in-principle acceptance, whereby reviewers and editor agree to a publication should the methods used in the manuscript either follow closely the in-principle accepted version, or appropriate justifications be made for any changes that may have occurred. Thus acceptance of publications is made based on the strength of the methods, which also are strengthened by a peer-review process prior to data collection.\nThe Registered-Report format is exceptionally well-suited to the collection of datasets intended for AI training and/or evaluation. Firstly, they allow for peer-review prior to collection, whereby a panel of experts will provide critiques that will either strengthen the eventual design, or reject it in favor of publishing other stronger designs. Given the scope and resources needed to collect AI datasets, this format could be adapted such that it is published in its entirety prior to data collection. This may thus allow for a more public critique of the design prior to paying the resource cost of collection, and further allow for the submissions of responses in the form of data that conforms to the design in the published manuscript in a decentralized fashion, from multiple stakeholders, thus reducing the bias from any single data collection point and allowing for the sharing of financial and other resource burdens."
  },
  {
    "objectID": "index.html#data-challenges",
    "href": "index.html#data-challenges",
    "title": "A Need for a Contemporary Field",
    "section": "Data challenges",
    "text": "Data challenges\n\nTraining Data\nImproving the way we design and collect training data, especially data that is likely to be widely re-used, is key to developing trustworthy models (Liang et al., 2022; Welty et al., 2019), but requires investment in deliberate and informed design (Hullman et al., 2022), and maintenance (Liem & Demetriou, 2023). Beyond the challenge of gathering data at the necessary scale, is the challenge of ensuring that it is generally representative of the environment where it is to be deployed (Hullman et al., 2022), has sufficiently diverse coverage of the various scenarios it will encounter (Liang et al., 2022), and is as free as possible of bias (Mehrabi et al., 2021). Such a process requires deliberate design, and cannot be compensated for by increasing the amount of data collected (Hullman et al., 2022).\nIn some instances, data requirements may be too great for this to be currently possible with data that currently exists. Data scarcity, or a lack of sufficient training data, is a common issue with contemporary techniques e.g. deep learning, across machine learning fields e.g. computer vision, healthcare, natural language processing (Bansal et al., 2022). A number of solutions have been proposed for once data has been collected (Alzubaidi et al., 2023; Bansal et al., 2022), but a clearer solution might be to begin with more carefully collected data. Despite the need for solutions in this regard, the overwhelming focus in AI fields remains on algorithmic work (Birhane et al., 2022), and not the creation and curation of load-bearing datasets.\nThis issue is well illustrated with the most contemporary trends of Generative AI systems. Taking the development of Large Language Models (LLMs) as a use-case: trends show massive increases in the requirements of the training data, with the data for Llama 3 including 15T tokens, up from 2T for Llama 2. Although not all details of the datasets have been shared, and setting aside questions of copyright, licensing, and ethical concerns, any available text that is likely to have some quality is scarce compared to these requirements. For example, among the more refined sources of text available are the 6.9M English Wikipedia articles5 at an estimated 6.24B tokens6 - articles across all languages comprise only approximately 10 times that amount. If we extend our reach to other repositories, e.g. the approximately 85-90 million academic papers available on Sci-hub7 or SCOPUS8, we gain a rough estimate of 700B tokens. A similar figure might be estimated from libegen and the 7.5M9 books there. The internet archive has some 44M books and texts, which may yield up to 4.4T tokens if we assume ‘texts’ have approximately the same length as books. Thus, a more likely source for the ever-increasing data requirements are repositories of randomly selected text from the internet, like Common Crawl10. But this too has limits, and we are projected to have too little human-generated text to continue the increase in model size this decade, even if all of Common Crawl is used (Villalobos et al., 2024).\nBeyond the issue of merely acquiring data that exists, is the issue of the thus-far-ignored cost of training data. GPT-4 cost an estimated 40M USD to train, including human labor, hardware, and energy. Cost for frontier models is projected to increase at a rate of 2.4 times per year, reaching an estimated 1B USD to train by 2027 (Cottier et al., 2024). Notably, these cost calculations ignore the cost of producing the text itself, and though its value is difficult to calculate, estimates range from 10-1000 times more than the total cost of model training, and would exceed annual revenues of the organizations that are training the models, like OpenAI (Kandpal & Raffel, 2025). Thus, the resources spent do not include the costs of producing training data, for which lack of appropriate compensation has given rise to a number of lawsuits (e.g. Authors Guild vs. OpenAI 11). In other words, the more valuable thing is the data and not the model, and the few organizations that have the resources to train the models may not have been able to afford training if the data weren’t acquired at a near-0 cost.\nThus, the field of AI must wrestle with opposing issues: we want to train models with data that has high quality - e.g. we want the data to be representative of conditions where the model will be deployed, and thus relevant distributions in the training data must reflect the environment in which the models will be deployed (Hullman et al., 2022) - but the data requirements to train them appear thus far to be ever-increasing (Villalobos et al., 2024), and have thus far not included the actual cost of production (Kandpal & Raffel, 2025). We want the data used in training to be ‘good’ because we want the models to be ‘trustworthy’ - in the case of LLMs, we want to have reason to think they will generate text that includes claims that we think are true, and thus likely prefer text that contains information that is likely to be true. Perhaps our closest approximation to ‘what is likely to be true’ is contained in the perspectives presented across all of academic work and official reports. And yet, even if there were little to no barriers to using all of human academic work to train AI systems, this amount of text pales in quantity to the data requirements. And although thus far some models have been made available for academic use (e.g. Meta’s Llama), it is not clear for how long this will be the case, and when, if ever, academia will have the resources to train its own.\n\n\nEvaluation Data\nThere is substantial room for improvement in terms of the practices of data labeling as well, however implementations of such practices are also costly. The field of AI metrics distinguishes between a) observable AI system behavior, and b) the targets of measurement. In other words, a given score on a given ‘AI’ benchmark designed to measure ‘intelligence’ is not itself a direct measure of ‘intelligence’. AI metrics treats the indirectly observable phenomena as computational constructs - indirectly observable aspects of AI system behavior, a parallel to constructs in other fields - indirectly observable phenomena (Gignac & Szodorai, 2024). Thus the benchmark itself is a measurement instrument, aimed at measuring a specific aspect of the AI system.\nThe process that gives rise to the labels in the data that, which include the annotation interface and task instructions shown to annotators, is also a measurement instrument (Beck et al., 2022). The case study in this thesis contributes to the field of AI metrics by proposing enriching design, collection, analysis, and reporting of training/evaluation data for AI systems, using knowledge from the social sciences Jacobs & Wallach (2021), metrology (measurement science) (Welty et al., 2019), and work in the computational sciences on ‘ground-truthing’ (Cabitza et al., 2023). It requires a higher cost in terms of scientific labor, as it includes an a-priori empirical investigation of the data collection process in addition to the data collection process itself. In other words, it requires research on how to collect the data. Applying the knowledge to other tasks would require similar research, in principle for every combination of construct (i.e. the latent phenomenon of interest being measured), content (e.g. text, video, audio etc. and in some cases also subgroups, e.g. tweets vs. podcast transcripts vs. formal speeches etc.), and for relevant characteristics of annotators (i.e. ethnicity, political affiliation, etc.). These costs are in addition to the annotation labor.\nAlthough it is clear there is substantial room for improvement along with the repercussive benefits from such improvements, they will require resources. Despite growing recognition of evaluation data problems (Hullman et al., 2022), which in turn have the potential to lead to harms (Mehrabi et al., 2021), actual progress toward better annotation data practices, from collection, to analysis, to reporting remains slow. Current incentives prioritize immediately observable proxies for accuracy (Birhane et al., 2022) rather than careful measurement design (Beck et al., 2022) or long-term dataset stewardship (Liem & Demetriou, 2023). Beyond the challenge of gathering data at the necessary scale, are issues about the quality of the data itself, specifically the quality of the overall process that created the data. This challenge is essentially one of measurement (Welty et al., 2019).\nDatasets for evaluation very often contain human input (Daneshjou et al., 2021; Geiger et al., 2020, 2021; Sav et al., 2023). When collecting annotations, labels, or other forms of input from people in order to construct training/evaluation datasets, we are attempting to collect measurements of latent, unobservable constructs (Gignac & Szodorai, 2024; Jacobs & Wallach, 2021). In other words, when we examine data created from the observations of multiple people in aggregate, we do not directly observe e.g. the presence or absence of an object in a digital image; rather we observe the probability that a person from a given population will indicate the presence of absence of the object in the image (Welty et al., 2019). In the parlance of psychology, one cannot directly observe an other’s intelligence as one might observe an other’s height (Gignac & Szodorai, 2024). And even though height is observable, our measurements of it are still imperfect: in using a measurement device like a ruler multiple times, should we measure precisely enough, we would observe variance in each measurement with the true score for height imperfectly represented by our collection of imperfect measurements (Welty et al., 2019).\nAny standardized procedure for comparing two or more individuals is treated like a measurement instrument in the social sciences (Urbina, 2014). The repeatable procedures that we use to gather annotations from humans are similarly measurement instruments (Beck et al., 2022). In addition, because we use the datasets that we collect to benchmark the performance of models, they are also measurement instruments (Welty et al., 2019). Given the complexity of measuring unobservable phenomena, instruments in other fields are subjected to scrutiny prior to being considered usable for their intended purpose, in the form of studies that examine the properties of the measurements produced by the instrument. For example, the process of construct validation involves estimating the extent to which an instrument measures an unobservable construct (Wehner et al., 2020). It assumes an unknowable true score, and that all attempts to measure the true score are imperfect. There is no single solution to demonstrating the validity of a construct, but rather an accumulation of evidence, across multiple studies, with observations made using different methods (Smith, 2005).\nThus, the field of AI must wrestle with the opposing pressure of accuracy vs. the cost of developing measurement instruments for practically infinite use-cases: we want measurements that are ‘accurate’ - i.e. we want the data to be as clear a representation of our phenomenon of interest as it could be - but developing an accurate instrument is a costly exercise to add to the cost of collecting the data itself. Further, the topics we wish to measure in the ever-increasingly-complex AI systems as themselves measurement challenges. Topics like ‘fairness’ (Jacobs & Wallach, 2021), and ‘intelligence’ (Gignac & Szodorai, 2024) are not only challenging to develop instruments for, but also to define."
  },
  {
    "objectID": "index.html#labor-challenges",
    "href": "index.html#labor-challenges",
    "title": "A Need for a Contemporary Field",
    "section": "Labor Challenges",
    "text": "Labor Challenges\n\nScientific Labor\nAddressing the aforementioned data challenges will require substantial additional efforts. The human labor responsible for the collection, curation, and eventual annotation of training data in addition to the training of models (including researchers, engineers, and managers, but not data center employees and operations staff) is estimated at 29%-49% of the overall cost, which already amounts to tens of millions for frontier models (Cottier et al., 2024). Thus, a key challenge to implementing better practices involves providing labor both in terms of the scientific work necessary to design data collection and annotation processes, but also to provide the annotations themselves. However our current knowledge gathering apparatus - science as it is now practiced and reported - is dated, and inefficient. Thus, a substantial amount of the labor will likely have to come from an already overburdened work force. A re-consideration of what duties comprise academic work is thus warranted, specifically those that contribute to the body of human knowledge, and whether they can be modernized.\nOne key example involves academic writing and publication, specifically a primary source of scientific labor in academic settings, the PhD student, and their requirements to produce a dissertation or thesis document in order to progress in their career. PhD students are responsible for as much as a third of all academic output (Larivière, 2012), likely by working long hours, including weekend hours, constantly under time-pressure to produce output (Tienoven et al., 2024). Unsurprisingly, PhD students are a vulnerable population, showing a high prevalence of depression and anxiety - as high as 24%, and 17% respectively in a recent meta-analysis (Satinsky et al., 2021). Like all early career researchers (ECRs, i.e. PhD students, Post Doctoral Researchers, and Assistant Professors), PhD students, contribute substantially to the body of human knowledge via academic publication (Rørstad & Aksnes, 2015).\nHowever, PhD candidates are bogged down by dated requirements to write and defend theses despite the declining relevance of the document, and the contribution it makes to poor outcomes for the student. For example, the trend of thesis citations over time shows decline (Larivière et al., 2008). In the Netherlands where this thesis will be defended, evidence has shown that PhD candidates with fixed duration contracts are exceeding that duration by several months, resorting to completing their thesis on their own time and risking failing at completion (Van de Schoot et al., 2013). Though a key ceremonial moment, a thesis defense is substantially less rigorous than a publication: it is curated, as the ‘peer reviewers’ are chosen by the supervisors of the student - in the ‘real world’ of academia, on the other hand, peer reviewers are selected from far broader networks than those of the PhD candidate’s supervisory staff (Larivière, 2012). It is thus not surprising that publications, and not theses, remain the key factor in the assessment of the value of Academics as scientists (Anderson et al., 2022). Although more and more thesis content is being comprised of academic publications anyway (i.e. thesis by publication, Jackson (2013)), the substantial labor required to assemble works into a single document, write additional (introductory / conclusiory) chapters that are themselves complete manuscripts or nearly so, then help organize a formal event for the defense take away from more meaningful labor that this workforce could provide.\nA second key example involves a cornerstone of trust in science: the editorial peer-review process dates back several hundred years, and exists as a means to encourage and maintain the quality of scholarly work (Kelly et al., 2014). Early career and experienced researchers alike contribute to peer-review12, labor conservatively valued at over 1.5B USD in 2020 (Aczel et al., 2021). ECRs perform a substantial amount of the work often without credit13, and in some cases perform the work completely on their own (McDowell et al., 2019), especially in high-volume conferences like NeurIPS (Shah et al., 2018). This is a crucial note, as the submissions to high-volume conferences show massive increases year over year14 and struggle to meet the review requirements: e.g. NeurIPS 2021 required outreach to recruit over 1k additional peer-reviewers beyond the over 7k initial volunteers, to produce the 31k reviews needed for the conference15. As conferences continue to grow, the amount of review work is being divided among a regularly stable labor pool, reducing the attention paid to each individual work, a problem exacerbated by the repeat submission of borderline papers - i.e. papers that were nearly accepted (Zhang et al., 2022). This results in important flaws being missed: e.g. (Kapoor & Narayanan, 2023) show a growing crisis across 17 Machine Learning fields from a lack of trustworthy results due to data leakage.\nA third key example involves the ever-increasing volume of published manuscripts on AI and related topics16, which makes it impossible to stay abreast of the overall field. 10% of over 4 million publications indexed on the SCOPUS academic database in 2024 had terms related to AI in their title, keywords or abstracts (see Appendix A), up from around 7% in 2022. The number of submitted manuscripts also increases year over year, with popular conferences like NeurIPS receiving upwards of 12K submissions in 202317. These figures do not include preprints posted on servers like arXiv, which show over 42K works with AI related terms in the abstract for 2024, more than doubling the about 17.5K posts in 2019. This makes it more and more difficult to monitor the overall quality of the field (Zhang et al., 2022).\nThese examples highlight the overburden of academic scientific labor, which in turn raises questions about whether research in fields pertaining to AI can sustainably retain an appropriate level of quality, while meeting increasing demands. Predictably, and perhaps understandably, academics appear to be turning to Large Language Models (LLMs) for assistance, as evidence of its use is showing in academic work in both peer reviews (Liang et al., 2024), and in manuscripts (Gray, 2024). This is problematic as LLMs often report information that is not factual (Wang et al., 2024). However, a more appropriate direction might be to question how cornerstone components of academic work are conducted, and whether they can meet the needs of the field in the future.\n\n\nAnnotation Labor\nThe labor required to gather annotations is a key component of the evaluation data for machine learning or AI systems, and a substantial contributor to the related costs. In specialist categories, e.g. Figure Eight, which hires annotators for projects related to defense, salaries can range from 41K - 67K USD per year18. Although commonly used platforms like mTurk19 list only minimum rates of .01 USD per task, other services, e.g. Prolific20, hire on a per-task basis and pay at least minimum wage in the UK. Although these costs may seem manageable, they have the potential to balloon at scale. For example, Open AI hired a San Francisco-based firm that sourced annotation labor from Kenya, Uganda, and India to provide the human inputs necessary to fine tune their models 21. Despite the pay rate of 2 USD per hour being far less than the 7.25 USD federal minimum wage in the US22, OpenAI spent 600K USD in 2021 to label text as being violent, sexual, or hatespeech alone. While the availability of such services has allowed for rapid gathering of evaluation data for AI-related projects, works that highlight data scarcity warn that it may be insufficient to meet the needs of coming models, and that the costs of such labor will be very high.\nWork in the case study of this thesis further suggests that more annotation work may be required than per project than is current, as it requires a focus on the development of an annotation instrument prior to primary data collection phases, pre-studies to estimate the required number of annotators, and accounting for possible variance in perspectives. For high-stakes use-cases, such as applications in government or healthcare, we may wish to have data independently examined prior to its certification for use, similarly to how some scholars recommend external examination of AI models prior to deployment (Garrett & Rudin, 2024), which may in turn require further iterations of data collection."
  },
  {
    "objectID": "index.html#collaboration-to-address-data-requirement-challenges",
    "href": "index.html#collaboration-to-address-data-requirement-challenges",
    "title": "A Need for a Contemporary Field",
    "section": "Collaboration to address data requirement challenges",
    "text": "Collaboration to address data requirement challenges\nOne approach to both gathering the necessary training data at scale as well as the labels or annotations have been shown in collaborations between scientists and the general public. Online platforms host and facilitate the creation of various resources, ranging from media and other forms of data, labels and annotation projects, as well as forums for discussion. For example inaturalist.org is an online community with over 8 million users who make contributions in the form of images taken on their smartphones, and/or labels of the species in the images (Van Horn et al., 2018). Zooniverse.org is an online community of over 2.8M users that hosts projects defined by scientists to gather labels from non-scientists (Fortson et al., 2012). A third example is commonvoice.mozilla.org/en, which is a large dataset of speech transcription in 76 languages, provided by approximately 150k participants (Ardila et al., 2020).\nAnother similarly scalable infrastructure for dataset creation might be possible by adapting an academic publication format called the Registered Report (Chambers, 2013). Initially designed to compensate for editorial decisions being made based on the results, rather than the quality of the methods. In many fields, aspects of the data collection design, as well as the design of analysis and prediction of results occur a-priori, in principle not to bias interpretation of results. In a Registered Report, researchers submit a manuscript that includes information relevant to how the study will be conducted, including motivation of the work (i.e. introduction), details of data collection processes, as well as analyses. Typical review stages apply, i.e. suggestions for revisions or rejections, or the manuscript may receive an in-principle acceptance, whereby reviewers and editor agree to a publication should the methods used in the manuscript either follow closely the in-principle accepted version, or appropriate justifications be made for any changes that may have occurred. Thus acceptance of publications is made based on the strength of the methods, which also are strengthened by a peer-review process prior to data collection.\nThe Registered-Report format is exceptionally well-suited to the collection of datasets intended for AI training and/or evaluation. Firstly, they allow for peer-review prior to collection, whereby a panel of experts will provide critiques that will either strengthen the eventual design, or reject it in favor of publishing other stronger designs. Given the scope and resources needed to collect AI datasets, this format could be adapted such that it is published in its entirety prior to data collection. This may thus allow for a more public critique of the design prior to paying the resource cost of collection, and further allow for the submissions of responses in the form of data that conforms to the design in the published manuscript in a decentralized fashion, from multiple stakeholders, thus reducing the bias from any single data collection point and allowing for the sharing of financial and other resource burdens."
  },
  {
    "objectID": "index.html#reporting-challenges",
    "href": "index.html#reporting-challenges",
    "title": "A Need for a Contemporary Field",
    "section": "Reporting challenges",
    "text": "Reporting challenges\n\nVolume and synthesis\nReceiving declining attention are theses. Components currently largely absent are peer-reviews (although open review tries to address this) and rejected manuscripts - many of which get resubmitted [citation]. Partially absent are teaching materials, conference materials, e.g. posters, slides, and recordings of talks etc. Importantly, code scripts and/or notebooks, various forms of data, parameters and settings, and the actual model(s) produced are often absent. This also doesn’t include replications, which have been shown to be useful in other fields. Only recently are there readily available guidelines to more completely report key information: data/model | sheets / cards, (Geiger et al., 2021).\nMany of the outputs of academic work would benefit from some sort of maintenance. i.e. we must treat academic insights like software artifacts, with management and possibly updates.\nReviewer quality, and the difficulty in finding reviewers."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "A Need for a Contemporary Field",
    "section": "Introduction",
    "text": "Introduction\nThe future quality of AI systems will be shaped by the data, evaluation, and reporting practices we establish today. AI systems are a product of the data used to train and evaluate them, thus we indirectly shape their behavior by the processes we design to collect training and evaluation data (Hullman et al., 2022). Training and evaluation data have ongoing effects as they are often re-used (Geiger et al., 2021), likely because of their high cost. Data professionals spend more time on data preparation and cleansing than they do on model selection, training and deployment 1 2 3, and yet the focus remains on the latter (Birhane et al., 2022) with data professionals indicating that they prefer the ‘model work’ over the ‘data work’ (Sambasivan et al., 2021).\n\n“Instead of focusing on the code, companies should focus on developing systematic engineering practices for improving data in ways that are reliable, efficient, and systematic. In other words, companies need to move from a model-centric approach to a data-centric approach.” - Andrew Ng4\n\nDecisions made when collecting data comprise the design, whether or not they are made consciously and carefully. Work over the past decade has emerged explaining limitations of decisions made in the design of commonly re-used datasets, which include a lack of representativeness (Hullman et al., 2022), measurement quality (Jacobs & Wallach, 2021), accounting for the full range of reasonable interpretations in terms of annotations (Cabitza et al., 2023), and completeness in reporting of the annotation process (Daneshjou et al., 2021; Geiger et al., 2021). Evidence points to a need for a more sophisticated approach to the design of datasets in AI fields, especially frequently re-used benchmarks that become ‘load-bearing’ cornerstones in their niches.\nAlthough improvements to data collection processes have been proposed, they are at best slowly being adopted, and at worst being largely ignored. The increasingly sophisticated systems we build have an accompanying cost not only in terms of sheer training data size, but also in terms of the labor required to curate the data (Kandpal & Raffel, 2025). This is exacerbated by the increasing scale required to train systems using the most advanced methods (Villalobos et al., 2024), and increasingly sophisticated qualities we aim to evaluate systems on, that in turn are challenging to define and measure e.g. fairness (Jacobs & Wallach, 2021), and intelligence (Gignac & Szodorai, 2024). Furthermore, as many systems aim to behave similar to humans, the human-like behavior of the sophisticated systems we are evaluating are a prime target for anthropomorphistically biased mis-interpretations of their outputs (Altmeyer et al., 2024).\nThe degree to which we invest in researching best practices for the design, collection, and analysis of training and evaluation data will determine the real-world performance of the AI systems we will build. It is thus crucial that we put in place better practices. The emerging field of AI metrics focuses on developing best practices for the measurement AI system performance (Gignac & Szodorai, 2024), by treating datasets as measurement instruments to be developed and maintained (Welty et al., 2019). However, improving how AI fields collect and label data requires substantial efforts beyond the already substantial efforts invested. Resources must thus be dedicated to determining what best practices are, which will require dedicated research to solve issues unique to training vs. evaluation data, and to implementing said practices.\n\nToo high a cost?\nQuestions remain as to how AI research as it is currently conducted can develop and implement solutions. This conclusiory manuscript thus highlights the crucial challenge for academic study of AI in the coming decade: developing an infrastructure that allows for the study of AI, including the data that are its raw materials, with little - or at the very least, substantially less - harmful bias. It highlights shortcomings in the data used for training, and the accompanying ‘ground truth’ (e.g. labels, annotations etc.), along with challenges in gathering these at scale, as well as the scientific and annotation labor necessary to meet these challenges. It highlights the need for identifiable academic publication venues that gather and disseminate works on the study of data design, collection, analysis, and reporting of data for AI training and evaluation, as well as more modern publication formats that allow for dataset requirements to be studied prior to their collection, and for infrastructure that allows the burden of their collection to be distributed among stakeholders. It concludes that, while works like the case study embedded in this thesis are necessary, the various fields studying topics related to AI are poorly positioned to implement them without substantial modernization."
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "A Need for a Contemporary Field",
    "section": "",
    "text": "AI systems are shaped by the training data, algorithm, and evaluation data used to build them. Recent work has shown a need for greater amounts of data, and greater effort in the design, collection, analysis and reporting of data used for building AI systems. Solving these problems may require more effort and resources than the field can offer. Present work makes suggestions aimed at making better use of labor and resources in the AI research field to meet these needs."
  },
  {
    "objectID": "index.html#a-conteporary-field",
    "href": "index.html#a-conteporary-field",
    "title": "A Need for a Contemporary Field",
    "section": "A conteporary field",
    "text": "A conteporary field\n\nAI Metrics should be a field\nData used for training and evaluation of models is of central importance. Solutions to problems in both training and evaluation data collection require deliberate study for standards to improve and disseminate. In other words, beyond the implementation of currently thought best practices, is the requirement that the best practices be improved along with demands, and knowledge of them disseminated such that general practice is affected.\nAs it stands, there is no central field of study for this topic, despite its central importance to all AI-related fields. Other disciplines (psychology, economics, software engineering) have entire fields dedicated to the design, collection, analysis, and reporting of data that involves human behavior (psychometrics, econometrics, software testing). Although datasets that become ‘load bearing’ are frequently cited e.g. Imagenet, AI Metrics itself is not a central focus at the most highly cited academic publication venues, nor is there a focused publication venue or conference. Thus, crucial topics remain under-resourced and under-researched.\n\n\nCollaboration to address data requirement challenges\nThe human labor responsible for the collection, curation, and eventual annotation of training data in addition to the training of the model (including researchers, engineers, and managers, but not data center employees and operations staff) is estimated at 29%-49% of the overall cost, which already amounts to tens of millions for frontier models (Cottier et al., 2024). Thus, a key challenge to implementing better practices involves providing labor both in terms of the scientific work necessary to design data collection and annotation processes, but also to provide the annotations themselves.\nOne approach to both gathering the necessary training data at scale as well as the labels or annotations have been shown in collaborations between scientists and the general public. Online platforms host and facilitate the creation of various resources, ranging from media and other forms of data, labels and annotation projects, as well as forums for discussion. For example inaturalist.org is an online community with over 8 million users who make contributions in the form of images taken on their smartphones, and/or labels of the species in the images (Van Horn et al., 2018). Zooniverse.org is an online community of over 2.8M users that hosts projects defined by scientists to gather labels from non-scientists (Fortson et al., 2012). A third example is commonvoice.mozilla.org/en, which is a large dataset of speech transcription in 76 languages, provided by approximately 150k participants (Ardila et al., 2020). People are willing to even share de-identified medical data as long as they are given agency over it (Liang et al., 2022). This approach simultaneously allows for the contribution of diverse media for annotation, labor needed to annotate them.\nAnother similarly scalable infrastructure for dataset creation within academia might be possible by adapting an academic publication format called the Registered Report (Chambers, 2013). Initially designed to compensate for editorial decisions being made based on the results, rather than the quality of the methods. In many fields, aspects of the data collection design, as well as the design of analysis and prediction of results occur a-priori, in principle not to bias interpretation of results. In a Registered Report, researchers submit a manuscript that includes information relevant to how the study will be conducted, including motivation of the work (i.e. introduction), details of data collection processes, as well as analyses. Typical review stages apply, i.e. suggestions for revisions or rejections, or the manuscript may receive an in-principle acceptance, whereby reviewers and editor agree to a publication should the methods used in the manuscript either follow closely the in-principle accepted version, or appropriate justifications be made for any changes that may have occurred. Thus acceptance of publications is made based on the strength of the methods, which also are strengthened by a peer-review process prior to data collection.\nThe Registered-Report format is exceptionally well-suited to the collection of datasets intended for AI training and/or evaluation. Firstly, they allow for peer-review prior to collection, whereby a panel of experts will provide critiques that will either strengthen the eventual design, or reject it in favor of publishing other stronger designs. Given the scope and resources needed to collect AI datasets, this format could be adapted such that it is published in its entirety prior to data collection. This may thus allow for a more public critique of the design prior to paying the resource cost of collection, and further allow for the submissions of responses in the form of data that conforms to the design in the published manuscript in a decentralized fashion, from multiple stakeholders, thus reducing the bias from any single data collection point and allowing for the sharing of financial and other resource burdens. As online platforms focused on different topics proliferate, they may be attached to approved data collection protocols.\nA further approach to contributing more scientific labor involves undergraduate bachelor and masters students. CREP is a crowdsourced initiative where undergraduate students, under faculty supervision, replicate high-impact psychology studies, thus allowing for direct instruction of students while provided needed replications of pivotal studies 17. These replications are pre-registered, and may be published. For instance, a meta-analysis of nine student-led replications of the “red-romance effect” found no significant effect (Wagge et al., 2019), thus functioning both to instruct students and contribute to the scientific record. Similar approaches can be taken towards annotating data, with students replicating registered data collection protocols, or supplying the annotations themselves.\n\n\nContemporary Approaches to Academic publication\nOSF.io and github have become repositories for materials. Open Review allows for peer reviews of works, as well as a trail of the reviews.\nLiving systematic reviews are useful for rapidly evolving fields. Notably, works are appearing that aim to develop AI tools to assist in the writing of such reviews by assisting with screening []."
  },
  {
    "objectID": "index.html#a-contemporary-field",
    "href": "index.html#a-contemporary-field",
    "title": "A Need for a Contemporary Field",
    "section": "A contemporary field",
    "text": "A contemporary field\nResearch on AI and related fields will require contemporary solutions to address problems related to training and evaluation data, and scientific and annotation labor. Although no clear solutions have been derived, some promising directions have been observed in recent years.\n\nCollaborative-platform data collection\nOne approach to both gathering the necessary training data at scale as well as the labels or annotations have been shown in collaborations between scientists and the general public. Online platforms, often initiated by academics, host and facilitate the creation of various resources, ranging from media and other forms of data, labels and annotation projects as well as forums for discussion. Of note, it has been shown that people are willing to even share de-identified medical data as long as they are given sufficient transparency and agency over its use (Liang et al., 2022). For example iNaturalist.org is an online community with over 8 million users who make contributions in the form of images taken on their smartphones, and/or labels of the species in the images (Van Horn et al., 2018). Zooniverse.org is an online community of over 2.8M users that hosts projects defined by scientists to gather labels from non-scientists (Fortson et al., 2012). A third example is Commonvoice, which is a large dataset of speech transcription in 76 languages, provided by approximately 150k participants (Ardila et al., 2020). Immediate challenges related to privacy must be solved, however this approach does simultaneously allow for the contribution of diverse media for annotation, and labor needed to annotate them.\nA means to further encourage collaborative data collection, that also allows for peer-review feedback prior to the collection, and that sits more closely within academia is the Registered Report (Chambers, 2013): an academic publication format where researchers submit a manuscript prior to data collection, which includes all information relevant to how the study will be conducted, including motivation of the work (i.e. introduction), details of data collection processes, as well as planned analyses. Typical review stages apply, i.e. suggestions for revisions or rejections, or an in-principle acceptance, whereby reviewers and editor agree to a publication should the methods used in the manuscript either follow closely the in-principle accepted version, or appropriate justifications be made for any changes made. This format was designed to allow for editorial decisions being made based on the strength of the methods, without the influence of the results, and to also strengthened the methods via peer-review prior to data collection.\nThe Registered Report format is exceptionally well-suited to the collection of data intended for AI training and/or evaluation. Firstly, it allows for peer-review prior to collection, whereby a panel of experts will provide critiques that will either strengthen the eventual design, or reject it in favor of publishing other stronger designs. Given the scope and resources needed to collect AI datasets, this format may thus allow for a more public critique of the design prior to investment of resource cost of collection. Secondly, it further allows for the submissions of responses in the form of data that conforms to the design in the published manuscript in a decentralized fashion, from multiple stakeholders, thus reducing the bias from any single data collection point and allowing for the sharing of financial and other resource burdens. Thirdly, as online platforms like Zooniverse grow and proliferate to focus on different topics and domains, hosted projects may be attached to approved data collection protocols. And lastly, platforms allow for further insight from input via the community of users. This could facilitate discovery, as was the case when a platform user discovered a new astronomical object - now named ‘Hanny’s Voorwerp23’ after her - while classifying galaxies on a platform called Galaxy Zoo24.\nA third approach integrates instruction with data collection by involving bachelor and masters students. The Collaborative Replications and Education Project (CREP) is an initiative where undergraduate students, under faculty supervision, replicate high-impact psychology studies. The aim is to allow for direct instruction of students while provided needed replications of pivotal studies25. One example output is a meta-analysis of nine student-led replications which showed a lack of evidence of the “red- (Wagge et al., 2019), which functioned both to instruct students and contribute to the scientific record. Similar approaches can be taken towards annotating data with students replicating registered data collection protocols, or supplying the annotations themselves. This may result in an increase in quality of annotations over crowd-sourcing platforms like mTurk.\n\n\nPublication and Maintenance of academic artifacts\nA number of publication formats currently exist that can assist in thoroughly summarizing and synthesizing evidence for topics in which there is sufficient volume of publications. Systematic Reviews26 and Meta-Analyses27 attempt to gather all relevant works on a topic, synthesize the evidence, extract insights and highligh research opportunities on a given topic. In fields where there is an exceptional amount of work on a topic, or set of related topics, Umbrella Reviews28 attempt to synthesize collections of systematic reviews and/or meta-analyses. While useful, the accuracy and currency of these formats atrophies in fields that are very rapidly developing. Living Systematic Reviews are useful for keeping up-to-date information visible, particularly in rapidly evolving fields (Elliott et al., 2017). Living Systematic Reviews use the same methods as any form of academic review (e.g. Systematic Reviews, Meta Analyses, etc.) for selecting, reviewing, and synthesizing available evidence, but include an a-priori commitment to gather and update what is reported for a given length of time. Although searching and selecting items for inclusion can be time consuming, they can be assisted with automation (Thomas et al., 2017), for which many open-source, free tools are available (e.g. asreview29).\nThe practice of science produces a number of artifacts beyond the paper that reports results of the work, including data, models, code / notebooks, configurations and lab notes. These artifacts are a relevant part of the work of science, although they are often obscured from view, and like software artifacts, would benefit from regular maintenance (Liem & Demetriou, 2023). With AI-related artifacts specifically, sites like Replicate30 and Huggingface31 have emerged that allow for hosting, documentation, and sharing of artifacts. Sites like the Open Science Framework32 have emerged that allow for the flexible documentation of various components of any scientific workflow as well as updates, for academia specifically. Similarly, online git-compatible repositories like GitHub can be leveraged not only as containers for the artifacts, but as easily maintainable documentation of the evolution of various projects (Wattanakriengkrai et al., 2022).\nTaking these a step further, platforms like Quarto33 allow for directly reproducible documents to be extracted from code agnostic notebooks, thereby shrinking the distance between published papers and the working environment of scientists.\nOpen Review allows for peer reviews of works, as well as a trail of the reviews.\n\n\nAI Metrics as a field\nData used for training and evaluation of models is of central importance. Solutions to problems in both training and evaluation data collection require deliberate study for standards to improve and disseminate. In other words, beyond the implementation of currently thought best practices, is the requirement that the best practices be improved along with demands, and knowledge of them disseminated such that general practice is affected.\nAs it stands, there is no central field of study for this topic, despite its central importance to all AI-related fields. Other disciplines (psychology, economics, software engineering) have entire fields dedicated to the design, collection, analysis, and reporting of data that involves human behavior (psychometrics, econometrics, software testing). Although datasets that become ‘load bearing’ are frequently cited e.g. Imagenet, AI Metrics itself is not a central focus at the most highly cited academic publication venues, nor is there a focused publication venue or conference. Thus, crucial topics remain under-resourced and under-researched.\nBenchmarks and measurement"
  }
]