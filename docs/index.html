<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>A Need for a Contemporary Field – thesis_conclusion</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-6bd9cfa162949bde0a231f530c97869d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">thesis_conclusion</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="./index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#abstract" id="toc-abstract" class="nav-link active" data-scroll-target="#abstract">Abstract</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a>
  <ul class="collapse">
  <li><a href="#too-high-a-cost" id="toc-too-high-a-cost" class="nav-link" data-scroll-target="#too-high-a-cost">Too high a cost?</a></li>
  </ul></li>
  <li><a href="#data-challenges" id="toc-data-challenges" class="nav-link" data-scroll-target="#data-challenges">Data challenges</a>
  <ul class="collapse">
  <li><a href="#training-data" id="toc-training-data" class="nav-link" data-scroll-target="#training-data">Training Data</a></li>
  <li><a href="#evaluation-data" id="toc-evaluation-data" class="nav-link" data-scroll-target="#evaluation-data">Evaluation Data</a></li>
  </ul></li>
  <li><a href="#labor-challenges" id="toc-labor-challenges" class="nav-link" data-scroll-target="#labor-challenges">Labor Challenges</a>
  <ul class="collapse">
  <li><a href="#scientific-labor" id="toc-scientific-labor" class="nav-link" data-scroll-target="#scientific-labor">Scientific Labor</a></li>
  <li><a href="#annotation-labor" id="toc-annotation-labor" class="nav-link" data-scroll-target="#annotation-labor">Annotation Labor</a></li>
  </ul></li>
  <li><a href="#a-contemporary-field" id="toc-a-contemporary-field" class="nav-link" data-scroll-target="#a-contemporary-field">A contemporary field</a>
  <ul class="collapse">
  <li><a href="#collaborative-platform-data-collection" id="toc-collaborative-platform-data-collection" class="nav-link" data-scroll-target="#collaborative-platform-data-collection">Collaborative-platform data collection</a></li>
  <li><a href="#publication-and-maintenance-of-academic-artifacts" id="toc-publication-and-maintenance-of-academic-artifacts" class="nav-link" data-scroll-target="#publication-and-maintenance-of-academic-artifacts">Publication and Maintenance of academic artifacts</a></li>
  <li><a href="#ai-metrics-as-a-field" id="toc-ai-metrics-as-a-field" class="nav-link" data-scroll-target="#ai-metrics-as-a-field">AI Metrics as a field</a></li>
  </ul></li>
  <li><a href="#a-call-to-action" id="toc-a-call-to-action" class="nav-link" data-scroll-target="#a-call-to-action">A Call to Action</a></li>
  <li><a href="#sec-appendix" id="toc-sec-appendix" class="nav-link" data-scroll-target="#sec-appendix">Appendix</a>
  <ul class="collapse">
  <li><a href="#appendix-a-citation-trends-plot" id="toc-appendix-a-citation-trends-plot" class="nav-link" data-scroll-target="#appendix-a-citation-trends-plot">Appendix A: Citation Trends Plot</a></li>
  <li><a href="#appendix-b-search-terms" id="toc-appendix-b-search-terms" class="nav-link" data-scroll-target="#appendix-b-search-terms">Appendix B: Search terms</a>
  <ul class="collapse">
  <li><a href="#scopus" id="toc-scopus" class="nav-link" data-scroll-target="#scopus">SCOPUS:</a></li>
  <li><a href="#arxiv" id="toc-arxiv" class="nav-link" data-scroll-target="#arxiv">arXiv:</a></li>
  </ul></li>
  <li><a href="#appendix-c-token-estimates" id="toc-appendix-c-token-estimates" class="nav-link" data-scroll-target="#appendix-c-token-estimates">Appendix C: Token estimates</a></li>
  </ul></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="index.docx"><i class="bi bi-file-word"></i>MS Word</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">A Need for a Contemporary Field</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<p>AI systems are shaped by the training data, algorithm, and evaluation data used to build them. Recent work has shown a need for greater amounts of data, and greater effort in the design, collection, analysis and reporting of data used for building AI systems. Solving these problems may require more effort and resources than the field can offer. Present work highlights possible directions aimed at making better use of labor and resources in the AI research field to meet these needs.</p>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>The future quality of AI systems will be shaped by the data, evaluation, and reporting practices we establish today. As AI systems are a product of the data sets used to train and evaluate them, we shape their behavior by the processes we design to collect training and evaluation data <span class="citation" data-cites="hullman2022worst">(<a href="#ref-hullman2022worst" role="doc-biblioref">Hullman et al., 2022</a>)</span>. Training and evaluation data have ongoing effects on AI and related fields as they are often re-used <span class="citation" data-cites="geiger2021garbage">(<a href="#ref-geiger2021garbage" role="doc-biblioref">Geiger et al., 2021</a>)</span>, likely because of the high effort and cost required to collect them. Altough data professionals spend more time on data preparation and cleansing than they do on model selection, training and deployment <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>, the focus in AI and related fields remains on the latter <span class="citation" data-cites="birhane2022values">(<a href="#ref-birhane2022values" role="doc-biblioref">Birhane et al., 2022</a>)</span>, possibly by preference as data professionals indicate they prefer the ‘model work’ over the ‘data work’ <span class="citation" data-cites="sambasivan2021everyone">(<a href="#ref-sambasivan2021everyone" role="doc-biblioref">Sambasivan et al., 2021</a>)</span>.</p>
<blockquote class="blockquote">
<p><em>“Instead of focusing on the code, companies should focus on developing systematic engineering practices for improving data in ways that are reliable, efficient, and systematic. In other words, companies need to move from a model-centric approach to a data-centric approach.”</em> - Andrew Ng<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
</blockquote>
<p>Decisions made when collecting data comprise the design, whether or not they are made consciously and carefully. Work over the past decade has emerged explaining limitations of decisions made in the design of commonly re-used datasets, which include a lack of representativeness <span class="citation" data-cites="hullman2022worst">(<a href="#ref-hullman2022worst" role="doc-biblioref">Hullman et al., 2022</a>)</span>, measurement quality <span class="citation" data-cites="jacobs2021measurement">(<a href="#ref-jacobs2021measurement" role="doc-biblioref">Jacobs &amp; Wallach, 2021</a>)</span>, accounting for the full range of reasonable interpretations of annotations<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> <span class="citation" data-cites="cabitza2023toward">(<a href="#ref-cabitza2023toward" role="doc-biblioref">Cabitza et al., 2023</a>)</span>, and completeness in reporting of the annotation process <span class="citation" data-cites="geiger2021garbage daneshjou2021lack">(<a href="#ref-daneshjou2021lack" role="doc-biblioref">Daneshjou et al., 2021</a>; <a href="#ref-geiger2021garbage" role="doc-biblioref">Geiger et al., 2021</a>)</span>. Evidence points to a need for a more sophisticated approach to the design of datasets in AI and related fields, especially frequently re-used benchmarks that become ‘load-bearing’ cornerstones in their niches.</p>
<p>Although improvements to data collection processes have been proposed, they are at best slowly being adopted, and at worst being largely ignored. The increasingly sophisticated systems we build have an accompanying cost not only in terms of sheer training data amount, but also in terms of the labor required to curate the data <span class="citation" data-cites="kandpal2025position">(<a href="#ref-kandpal2025position" role="doc-biblioref">Kandpal &amp; Raffel, 2025</a>)</span>. This is exacerbated by the increasing scale required to train systems using the most advanced methods <span class="citation" data-cites="villalobos2024position">(<a href="#ref-villalobos2024position" role="doc-biblioref">Villalobos et al., 2024</a>)</span>, and increasingly sophisticated qualities we aim to evaluate systems on, that in turn are challenging to define and measure - e.g.&nbsp;<em>fairness</em> <span class="citation" data-cites="jacobs2021measurement">(<a href="#ref-jacobs2021measurement" role="doc-biblioref">Jacobs &amp; Wallach, 2021</a>)</span>, and <em>intelligence</em> <span class="citation" data-cites="gignac2024defining">(<a href="#ref-gignac2024defining" role="doc-biblioref">Gignac &amp; Szodorai, 2024</a>)</span>. This in turn is exacerbated by the aim of many systems to behave similarly to humans, making the human-like behavior of the sophisticated systems we are evaluating are a prime target for anthropomorphistically biased mis-interpretations of their outputs <span class="citation" data-cites="altmeyer2024position">(<a href="#ref-altmeyer2024position" role="doc-biblioref">Altmeyer et al., 2024</a>)</span>.</p>
<p>The degree to which we invest in researching best practices for the design, collection, and analysis of training and evaluation data will determine the real-world performance of the AI systems we will build. It is thus crucial that we put in place better practices. An emerging field - called <em>AI metrics -</em> does focus on developing best practices for the measurement of AI system performance <span class="citation" data-cites="gignac2024defining">(<a href="#ref-gignac2024defining" role="doc-biblioref">Gignac &amp; Szodorai, 2024</a>)</span> by treating data sets and the processes by which we gather them as measurement instruments to be developed and maintained <span class="citation" data-cites="welty2019metrology">(<a href="#ref-welty2019metrology" role="doc-biblioref">Welty et al., 2019</a>)</span>. However, improving how AI fields collect and label data requires substantial efforts beyond the already substantial efforts invested, along with appropriate incentives, and an infrastructure. Resources must thus be dedicated to determining what best practices are, which will require dedicated research to solve issues unique to training vs.&nbsp;evaluation data, and to implementing said practices.</p>
<section id="too-high-a-cost" class="level3">
<h3 class="anchored" data-anchor-id="too-high-a-cost">Too high a cost?</h3>
<p>Questions remain as to how AI research as it is currently conducted can develop and implement solutions. This conclusiory manuscript thus highlights the crucial challenge for academic study of AI in the coming decade: developing an infrastructure that allows for the study of AI, including the data that are its raw materials, with little - or at the very least, substantially less - harmful bias. It highlights shortcomings in the data used for training, and the accompanying annotations, along with challenges in gathering these at scale, as well as the scientific and annotation labor necessary to meet these challenges. It highlights the need for identifiable academic publication venues that gather and disseminate works on the study of data design, collection, analysis, and reporting of data for AI training and evaluation, as well as more modern publication formats that allow for dataset requirements to be studied prior to their collection, and for infrastructure that allows the burden of their collection to be distributed among stakeholders. It concludes that, while works like the case study embedded in this thesis are necessary, the various fields studying topics related to AI are poorly positioned to implement them without substantial modernization.</p>
</section>
</section>
<section id="data-challenges" class="level2">
<h2 class="anchored" data-anchor-id="data-challenges">Data challenges</h2>
<section id="training-data" class="level3">
<h3 class="anchored" data-anchor-id="training-data">Training Data</h3>
<p>Improving the way we design and collect training data, especially data that is likely to be widely re-used, is key to developing trustworthy models <span class="citation" data-cites="liang2022advances welty2019metrology">(<a href="#ref-liang2022advances" role="doc-biblioref">Liang et al., 2022</a>; <a href="#ref-welty2019metrology" role="doc-biblioref">Welty et al., 2019</a>)</span>, but requires investment in deliberate and informed design <span class="citation" data-cites="hullman2022worst">(<a href="#ref-hullman2022worst" role="doc-biblioref">Hullman et al., 2022</a>)</span>, and maintenance <span class="citation" data-cites="liem2023treat">(<a href="#ref-liem2023treat" role="doc-biblioref">Liem &amp; Demetriou, 2023</a>)</span>. Beyond the challenge of gathering data at the necessary scale, is the challenge of ensuring that it is generally representative of the environment where it is to be deployed <span class="citation" data-cites="hullman2022worst">(<a href="#ref-hullman2022worst" role="doc-biblioref">Hullman et al., 2022</a>)</span>, has sufficiently diverse coverage of the various scenarios it will encounter <span class="citation" data-cites="liang2022advances">(<a href="#ref-liang2022advances" role="doc-biblioref">Liang et al., 2022</a>)</span>, and is as free as possible of bias <span class="citation" data-cites="mehrabi2021survey">(<a href="#ref-mehrabi2021survey" role="doc-biblioref">Mehrabi et al., 2021</a>)</span>. Such a process requires deliberate design, and cannot be compensated for by increasing the amount of data collected <span class="citation" data-cites="hullman2022worst">(<a href="#ref-hullman2022worst" role="doc-biblioref">Hullman et al., 2022</a>)</span>.</p>
<p>In some instances, data requirements may be too great for this to be currently possible with data that currently exists. <em>Data scarcity</em>, or a lack of sufficient training data, is a common issue with contemporary techniques e.g.&nbsp;deep learning, across machine learning fields e.g. computer vision, healthcare, natural language processing <span class="citation" data-cites="bansal2022systematic">(<a href="#ref-bansal2022systematic" role="doc-biblioref">Bansal et al., 2022</a>)</span>. A number of solutions have been proposed for once data has been collected <span class="citation" data-cites="bansal2022systematic alzubaidi2023survey">(<a href="#ref-alzubaidi2023survey" role="doc-biblioref">Alzubaidi et al., 2023</a>; <a href="#ref-bansal2022systematic" role="doc-biblioref">Bansal et al., 2022</a>)</span>, but a clearer solution might be to begin with data that has been more carefully designed and collected. Despite the need for solutions in this regard, the overwhelming focus in AI fields remains on algorithmic work <span class="citation" data-cites="birhane2022values">(<a href="#ref-birhane2022values" role="doc-biblioref">Birhane et al., 2022</a>)</span>, and not the creation and curation of load-bearing datasets.</p>
<p>This issue is well illustrated with the most contemporary trends of Generative AI systems. Taking the development of Large Language Models (LLMs) as a use-case: trends show massive increases in the requirements of the training data, with the data for Llama 3 including approximately 15T tokens, up from approximately 2T for Llama 2<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>. Although not all details of the datasets have been shared, and setting aside questions of copyright, licensing, and ethical concerns, any available text that is likely to have some quality is scarce compared to these requirements. For example, among the more refined sources of text available are the 6.9M English Wikipedia articles<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> at an estimated 6.24B tokens<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> - articles across all languages comprise only approximately 10 times that amount. If we extend our reach to other repositories, e.g.&nbsp;the approximately 85-90 million academic papers available on Sci-hub<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> or SCOPUS<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>, we gain roughly 700B tokens. A similar figure might be estimated from libegen and the 7.5M<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> books there. The internet archive has some 44M books and texts, which may yield up to 4.4T tokens if we assume ‘texts’ have approximately the same length as books. Thus, a more likely source for the ever-increasing data requirements are repositories of randomly selected text from the internet, like Common Crawl<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>. But this too has limits, and we are projected to have too little human-generated text to continue the increase in model size this decade, even if all of Common Crawl is used <span class="citation" data-cites="villalobos2024position">(<a href="#ref-villalobos2024position" role="doc-biblioref">Villalobos et al., 2024</a>)</span>.</p>
<p>Beyond the issue of merely acquiring data that exists, is the issue of the thus-far-ignored cost of training data. GPT-4 cost an estimated 40M USD to train, including human labor, hardware, and energy. Cost for frontier models is projected to increase at a rate of 2.4 times per year, reaching an estimated 1B USD to train by 2027 <span class="citation" data-cites="cottier2024rising">(<a href="#ref-cottier2024rising" role="doc-biblioref">Cottier et al., 2024</a>)</span>. Notably, these cost calculations ignore the cost of producing the text itself, and though its value is difficult to calculate, estimates range from 10-1000 times more than the total cost of model training, and would exceed annual revenues of the organizations that are training the models, like OpenAI <span class="citation" data-cites="kandpal2025position">(<a href="#ref-kandpal2025position" role="doc-biblioref">Kandpal &amp; Raffel, 2025</a>)</span>. Thus, the resources spent do not include the costs of producing training data, for which lack of appropriate compensation has given rise to a number of lawsuits (e.g.&nbsp;Authors Guild vs.&nbsp;OpenAI<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a>). In other words, the more valuable thing is the data and not the model, and the few organizations that have the resources to train the models may not have been able to afford training if the data weren’t acquired at a near-0 cost.</p>
<p>Thus, the field of AI must wrestle with opposing issues: we want to train models with data that has high quality - e.g.&nbsp;we want the data to be representative of conditions where the model will be deployed, and thus relevant distributions in the training data must reflect the environment in which the models will be deployed <span class="citation" data-cites="hullman2022worst">(<a href="#ref-hullman2022worst" role="doc-biblioref">Hullman et al., 2022</a>)</span> - but the data requirements to train them appear thus far to be ever-increasing <span class="citation" data-cites="villalobos2024position">(<a href="#ref-villalobos2024position" role="doc-biblioref">Villalobos et al., 2024</a>)</span>, and have thus far not included the actual cost of production <span class="citation" data-cites="kandpal2025position">(<a href="#ref-kandpal2025position" role="doc-biblioref">Kandpal &amp; Raffel, 2025</a>)</span>. We want the data used in training to be ‘good’ because we want the models to be ‘trustworthy’ - in the case of LLMs, we want to have reason to think they will generate text that includes claims that we think are true, and thus likely prefer text that contains information that is likely to be true. Perhaps our closest approximation to ‘what is likely to be true’ is contained in the perspectives presented across all of academic work and official reports. And yet, even if there were little to no barriers to using all of human academic work to train AI systems, this amount of text pales in quantity to the data requirements. And although thus far some models have been made available for academic use (e.g.&nbsp;Meta’s Llama), it is not clear for how long this will be the case, and when, if ever, academia will have the resources to train its own.</p>
</section>
<section id="evaluation-data" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-data">Evaluation Data</h3>
<p>There is substantial room for improvement in terms of the practices of data labeling as well, however implementations of such practices are also costly. The field of <em>AI metrics</em> distinguishes between a) observable AI system behavior, and b) the targets of measurement. In other words, a given score on a given ‘AI’ benchmark designed to measure ‘intelligence’ is not itself a direct measure of ‘intelligence’. The field of AI metrics treats the indirectly observable phenomena as <em>computational constructs</em> - indirectly observable aspects of AI system behavior, a parallel to <em>constructs</em> that are the targets of measurement in other fields - e.g.&nbsp;‘energy’, an unobservable property of all matter and systems from the field of physics, ‘chemical bonds’, the unobservable property that holds molecules together from the field of chemistry, and ‘fitness’, an organisms ability to survive, reproduce and thrive in a given environment <span class="citation" data-cites="gignac2024defining">(<a href="#ref-gignac2024defining" role="doc-biblioref">Gignac &amp; Szodorai, 2024</a>)</span>. Thus the benchmark itself is a measurement instrument, aimed at measuring a computational construct, typically the ‘accuracy’ of an AI system for a given task.</p>
<p>The challenge is essentially one of measurement <span class="citation" data-cites="welty2019metrology">(<a href="#ref-welty2019metrology" role="doc-biblioref">Welty et al., 2019</a>)</span> specifically of human responses, as datasets for evaluation very often contain human input <span class="citation" data-cites="geiger2020garbage geiger2021garbage daneshjou2021lack sav2023annotation">(<a href="#ref-daneshjou2021lack" role="doc-biblioref">Daneshjou et al., 2021</a>; <a href="#ref-geiger2020garbage" role="doc-biblioref">Geiger et al., 2020</a>, <a href="#ref-geiger2021garbage" role="doc-biblioref">2021</a>; <a href="#ref-sav2023annotation" role="doc-biblioref">Sav et al., 2023</a>)</span>. When collecting annotations, labels, or other forms of input from people in order to construct evaluation datasets, we are attempting to collect measurements of latent, unobservable <em>constructs</em> <span class="citation" data-cites="jacobs2021measurement gignac2024defining">(<a href="#ref-gignac2024defining" role="doc-biblioref">Gignac &amp; Szodorai, 2024</a>; <a href="#ref-jacobs2021measurement" role="doc-biblioref">Jacobs &amp; Wallach, 2021</a>)</span>. In other words, when we examine data created from the observations of multiple people in aggregate, we do not directly observe e.g.&nbsp;the presence or absence of an object in a digital image; rather we observe the probability that a person from a given population will indicate the presence of absence of the object in the image <span class="citation" data-cites="welty2019metrology">(<a href="#ref-welty2019metrology" role="doc-biblioref">Welty et al., 2019</a>)</span>. In the parlance of psychology, one cannot directly observe an other’s ‘intelligence’ as one might observe an other’s height <span class="citation" data-cites="gignac2024defining">(<a href="#ref-gignac2024defining" role="doc-biblioref">Gignac &amp; Szodorai, 2024</a>)</span>. And even though height is observable, our measurements of it are still imperfect: in using a measurement device like a ruler multiple times, should we measure precisely enough, we would observe variance in each measurement with the true score for height imperfectly represented by our collection of imperfect measurements <span class="citation" data-cites="welty2019metrology">(<a href="#ref-welty2019metrology" role="doc-biblioref">Welty et al., 2019</a>)</span>.</p>
<p>The process that gives rise to the labels in the data, e.g.&nbsp;the annotation interface, task instructions, and any training given to annotators, is also a measurement instrument - in this case, one that produces the annotations from the labor of annotators <span class="citation" data-cites="beck2022improving">(<a href="#ref-beck2022improving" role="doc-biblioref">Beck et al., 2022</a>)</span>. The case study in this thesis contributes to the field of AI metrics by proposing enriching design, collection, analysis, and reporting of training/evaluation data for AI systems, using knowledge from the social sciences <span class="citation" data-cites="beck2022improving">Jacobs &amp; Wallach (<a href="#ref-jacobs2021measurement" role="doc-biblioref">2021</a>)</span>, metrology (measurement science) <span class="citation" data-cites="welty2019metrology">(<a href="#ref-welty2019metrology" role="doc-biblioref">Welty et al., 2019</a>)</span>, and work in the computational sciences on improving annotations <span class="citation" data-cites="cabitza2023toward">(<a href="#ref-cabitza2023toward" role="doc-biblioref">Cabitza et al., 2023</a>)</span>. It requires a higher cost in terms of scientific labor, as it includes an <em>a-priori</em> empirical investigation of the data collection process in addition to the data collection process itself. In other words, it requires research on <em>how</em> to collect the data in order to produce sufficiently accurate measurements. Applying the knowledge to other tasks would require similar research, in principle for every combination of <em>construct</em> (i.e.&nbsp;the latent phenomenon of interest being measured), <em>content</em> (e.g. text, video, audio etc. and in some cases also subgroups, e.g.&nbsp;tweets vs.&nbsp;podcast transcripts vs.&nbsp;formal speeches etc.), and for relevant <em>characteristics</em> of annotators (i.e.&nbsp;ethnicity, political affiliation, etc.).</p>
<p>Any standardized procedure for comparing two or more individuals is treated like a measurement instrument in the social sciences <span class="citation" data-cites="urbina2014essentials">(<a href="#ref-urbina2014essentials" role="doc-biblioref">Urbina, 2014</a>)</span>. The repeatable procedures that we use to gather annotations from humans are similarly measurement instruments <span class="citation" data-cites="beck2022improving">(<a href="#ref-beck2022improving" role="doc-biblioref">Beck et al., 2022</a>)</span>. In addition, because we use the datasets that we collect to benchmark the performance of models, they are also measurement instruments <span class="citation" data-cites="welty2019metrology">(<a href="#ref-welty2019metrology" role="doc-biblioref">Welty et al., 2019</a>)</span>. Given the complexity of measuring unobservable phenomena, instruments in other fields are subjected to scrutiny prior to being considered usable for their intended purpose, in the form of studies that examine the properties of the measurements produced by the instrument. For example, the process of <em>construct validation</em> involves estimating the extent to which an instrument measures an unobservable construct <span class="citation" data-cites="Wehner2020">(<a href="#ref-Wehner2020" role="doc-biblioref">Wehner et al., 2020</a>)</span>. It assumes an unknowable true score, and that all attempts to measure the true score are imperfect. There is no single solution to demonstrating the validity of a construct, but rather an accumulation of evidence, across multiple studies, with observations made using different methods <span class="citation" data-cites="smith2005construct">(<a href="#ref-smith2005construct" role="doc-biblioref">Smith, 2005</a>)</span>.</p>
<p>Although it is clear there is substantial room for improvement along with the repercussive benefits from such improvements, they will require resources. Despite growing recognition of evaluation data problems <span class="citation" data-cites="hullman2022worst">(<a href="#ref-hullman2022worst" role="doc-biblioref">Hullman et al., 2022</a>)</span>, which in turn have the potential to lead to harms <span class="citation" data-cites="mehrabi2021survey">(<a href="#ref-mehrabi2021survey" role="doc-biblioref">Mehrabi et al., 2021</a>)</span>, actual progress toward better annotation data practices, from collection, to analysis, to reporting, remains slow. Current incentives prioritize immediately observable proxies for accuracy <span class="citation" data-cites="birhane2022values">(<a href="#ref-birhane2022values" role="doc-biblioref">Birhane et al., 2022</a>)</span> rather than careful measurement design <span class="citation" data-cites="beck2022improving">(<a href="#ref-beck2022improving" role="doc-biblioref">Beck et al., 2022</a>)</span> or long-term dataset stewardship <span class="citation" data-cites="liem2023treat">(<a href="#ref-liem2023treat" role="doc-biblioref">Liem &amp; Demetriou, 2023</a>)</span>. Beyond the challenge of gathering data at the necessary scale, are issues about the quality of the data itself, specifically the quality of the overall process that created the data.</p>
<p>Thus, the field of AI must wrestle with the opposing pressure of accuracy vs.&nbsp;the cost of developing measurement instruments for practically infinite use-cases: we want measurements that are ‘accurate’ - i.e.&nbsp;we want the data to be as clear a representation of our phenomenon of interest as it could be - but developing an accurate instrument is a costly exercise to add to the cost of collecting the data itself. Further, the topics we wish to measure in the ever-increasingly-complex AI systems as themselves measurement challenges. Topics like ‘fairness’ <span class="citation" data-cites="jacobs2021measurement">(<a href="#ref-jacobs2021measurement" role="doc-biblioref">Jacobs &amp; Wallach, 2021</a>)</span>, and ‘intelligence’ <span class="citation" data-cites="gignac2024defining">(<a href="#ref-gignac2024defining" role="doc-biblioref">Gignac &amp; Szodorai, 2024</a>)</span> are not only challenging to develop instruments for, but also to define.</p>
</section>
</section>
<section id="labor-challenges" class="level2">
<h2 class="anchored" data-anchor-id="labor-challenges">Labor Challenges</h2>
<section id="scientific-labor" class="level3">
<h3 class="anchored" data-anchor-id="scientific-labor">Scientific Labor</h3>
<p>Addressing the aforementioned data challenges will require substantial additional efforts. The human labor responsible for the collection, curation, and eventual annotation of training data in addition to the training of models is estimated at 29%-49% of the overall cost (estimates include researchers, engineers, and managers, but not data center employees and operations staff), which already amounts to tens of millions for frontier models <span class="citation" data-cites="cottier2024rising">(<a href="#ref-cottier2024rising" role="doc-biblioref">Cottier et al., 2024</a>)</span>. Thus, a key challenge to implementing better practices involves providing labor in terms of the scientific work necessary to design data collection and annotation processes. However our current knowledge gathering apparatus - science as it is now practiced and reported - is poorly positioned to tackle this. Thus, a substantial amount of the labor will likely have to come from an already overburdened work force. A re-consideration of what duties comprise academic work is thus warranted, specifically those that contribute to the body of human knowledge, and whether they can be modernized.</p>
<p>One key example involves academic writing and publication, specifically a primary source of scientific labor in academic settings, the PhD student, and their requirements to produce a dissertation or thesis document in order to progress in their career. PhD students are responsible for as much as a third of all academic output <span class="citation" data-cites="lariviere2012shoulders">(<a href="#ref-lariviere2012shoulders" role="doc-biblioref">Larivière, 2012</a>)</span>, likely by working long hours, including weekend hours, constantly under time-pressure to produce output <span class="citation" data-cites="van2024caught">(<a href="#ref-van2024caught" role="doc-biblioref">Tienoven et al., 2024</a>)</span>. Unsurprisingly, PhD students are a vulnerable population, showing a high prevalence of depression and anxiety - as high as 24%, and 17% respectively in a recent meta-analysis <span class="citation" data-cites="satinsky2021systematic">(<a href="#ref-satinsky2021systematic" role="doc-biblioref">Satinsky et al., 2021</a>)</span>. Like all early career researchers (ECRs, i.e. PhD students, Post Doctoral Researchers, and Assistant Professors), PhD students, contribute substantially to the body of human knowledge via academic publication <span class="citation" data-cites="rorstad2015publication">(<a href="#ref-rorstad2015publication" role="doc-biblioref">Rørstad &amp; Aksnes, 2015</a>)</span>.</p>
<p>However, PhD candidates are bogged down by dated requirements to write and defend theses despite the declining relevance of the document, and the contribution it makes to poor outcomes for the student. For example, the trend of thesis citations over time shows decline <span class="citation" data-cites="lariviere2008declining">(<a href="#ref-lariviere2008declining" role="doc-biblioref">Larivière et al., 2008</a>)</span>. In the Netherlands where this thesis will be defended, evidence has shown that PhD candidates with fixed duration contracts are exceeding that duration by several months, resorting to completing their thesis on their own time and risking failing at completion <span class="citation" data-cites="van2013took">(<a href="#ref-van2013took" role="doc-biblioref">Van de Schoot et al., 2013</a>)</span>. Though a key ceremonial moment, a thesis defense is substantially less rigorous than a peer review for a publication: it is curated, as the ‘peer reviewers’ are chosen by the supervisors of the student, where in the ‘real world’ of academia, peer reviewers are selected from far broader networks <span class="citation" data-cites="lariviere2012shoulders">(<a href="#ref-lariviere2012shoulders" role="doc-biblioref">Larivière, 2012</a>)</span>. It is thus not surprising that publications, and not theses, remain the key factor in the assessment of the value of Academics as scientists <span class="citation" data-cites="anderson2022effect">(<a href="#ref-anderson2022effect" role="doc-biblioref">Anderson et al., 2022</a>)</span>. Although more and more thesis content is being comprised of academic publications anyway (i.e. <em>thesis by publication</em>, <span class="citation" data-cites="jackson2013completing">Jackson (<a href="#ref-jackson2013completing" role="doc-biblioref">2013</a>)</span>), the substantial labor required to assemble works into a single document, write additional (introductory / conclusiory) chapters that are themselves complete manuscripts or nearly so, organize its printing into paperback books, organize a formal event for the defense etc. takes away from more meaningful labor that this workforce could provide.</p>
<p>A second key example involves a cornerstone of trust in science: the editorial peer-review process dates back several hundred years, and exists as a means to encourage and maintain the quality of scholarly work <span class="citation" data-cites="kelly2014peer">(<a href="#ref-kelly2014peer" role="doc-biblioref">Kelly et al., 2014</a>)</span>. Early career and experienced researchers alike contribute to peer-review<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a>, a massive amount of collective labor conservatively valued at over 1.5B USD in 2020 <span class="citation" data-cites="aczel2021billion">(<a href="#ref-aczel2021billion" role="doc-biblioref">Aczel et al., 2021</a>)</span>. ECRs perform a substantial amount of the work often without credit<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a>, and in some cases perform the work completely on their own <span class="citation" data-cites="mcdowell2019co">(<a href="#ref-mcdowell2019co" role="doc-biblioref">McDowell et al., 2019</a>)</span>, especially in high-volume conferences like NeurIPS <span class="citation" data-cites="shah2018design">(<a href="#ref-shah2018design" role="doc-biblioref">Shah et al., 2018</a>)</span>. This is a crucial note, as the submissions to high-volume conferences show massive increases year over year<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a> and struggle to meet the review requirements: e.g.&nbsp;NeurIPS 2021 required outreach to recruit over 1k additional peer-reviewers beyond the over 7k initial volunteers, to produce the 31k reviews needed for the conference<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a>. As conferences continue to grow, the amount of review work is being divided among a regularly stable labor pool, reducing the attention paid to each individual work, a problem exacerbated by the repeat submission of borderline papers - i.e.&nbsp;papers that were nearly accepted <span class="citation" data-cites="Zhang_2022">(<a href="#ref-Zhang_2022" role="doc-biblioref">Zhang et al., 2022</a>)</span>. This results in important flaws being missed: e.g. <span class="citation" data-cites="kapoor2023leakage">(<a href="#ref-kapoor2023leakage" role="doc-biblioref">Kapoor &amp; Narayanan, 2023</a>)</span> show a growing crisis across 17 Machine Learning fields from a lack of trustworthy results due to data leakage.</p>
<p>A third key example involves the ever-increasing volume of published manuscripts on AI and related topics<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a>, which makes it impossible to stay abreast of the overall field. 10% of over 4 million publications indexed on the SCOPUS academic database in 2024 had terms related to AI in their title, keywords or abstracts (see Appendix A), up from around 7% in 2022. The number of submitted manuscripts also increases year over year, with popular conferences like NeurIPS receiving upwards of 12K submissions in 2023<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a>. These figures do not include preprints posted on servers like arXiv, which show over 42K works with AI related terms in the abstract for 2024, more than doubling the about 17.5K posts in 2019. This makes it more and more difficult to monitor the overall quality of the field <span class="citation" data-cites="Zhang_2022">(<a href="#ref-Zhang_2022" role="doc-biblioref">Zhang et al., 2022</a>)</span>.</p>
<p>These examples highlight the overburden of academic scientific labor, which in turn raises questions about whether research in fields pertaining to AI can sustainably retain an appropriate level of quality, while meeting increasing demands. Predictably, and perhaps understandably, academics appear to be turning to Large Language Models (LLMs) for assistance, as evidence of its use is showing in academic work in both peer reviews <span class="citation" data-cites="liang2024monitoring">(<a href="#ref-liang2024monitoring" role="doc-biblioref">Liang et al., 2024</a>)</span>, and in manuscripts <span class="citation" data-cites="gray2024chatgpt">(<a href="#ref-gray2024chatgpt" role="doc-biblioref">Gray, 2024</a>)</span>. This is problematic as LLMs often report information that is not factual <span class="citation" data-cites="wang-etal-2024-factuality">(<a href="#ref-wang-etal-2024-factuality" role="doc-biblioref">Wang et al., 2024</a>)</span>. However, a more appropriate direction might be to question how cornerstone components of academic work are conducted, and whether they can meet the needs of the field in the future.</p>
</section>
<section id="annotation-labor" class="level3">
<h3 class="anchored" data-anchor-id="annotation-labor">Annotation Labor</h3>
<p>The labor required to gather annotations is a key component of the evaluation data for machine learning or AI systems, and a substantial contributor to the related costs. In specialist categories, e.g.&nbsp;as with Figure Eight which hires annotators for projects related to defense, salaries can range from 41K - 67K USD per year<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a>. Although commonly used platforms like mTurk<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a> list only minimum rates of .01 USD per task, other services, e.g.&nbsp;Prolific<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a>, hire on a per-task basis and pay at least minimum wage in the UK. Although these costs may seem manageable, they have the potential to balloon at scale. For example, Open AI hired a San Francisco-based firm that sourced annotation labor from Kenya, Uganda, and India to provide the human inputs necessary to fine tune their models <a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a>. Despite the pay rate of 2 USD per hour being far less than the 7.25 USD federal minimum wage in the US<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a>, OpenAI spent 600K USD in 2021 to label text as being violent, sexual, or hate speech alone. While the availability of such services has allowed for rapid gathering of evaluation data for AI-related projects, works that highlight data scarcity warn that it may be insufficient to meet the needs of coming models, and that the costs of such labor will be very high.</p>
<p>Work in the case study of this thesis further suggests that <em>more</em> annotation work may be required per project than is current, as it requires a focus on the development of an annotation instrument prior to primary data collection phases, pre-studies to estimate the required number of annotators, and accounting for possible variance in perspectives. For high-stakes use-cases, such as applications in government or healthcare, we may wish to have data independently examined prior to its certification for use, similarly to how some scholars recommend external examination of AI models prior to deployment <span class="citation" data-cites="garrett2024testing">(<a href="#ref-garrett2024testing" role="doc-biblioref">Garrett &amp; Rudin, 2024</a>)</span>, which may in turn require further iterations of data collection.</p>
</section>
</section>
<section id="a-contemporary-field" class="level2">
<h2 class="anchored" data-anchor-id="a-contemporary-field">A contemporary field</h2>
<p>Research on AI and related fields will require contemporary solutions to address problems related to training and evaluation data, and scientific and annotation labor. Although no clear solutions have been derived, and more focus is clearly needed to source ideas and vet them for use, some promising directions have been observed in recent years.</p>
<section id="collaborative-platform-data-collection" class="level3">
<h3 class="anchored" data-anchor-id="collaborative-platform-data-collection">Collaborative-platform data collection</h3>
<p>One approach to both gathering the necessary training and evaluation data at scale have been shown in collaborations between scientists and the general public. Online platforms, often initiated by academics, host and facilitate the creation of various resources, ranging from media and other forms of data, annotation projects as well as forums for discussion. Of note, it has been shown that the general public is even willing to share data as sensitive as medical data as long as it de-identified, and they are given sufficient transparency and agency over its use and/or removal <span class="citation" data-cites="liang2022advances">(<a href="#ref-liang2022advances" role="doc-biblioref">Liang et al., 2022</a>)</span>. For example iNaturalist.org is an online community with over 8 million users who make contributions in the form of images taken on their smartphones, and/or labels of the species in the images <span class="citation" data-cites="van2018inaturalist">(<a href="#ref-van2018inaturalist" role="doc-biblioref">Van Horn et al., 2018</a>)</span>. Zooniverse.org is an online community of over 2.8M users that hosts projects defined by scientists to gather labels from non-scientists <span class="citation" data-cites="fortson2012galaxy">(<a href="#ref-fortson2012galaxy" role="doc-biblioref">Fortson et al., 2012</a>)</span>. A third example is Commonvoice, which is a large dataset of speech transcription in 76 languages, provided by approximately 150k participants <span class="citation" data-cites="ardila-etal-2020-common">(<a href="#ref-ardila-etal-2020-common" role="doc-biblioref">Ardila et al., 2020</a>)</span>. Immediate challenges related to privacy must be solved, however this approach does simultaneously allow for the contribution of diverse media for annotation, and labor needed to annotate them.</p>
<p>A means to further encourage collaborative data collection, that also allows for peer-review feedback prior to the collection, and that sits more closely within academia is the <em>Registered Report</em> <span class="citation" data-cites="chambers2013registered">(<a href="#ref-chambers2013registered" role="doc-biblioref">Chambers, 2013</a>)</span>: an academic publication format where researchers submit a manuscript prior to data collection, which includes all information relevant to how the study will be conducted, including motivation of the work (i.e.&nbsp;introduction), details of data collection processes, as well as planned analyses. Typical review stages apply, i.e.&nbsp;suggestions for revisions or rejections, or an <em>in-principle acceptance</em>, whereby reviewers and editor agree to a publication should the methods used in the manuscript either follow closely the in-principle accepted version, or appropriate justifications be made for any changes made. This format was designed to allow for editorial decisions being made based on the strength of the methods, without the influence of the results, and to also strengthened the methods via peer-review prior to data collection.</p>
<p>The Registered Report format may prove useful as a means for collaborative collection of data intended for AI training and/or evaluation. Firstly, it allows for peer-review prior to collection, whereby a panel of experts will provide critiques that will either strengthen the eventual design, or reject it in favor of publishing other stronger designs. Given the scope and resources needed to collect AI datasets, this format may thus allow for a critique of the design prior to investment of resource cost of collection. Secondly, it can be adjusted for the gathering of data for AI, by allowing for the submissions of responses in the form of data that conforms to the design in the published manuscript in a decentralized fashion, from multiple stakeholders, thus reducing the bias from any single data collection point and allowing for the sharing of financial and other resource burdens. Thirdly, as online platforms like Zooniverse grow and proliferate to focus on different topics and domains, hosted projects may be attached to peer-reviewed data collection protocols, sharing existing, working infrastructure. And lastly, platforms allow for further insight from input via the community of users. This could facilitate discovery, as was the case when a platform user discovered a new type of astronomical object - now named ‘Hanny’s Voorwerp’<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a> after her - while classifying galaxies on a platform called Galaxy Zoo<a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a>.</p>
<p>A third approach integrates instruction with data collection by making use of the labor bachelor and masters students while also providing hands-on research experience for the purposes of instruction. One example is the Collaborative Replications and Education Project (CREP), an initiative where undergraduate students, replicate high-impact psychology studies under faculty supervision. The aim is to allow for direct instruction of students while providing needed labor in the form of replications of pivotal studies<a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a>. One example output is a meta-analysis of nine student-led replications which showed a lack of evidence of the “red-effect” <span class="citation" data-cites="wagge2019demonstration">(<a href="#ref-wagge2019demonstration" role="doc-biblioref">Wagge et al., 2019</a>)</span>, which functioned both to instruct students and contribute to the scientific record. Similar approaches can be taken towards annotating data with students replicating registered data collection protocols, or supplying the annotations themselves. This may result in an increase in quality of annotations over crowd-sourcing platforms like mTurk, in addition to hands-on instruction for students and an additional pool of labor.</p>
</section>
<section id="publication-and-maintenance-of-academic-artifacts" class="level3">
<h3 class="anchored" data-anchor-id="publication-and-maintenance-of-academic-artifacts">Publication and Maintenance of academic artifacts</h3>
<p>Given the large volume of papers published on AI and related fields, a number of publication formats currently exist that can assist in thoroughly summarizing and synthesizing evidence for topics in which there is high publication volume. <em>Systematic Reviews</em><a href="#fn28" class="footnote-ref" id="fnref28" role="doc-noteref"><sup>28</sup></a> and <em>Meta-Analyses</em><a href="#fn29" class="footnote-ref" id="fnref29" role="doc-noteref"><sup>29</sup></a> attempt to gather all relevant works on a topic, synthesize the evidence, extract insights and highlight research opportunities on a given topic. In fields where there is an exceptional amount of work on a topic, or set of related topics, <em>Umbrella Reviews</em><a href="#fn30" class="footnote-ref" id="fnref30" role="doc-noteref"><sup>30</sup></a> attempt to synthesize collections of systematic reviews and/or meta-analyses. While useful, the accuracy and currency of these formats atrophies in fields that are very rapidly developing. <em>Living Systematic Reviews</em> are useful for keeping up-to-date information visible, particularly in rapidly evolving fields <span class="citation" data-cites="elliott2017living">(<a href="#ref-elliott2017living" role="doc-biblioref">Elliott et al., 2017</a>)</span>. Living Systematic Reviews use the same methods as any form of academic review (e.g.&nbsp;Systematic Reviews, Meta Analyses, etc.) for selecting, reviewing, and synthesizing available evidence, but include an <em>a-priori</em> commitment to gather and update what is reported, often for a given length of time. Although searching and selecting items for inclusion can be time consuming, they can be assisted with automation <span class="citation" data-cites="thomas2017living schmidt2025data">(<a href="#ref-schmidt2025data" role="doc-biblioref">Schmidt et al., 2025</a>; <a href="#ref-thomas2017living" role="doc-biblioref">Thomas et al., 2017</a>)</span>, for which many open-source, free tools are available (e.g.&nbsp;asreview<a href="#fn31" class="footnote-ref" id="fnref31" role="doc-noteref"><sup>31</sup></a>). Thus, machine learning tools can contribute to the rapid synthesis of quickly evolving scientific evidence, as is the case with rapidly developing fields, like Large Language Models.</p>
<p>The practice of AI and related science produces a number of artifacts beyond the manuscript that reports results of the work, including data, models, code / notebooks, configurations and lab notes. These artifacts are a relevant part of the work of science, although they are often obscured from view, and like software artifacts, would benefit from regular maintenance <span class="citation" data-cites="liem2023treat bartlett2025raiders">(<a href="#ref-bartlett2025raiders" role="doc-biblioref">Bartlett et al., 2025</a>; <a href="#ref-liem2023treat" role="doc-biblioref">Liem &amp; Demetriou, 2023</a>)</span>. With AI-related artifacts specifically, sites like Replicate<a href="#fn32" class="footnote-ref" id="fnref32" role="doc-noteref"><sup>32</sup></a> and Huggingface<a href="#fn33" class="footnote-ref" id="fnref33" role="doc-noteref"><sup>33</sup></a> have emerged that allow for hosting, documentation, sharing and discussion. More widely used in other fields, sites like the Open Science Framework<a href="#fn34" class="footnote-ref" id="fnref34" role="doc-noteref"><sup>34</sup></a> have emerged that allow for the flexible documentation of various components of any scientific workflow as well as updates. Similarly, online git-compatible repositories like GitHub can be leveraged not only as containers for the artifacts, but as easily maintainable documentation of the evolution of various projects <span class="citation" data-cites="wattanakriengkrai2022github">(<a href="#ref-wattanakriengkrai2022github" role="doc-biblioref">Wattanakriengkrai et al., 2022</a>)</span>. Taken together, these technologies allow for creation, versioning, and maintenance of artifacts of a research project similar to software artifacts - <em>Living Scientific Artifacts</em>.</p>
<p>Artifact-inclusive, and artifact-specific publication is also possible, though not yet widely established. Furthermore research output in the form of manuscripts, and artifacts resulting from research projects can directly be combined for publication. Platforms like Quarto<a href="#fn35" class="footnote-ref" id="fnref35" role="doc-noteref"><sup>35</sup></a> and Jupyter<a href="#fn36" class="footnote-ref" id="fnref36" role="doc-noteref"><sup>36</sup></a> allow for the creation of reproducible documents, which combine text and text formatting, with the output of code snippets, media, and visualizations. Thus the papers that we read could be rendered from documents that also contain all the code and artifacts from which results are derived. These documents can then directly be published, along with the underlying artifacts on platforms like f1000<a href="#fn37" class="footnote-ref" id="fnref37" role="doc-noteref"><sup>37</sup></a> and ResearchEquals<a href="#fn38" class="footnote-ref" id="fnref38" role="doc-noteref"><sup>38</sup></a>. Further, platforms like ResearchHub<a href="#fn39" class="footnote-ref" id="fnref39" role="doc-noteref"><sup>39</sup></a> connect publication directly to a github-like repository that hosts the artifacts. At Delft University of technology, in a proof-of-concept built and designed by students that we called Alexandria<a href="#fn40" class="footnote-ref" id="fnref40" role="doc-noteref"><sup>40</sup></a>, we extend the idea of the artifact-inclusive/artifact-specific publication to include <em>micro-publication</em> - the publication of scientific contributions smaller than complete manuscripts - in <em>collaborative articles</em> - articles written and edited by communities, similar to wikipedia, but differing in that they are also peer-reviewed, and containing scientific artifacts in repositories like github.</p>
<p>While it is unclear what direction all of these ideas will take, and whether any may become a viable alternative to the traditional journal or conference publication, the general themes of living, collaborative manuscripts and scientific artifacts allow for a number of potential benefits to the AI and related fields: a) they suggest a more efficient pipeline from research project to publication, as the materials used during the research phase directly become the submitted manuscript and accompanying artifacts, thereby shrinking the distance between research project and report, b) considering how many artifacts play a role in AI and related field science, the publication and maintenance of artifacts in addition to the manuscript allows for more of the contributions of scientists be visible, in addition to current, and thus immediately re-usable, and c) they provide the infrastructure within peer-reviewed academia that allows for coordination of collaborative scientific and annotation labor, especially as regards the collection of training and evaluation data.</p>
</section>
<section id="ai-metrics-as-a-field" class="level3">
<h3 class="anchored" data-anchor-id="ai-metrics-as-a-field">AI Metrics as a field</h3>
<p>More recognition of AI Metrics as a formalized research field <span class="citation" data-cites="gignac2024defining">(<a href="#ref-gignac2024defining" role="doc-biblioref">Gignac &amp; Szodorai, 2024</a>)</span> would allow for the systematic study of how and what we measure from AI system outputs, with a focus on designing, validating, and maintaining evaluation instruments. As other disciplines (psychology, economics, software engineering) have entire fields dedicated to the design, collection, analysis, and reporting of data that involves human behavior (psychometrics, econometrics, software testing), so too should AI and related fields. As it stands, there are little to no dedicated venues for the publications (i.e.&nbsp;journals, conferences, but possibly also dedicated, regular tracks at conferences) for this field of study, despite its central importance, and the ballooning number of benchmarks and claims of AI system capabilities.</p>
<p>The focus of AI metrics would be to establish processes for defining measurement targets, and developing and validating the instruments used to assess these targets from AI system behaviors. Such a dedicated field could develop epistemological and methodological frameworks for how to define constructs, determine the requirements of processes designed to measure them, methods to estimate the resources needed to meet requirements (e.g.&nbsp;the number of annotators, or items to in the training set), estimate the reliability and construct validity of measurement methods, and identify potential sources of measurement error. This includes strategies for sampling training data, annotation interfaces, schemas and protocols, the establishment of benchmarks, and any other potential methods for measuring constructs like fairness, or ‘intelligence’ in AI model outputs. Many benchmarks were developed years ago under very different assumptions and are re-used, some with incredible frequency e.g.&nbsp;Imagenet, without re-examination. AI Metrics could investigate not just initial design of datasets and benchmarks, but also their current usefulness as models and deployment environments evolve. Futher, in cases where the development of the benchmark was costly, AI metrics can investigated what adjustments to make so that they can be made useful - e.g.&nbsp;via re-sampling, re-or annotation.</p>
<p>Without a dedicated field of study, many current practices remain ad hoc, with limited scrutiny despite their far-reaching influence. Solutions to problems in both training and evaluation data collection require deliberate study for standards to improve and disseminate. In other words, beyond the implementation of currently thought best practices, is the requirement that the best practices be improved along with demands, and knowledge of them disseminated such that general practice is affected. As part of this, the field must include investigations of the necessary infrastructure to support and disseminate best practices, and best make use of available labor.</p>
</section>
</section>
<section id="a-call-to-action" class="level2">
<h2 class="anchored" data-anchor-id="a-call-to-action">A Call to Action</h2>
<p>Better data design enables better science, more responsible innovation, and safer, more trustworthy AI systems. Substantial imperfections have been observed in the training and evaluation datasets often used to build AI systems. Despite this, the bulk of AI research incentives remain concentrated on models and algorithms, while the design, creation, and maintenance of training and evaluation data are treated as secondary, ad-hoc tasks. This must change. Researchers, academic departments, conference venues, and funding bodies must ostensibly recognize and credit the data work. And the data work must be expanded to include research on design prior to collection, definition of the complex attributes we wish the systems to have, and the validation of instruments to measure the complex phenomena in question. Crucially, we must rethink how scientific and annotation labor is structured if we are to meet the demands of trustworthy AI systems in the future. Dated practices must be modernized. New publication formats, as well as publication infrastructure must be developed, aimed at synthesis of the massive and increasing volume of academic publications, distribution of the burden of training and evaluation data design and creation, and collaboration with relevant stakeholders including the general public. This will require far greater thought and investment, even in comparison to the current amount of effort and funding dedicated to AI (and related fields) research. With the a number of promising directions emerging, academia must take an active role in exploring and vetting them for use if it is to be relevant to AI and related fields in the future.</p>
</section>
<section id="sec-appendix" class="level1">
<h1>Appendix</h1>
<section id="appendix-a-citation-trends-plot" class="level2">
<h2 class="anchored" data-anchor-id="appendix-a-citation-trends-plot">Appendix A: Citation Trends Plot</h2>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/publication_plots-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="appendix-b-search-terms" class="level2">
<h2 class="anchored" data-anchor-id="appendix-b-search-terms">Appendix B: Search terms</h2>
<section id="scopus" class="level3">
<h3 class="anchored" data-anchor-id="scopus">SCOPUS:</h3>
<p>for AI related topics: TITLE-ABS-KEY ( ( ( ( machine OR deep OR reinforcement OR supervised OR unsupervised ) AND learning ) OR ( “neural networks” ) OR ( ai OR “artificial intelligence” ) ) ) AND PUBYEAR &gt; 1999 AND PUBYEAR &lt; 2027</p>
<p>for overall publication records: PUBYEAR &gt; 1999 AND PUBYEAR &lt; 2027</p>
</section>
<section id="arxiv" class="level3">
<h3 class="anchored" data-anchor-id="arxiv">arXiv:</h3>
<p>[Abstract] AI or “artificial intelligence” OR machine AND learning OR supervised AND learning OR reinforcement AND learning OR neural AND networks</p>
<p>17,459 results in 2019 23,923 results in 2020 27,610 results in 2021 29,690 results in 2022 33,419 results in 2023 42,183 in 2024</p>
</section>
</section>
<section id="appendix-c-token-estimates" class="level2">
<h2 class="anchored" data-anchor-id="appendix-c-token-estimates">Appendix C: Token estimates</h2>
<p>Taking a study on the word length requirements of education journals as a proxy, <span class="citation" data-cites="fairbairn2009profile">Fairbairn et al. (<a href="#ref-fairbairn2009profile" role="doc-biblioref">2009</a>)</span> report the following figures:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/token_counts-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>



</section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-aczel2021billion" class="csl-entry" role="listitem">
Aczel, B., Szaszi, B., &amp; Holcombe, A. O. (2021). A billion-dollar donation: Estimating the cost of researchers’ time spent on peer review. <em>Research Integrity and Peer Review</em>, <em>6</em>(1), 1–8.
</div>
<div id="ref-altmeyer2024position" class="csl-entry" role="listitem">
Altmeyer, P., Demetriou, A. M., Bartlett, A., &amp; Liem, C. (2024). Position: Stop making unscientific AGI performance claims. <em>arXiv Preprint arXiv:2402.03962</em>.
</div>
<div id="ref-alzubaidi2023survey" class="csl-entry" role="listitem">
Alzubaidi, L., Bai, J., Al-Sabaawi, A., Santamarı́a, J., Albahri, A. S., Al-Dabbagh, B. S. N., Fadhel, M. A., Manoufali, M., Zhang, J., Al-Timemy, A. H., et al. (2023). A survey on deep learning tools dealing with data scarcity: Definitions, challenges, solutions, tips, and applications. <em>Journal of Big Data</em>, <em>10</em>(1), 46.
</div>
<div id="ref-anderson2022effect" class="csl-entry" role="listitem">
Anderson, C. G., McQuaid, R. W., &amp; Wood, A. M. (2022). The effect of journal metrics on academic resume assessment. <em>Studies in Higher Education</em>, <em>47</em>(11), 2310–2322.
</div>
<div id="ref-ardila-etal-2020-common" class="csl-entry" role="listitem">
Ardila, R., Branson, M., Davis, K., Kohler, M., Meyer, J., Henretty, M., Morais, R., Saunders, L., Tyers, F., &amp; Weber, G. (2020). Common voice: A massively-multilingual speech corpus. In N. Calzolari, F. Béchet, P. Blache, K. Choukri, C. Cieri, T. Declerck, S. Goggi, H. Isahara, B. Maegaard, J. Mariani, H. Mazo, A. Moreno, J. Odijk, &amp; S. Piperidis (Eds.), <em>Proceedings of the twelfth language resources and evaluation conference</em> (pp. 4218–4222). European Language Resources Association. <a href="https://aclanthology.org/2020.lrec-1.520/">https://aclanthology.org/2020.lrec-1.520/</a>
</div>
<div id="ref-bansal2022systematic" class="csl-entry" role="listitem">
Bansal, M. A., Sharma, D. R., &amp; Kathuria, D. M. (2022). A systematic review on data scarcity problem in deep learning: Solution and applications. <em>ACM Computing Surveys (Csur)</em>, <em>54</em>(10s), 1–29.
</div>
<div id="ref-bartlett2025raiders" class="csl-entry" role="listitem">
Bartlett, A., Liem, C., &amp; Panichella, A. (2025). Raiders of the lost dependency: Fixing dependency conflicts in python using LLMs. <em>arXiv Preprint arXiv:2501.16191</em>.
</div>
<div id="ref-beck2022improving" class="csl-entry" role="listitem">
Beck, J., Eckman, S., Chew, R., &amp; Kreuter, F. (2022). Improving labeling through social science insights: Results and research agenda. <em>International Conference on Human-Computer Interaction</em>, 245–261.
</div>
<div id="ref-birhane2022values" class="csl-entry" role="listitem">
Birhane, A., Kalluri, P., Card, D., Agnew, W., Dotan, R., &amp; Bao, M. (2022). The values encoded in machine learning research. <em>Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency</em>, 173–184.
</div>
<div id="ref-cabitza2023toward" class="csl-entry" role="listitem">
Cabitza, F., Campagner, A., &amp; Basile, V. (2023). Toward a perspectivist turn in ground truthing for predictive computing. <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, <em>37</em>, 6860–6868.
</div>
<div id="ref-chambers2013registered" class="csl-entry" role="listitem">
Chambers, C. D. (2013). Registered reports. <em>A New Publishing Initiative at Cortex. Cortex</em>, <em>49</em>(3), 609–610.
</div>
<div id="ref-cottier2024rising" class="csl-entry" role="listitem">
Cottier, B., Rahman, R., Fattorini, L., Maslej, N., Besiroglu, T., &amp; Owen, D. (2024). The rising costs of training frontier AI models. <em>arXiv Preprint arXiv:2405.21015</em>.
</div>
<div id="ref-daneshjou2021lack" class="csl-entry" role="listitem">
Daneshjou, R., Smith, M. P., Sun, M. D., Rotemberg, V., &amp; Zou, J. (2021). Lack of transparency and potential bias in artificial intelligence data sets and algorithms: A scoping review. <em>JAMA Dermatology</em>, <em>157</em>(11), 1362–1369.
</div>
<div id="ref-elliott2017living" class="csl-entry" role="listitem">
Elliott, J. H., Synnot, A., Turner, T., Simmonds, M., Akl, E. A., McDonald, S., Salanti, G., Meerpohl, J., MacLehose, H., Hilton, J., et al. (2017). Living systematic review: 1. Introduction—the why, what, when, and how. <em>Journal of Clinical Epidemiology</em>, <em>91</em>, 23–30.
</div>
<div id="ref-fairbairn2009profile" class="csl-entry" role="listitem">
Fairbairn, H., Holbrook, A., Bourke, S., Preston, G., Cantwell, R., &amp; Scevak, J. (2009). A profile of education journals. <em>AARE 2008 International Educational Research Conference</em>, 1–20.
</div>
<div id="ref-fortson2012galaxy" class="csl-entry" role="listitem">
Fortson, L., Masters, K., Nichol, R., Edmondson, E., Lintott, C., Raddick, J., &amp; Wallin, J. (2012). Galaxy zoo. <em>Advances in Machine Learning and Data Mining for Astronomy</em>, <em>2012</em>, 213–236.
</div>
<div id="ref-garrett2024testing" class="csl-entry" role="listitem">
Garrett, B. L., &amp; Rudin, C. (2024). Testing AI. <em>Available at SSRN 4948789</em>. <a href="https://papers-ssrn-com.tudelft.idm.oclc.org/sol3/papers.cfm?abstract_id=4948789">https://papers-ssrn-com.tudelft.idm.oclc.org/sol3/papers.cfm?abstract_id=4948789</a>
</div>
<div id="ref-geiger2021garbage" class="csl-entry" role="listitem">
Geiger, R. S., Cope, D., Ip, J., Lotosh, M., Shah, A., Weng, J., &amp; Tang, R. (2021). " garbage in, garbage out" revisited: What do machine learning application papers report about human-labeled training data? <em>arXiv Preprint arXiv:2107.02278</em>.
</div>
<div id="ref-geiger2020garbage" class="csl-entry" role="listitem">
Geiger, R. S., Yu, K., Yang, Y., Dai, M., Qiu, J., Tang, R., &amp; Huang, J. (2020). Garbage in, garbage out? Do machine learning application papers in social computing report where human-labeled training data comes from? <em>Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</em>, 325–336.
</div>
<div id="ref-gignac2024defining" class="csl-entry" role="listitem">
Gignac, G. E., &amp; Szodorai, E. T. (2024). Defining intelligence: Bridging the gap between human and artificial perspectives. <em>Intelligence</em>, <em>104</em>, 101832.
</div>
<div id="ref-gray2024chatgpt" class="csl-entry" role="listitem">
Gray, A. (2024). ChatGPT" contamination": Estimating the prevalence of LLMs in the scholarly literature. <em>arXiv Preprint arXiv:2403.16887</em>.
</div>
<div id="ref-hullman2022worst" class="csl-entry" role="listitem">
Hullman, J., Kapoor, S., Nanayakkara, P., Gelman, A., &amp; Narayanan, A. (2022). The worst of both worlds: A comparative analysis of errors in learning from data in psychology and machine learning. <em>Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society</em>, 335–348.
</div>
<div id="ref-jackson2013completing" class="csl-entry" role="listitem">
Jackson, D. (2013). Completing a PhD by publication: A review of australian policy and implications for practice. <em>Higher Education Research &amp; Development</em>, <em>32</em>(3), 355–368.
</div>
<div id="ref-jacobs2021measurement" class="csl-entry" role="listitem">
Jacobs, A. Z., &amp; Wallach, H. (2021). Measurement and fairness. <em>Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</em>, 375–385.
</div>
<div id="ref-kandpal2025position" class="csl-entry" role="listitem">
Kandpal, N., &amp; Raffel, C. (2025). Position: The most expensive part of an LLM should be its training data. <em>arXiv Preprint arXiv:2504.12427</em>.
</div>
<div id="ref-kapoor2023leakage" class="csl-entry" role="listitem">
Kapoor, S., &amp; Narayanan, A. (2023). Leakage and the reproducibility crisis in machine-learning-based science. <em>Patterns</em>, <em>4</em>(9).
</div>
<div id="ref-kelly2014peer" class="csl-entry" role="listitem">
Kelly, J., Sadeghieh, T., &amp; Adeli, K. (2014). Peer review in scientific publications: Benefits, critiques, &amp; a survival guide. <em>Ejifcc</em>, <em>25</em>(3), 227.
</div>
<div id="ref-lariviere2012shoulders" class="csl-entry" role="listitem">
Larivière, V. (2012). On the shoulders of students? The contribution of PhD students to the advancement of knowledge. <em>Scientometrics</em>, <em>90</em>(2), 463–481.
</div>
<div id="ref-lariviere2008declining" class="csl-entry" role="listitem">
Larivière, V., Zuccala, A., &amp; Archambault, É. (2008). The declining scientific impact of theses: Implications for electronic thesis and dissertation repositories and graduate studies. <em>Scientometrics</em>, <em>74</em>(1), 109–121.
</div>
<div id="ref-liang2024monitoring" class="csl-entry" role="listitem">
Liang, W., Izzo, Z., Zhang, Y., Lepp, H., Cao, H., Zhao, X., Chen, L., Ye, H., Liu, S., Huang, Z., et al. (2024). Monitoring ai-modified content at scale: A case study on the impact of chatgpt on ai conference peer reviews. <em>arXiv Preprint arXiv:2403.07183</em>.
</div>
<div id="ref-liang2022advances" class="csl-entry" role="listitem">
Liang, W., Tadesse, G. A., Ho, D., Fei-Fei, L., Zaharia, M., Zhang, C., &amp; Zou, J. (2022). Advances, challenges and opportunities in creating data for trustworthy AI. <em>Nature Machine Intelligence</em>, <em>4</em>(8), 669–677.
</div>
<div id="ref-liem2023treat" class="csl-entry" role="listitem">
Liem, C. C., &amp; Demetriou, A. M. (2023). Treat societally impactful scientific insights as open-source software artifacts. <em>2023 IEEE/ACM 45th International Conference on Software Engineering: Software Engineering in Society (ICSE-SEIS)</em>, 150–156.
</div>
<div id="ref-mcdowell2019co" class="csl-entry" role="listitem">
McDowell, G. S., Knutsen, J. D., Graham, J. M., Oelker, S. K., &amp; Lijek, R. S. (2019). Co-reviewing and ghostwriting by early-career researchers in the peer review of manuscripts. <em>Elife</em>, <em>8</em>, e48425.
</div>
<div id="ref-mehrabi2021survey" class="csl-entry" role="listitem">
Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., &amp; Galstyan, A. (2021). A survey on bias and fairness in machine learning. <em>ACM Computing Surveys (CSUR)</em>, <em>54</em>(6), 1–35.
</div>
<div id="ref-rorstad2015publication" class="csl-entry" role="listitem">
Rørstad, K., &amp; Aksnes, D. W. (2015). Publication rate expressed by age, gender and academic position–a large-scale analysis of norwegian academic staff. <em>Journal of Informetrics</em>, <em>9</em>(2), 317–333.
</div>
<div id="ref-sambasivan2021everyone" class="csl-entry" role="listitem">
Sambasivan, N., Kapania, S., Highfill, H., Akrong, D., Paritosh, P., &amp; Aroyo, L. M. (2021). <span>“Everyone wants to do the model work, not the data work”</span>: Data cascades in high-stakes AI. <em>Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em>, 1–15.
</div>
<div id="ref-satinsky2021systematic" class="csl-entry" role="listitem">
Satinsky, E. N., Kimura, T., Kiang, M. V., Abebe, R., Cunningham, S., Lee, H., Lin, X., Liu, C. H., Rudan, I., Sen, S., et al. (2021). Systematic review and meta-analysis of depression, anxiety, and suicidal ideation among ph. D. students. <em>Scientific Reports</em>, <em>11</em>(1), 14370.
</div>
<div id="ref-sav2023annotation" class="csl-entry" role="listitem">
Sav, A.-G., Demetriou, A. M., &amp; Liem, C. C. (2023). Annotation practices in societally impactful machine learning applications: What are popular recommender systems models actually trained on? <em>Perspectives@ RecSys</em>.
</div>
<div id="ref-schmidt2025data" class="csl-entry" role="listitem">
Schmidt, L., Mutlu, A. N. F., Elmore, R., Olorisade, B. K., Thomas, J., &amp; Higgins, J. P. (2025). Data extraction methods for systematic review (semi) automation: Update of a living systematic review. <em>F1000Research</em>, <em>10</em>, 401.
</div>
<div id="ref-shah2018design" class="csl-entry" role="listitem">
Shah, N. B., Tabibian, B., Muandet, K., Guyon, I., &amp; Von Luxburg, U. (2018). Design and analysis of the NIPS 2016 review process. <em>Journal of Machine Learning Research</em>, <em>19</em>(49), 1–34.
</div>
<div id="ref-smith2005construct" class="csl-entry" role="listitem">
Smith, G. T. (2005). On construct validity: Issues of method and measurement. <em>Psychological Assessment</em>, <em>17</em>(4), 396.
</div>
<div id="ref-thomas2017living" class="csl-entry" role="listitem">
Thomas, J., Noel-Storr, A., Marshall, I., Wallace, B., McDonald, S., Mavergames, C., Glasziou, P., Shemilt, I., Synnot, A., Turner, T., et al. (2017). Living systematic reviews: 2. Combining human and machine effort. <em>Journal of Clinical Epidemiology</em>, <em>91</em>, 31–37.
</div>
<div id="ref-van2024caught" class="csl-entry" role="listitem">
Tienoven, T. P. van, Glorieux, A., Minnen, J., &amp; Spruyt, B. (2024). Caught between academic calling and academic pressure? Working time characteristics, time pressure and time sovereignty predict PhD students’ research engagement. <em>Higher Education</em>, <em>87</em>(6), 1885–1904.
</div>
<div id="ref-urbina2014essentials" class="csl-entry" role="listitem">
Urbina, S. (2014). <em>Essentials of psychological testing</em>. John Wiley &amp; Sons.
</div>
<div id="ref-van2013took" class="csl-entry" role="listitem">
Van de Schoot, R., Yerkes, M. A., Mouw, J. M., &amp; Sonneveld, H. (2013). What took them so long? Explaining PhD delays among doctoral candidates. <em>PloS One</em>, <em>8</em>(7), e68839.
</div>
<div id="ref-van2018inaturalist" class="csl-entry" role="listitem">
Van Horn, G., Mac Aodha, O., Song, Y., Cui, Y., Sun, C., Shepard, A., Adam, H., Perona, P., &amp; Belongie, S. (2018). The inaturalist species classification and detection dataset. <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, 8769–8778.
</div>
<div id="ref-villalobos2024position" class="csl-entry" role="listitem">
Villalobos, P., Ho, A., Sevilla, J., Besiroglu, T., Heim, L., &amp; Hobbhahn, M. (2024). Position: Will we run out of data? Limits of LLM scaling based on human-generated data. <em>Forty-First International Conference on Machine Learning</em>.
</div>
<div id="ref-wagge2019demonstration" class="csl-entry" role="listitem">
Wagge, J. R., Baciu, C., Banas, K., Nadler, J. T., Schwarz, S., Weisberg, Y., IJzerman, H., Legate, N., &amp; Grahe, J. (2019). A demonstration of the collaborative replication and education project: Replication attempts of the red-romance effect. <em>Collabra: Psychology</em>, <em>5</em>(1).
</div>
<div id="ref-wang-etal-2024-factuality" class="csl-entry" role="listitem">
Wang, Y., Wang, M., Manzoor, M. A., Liu, F., Georgiev, G. N., Das, R. J., &amp; Nakov, P. (2024). Factuality of large language models: A survey. In Y. Al-Onaizan, M. Bansal, &amp; Y.-N. Chen (Eds.), <em>Proceedings of the 2024 conference on empirical methods in natural language processing</em> (pp. 19519–19529). Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2024.emnlp-main.1088">https://doi.org/10.18653/v1/2024.emnlp-main.1088</a>
</div>
<div id="ref-wattanakriengkrai2022github" class="csl-entry" role="listitem">
Wattanakriengkrai, S., Chinthanet, B., Hata, H., Kula, R. G., Treude, C., Guo, J., &amp; Matsumoto, K. (2022). GitHub repositories with links to academic papers: Public access, traceability, and evolution. <em>Journal of Systems and Software</em>, <em>183</em>, 111117.
</div>
<div id="ref-Wehner2020" class="csl-entry" role="listitem">
Wehner, C., Roemer, L., &amp; Ziegler, M. (2020). Construct validity. In V. Zeigler-Hill &amp; T. K. Shackelford (Eds.), <em>Encyclopedia of personality and individual differences</em> (pp. 875–878). Springer International Publishing. <a href="https://doi.org/10.1007/978-3-319-24612-3_1288">https://doi.org/10.1007/978-3-319-24612-3_1288</a>
</div>
<div id="ref-welty2019metrology" class="csl-entry" role="listitem">
Welty, C., Paritosh, P., &amp; Aroyo, L. (2019). Metrology for AI: From benchmarks to instruments. <em>arXiv Preprint arXiv:1911.01875</em>.
</div>
<div id="ref-Zhang_2022" class="csl-entry" role="listitem">
Zhang, Y., Yu, F.-Y., Schoenebeck, G., &amp; Kempe, D. (2022). A system-level analysis of conference peer review. <em>Proceedings of the 23rd ACM Conference on Economics and Computation</em>, 1041–1080. <a href="https://doi.org/10.1145/3490486.3538235">https://doi.org/10.1145/3490486.3538235</a>
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Anaconda data science report 2022: https://www.anaconda.com/resources/whitepaper/state-of-data-science-report-2022<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Anaconda data science report 2023: https://know.anaconda.com/rs/387-XNW-688/images/Anaconda%202023%20State%20of%20Data%20Science%20Report.pdf<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Anaconda data science report 2024: https://www.anaconda.com/wp-content/uploads/2025/01/Anaconda_SODS.pdf<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p><a href="https://landing.ai/data-centric-ai#:~:text=%E2%80%9CInstead%20of%20focusing%20on%20the,a%20data%2Dcentric%20approach.%E2%80%9D">landing.ai</a><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>For brevity I refer to all human input as ‘annotations’ in this work, which encapsulate all related terms, e.g.&nbsp;labels, gold standard, ground-truth etc.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>https://ai.meta.com/blog/meta-llama-3/<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>According to <a href="https://en.wikipedia.org/wiki/Wikipedia:Size_of_Wikipedia#:~:text=Monthly%20statistics-,Number%20of%20pages,pages%20are%20created%20than%20articles.">wikipedia</a> on 9 May, 2025.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>Using the conversion method of words to tokens recommended by <a href="https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them">Open AI</a>.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>According to <a href="https://www.sci-hub.se/about">sci-hub</a> on 9 May, 2025.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>According to <a href="https://blog.scopus.com/posts/scopus-roadmap-whats-new-in-2022#:~:text=There%20are%20currently%2087%2B%20million,new%20articles%20per%20day%20indexed.">SCOPUS</a> on 9 May, 2025.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>According to <a href="https://www.theatlantic.com/technology/archive/2025/03/libgen-meta-openai/682093/LibGen">The Atlantic</a>.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>According to <a href="https://commoncrawl.org/">Common Crawl</a> on 9 May, 2025.<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>https://authorsguild.org/app/uploads/2023/12/Authors-Guild-OpenAI-Microsoft-Class-Action-Complaint-Dec-2023.pdf<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>https://www.elsevier.com/connect/more-early-career-researchers-are-stepping-up-to-peer-review<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p>https://elifesciences.org/inside-elife/982053f4/early-career-researchers-views-on-peer-review<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16"><p>https://github.com/tranhungnghiep/AI-Conference-Info?utm_source=chatgpt.com<a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17"><p>https://neuripsconf.medium.com/what-we-learned-from-neurips-2020-reviewing-process-e24549eea38f<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18"><p>Search terms included AI, artificial intelligence, machine learning, and neural networks. See Appendix B for details.<a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19"><p>https://github.com/tranhungnghiep/AI-Conference-Info?utm_source=chatgpt.com<a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn20"><p>according to <a href="https://www.glassdoor.com/Salary/Figure-Eight-Federal-data-annotation-specialist-Salaries-E4588136_D_KO21,47.htm">glassdoor</a> on May 28, 2025<a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn21"><p>https://www.mturk.com/pricing#:~:text=MTurk%20Fee,per%20assignment%20or%20bonus%20payment.<a href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn22"><p>https://www.prolific.com/participants<a href="#fnref22" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn23"><p>https://www.business-humanrights.org/pt/%C3%BAltimas-not%C3%ADcias/openai-and-sama-hired-underpaid-workers-in-kenia-to-filter-toxic-content-for-chatgpt/<a href="#fnref23" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn24"><p><a href="https://www.usa.gov/minimum-wage#:~:text=The%20federal%20minimum%20wage%20is,tips%20is%20%242.13%20per%20hour.">usa.gov</a><a href="#fnref24" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn25"><p>https://daily.zooniverse.org/2013/09/24/hannys-voorwerp/<a href="#fnref25" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn26"><p>https://www.zooniverse.org/projects/zookeeper/galaxy-zoo/<a href="#fnref26" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn27"><p><a href="https://osf.io/wfc6u/">Collaborative Replications and Education Project</a><a href="#fnref27" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn28"><p>https://en.wikipedia.org/wiki/Systematic_review<a href="#fnref28" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn29"><p>https://en.wikipedia.org/wiki/Meta-analysis<a href="#fnref29" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn30"><p>https://en.wikipedia.org/wiki/Umbrella_review<a href="#fnref30" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn31"><p>https://asreview.nl/<a href="#fnref31" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn32"><p>https://replicate.com/<a href="#fnref32" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn33"><p>https://huggingface.co/<a href="#fnref33" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn34"><p>https://osf.io/<a href="#fnref34" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn35"><p>https://quarto.org/<a href="#fnref35" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn36"><p>https://jupyter.org/<a href="#fnref36" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn37"><p>https://f1000research.com/<a href="#fnref37" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn38"><p>https://www.researchequals.com/<a href="#fnref38" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn39"><p>https://www.researchhub.com/about<a href="#fnref39" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn40"><p>https://alexandria.ewi.tudelft.nl/about<a href="#fnref40" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>